{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f73dc36",
   "metadata": {},
   "source": [
    "\n",
    "# Exercise 11: Visual Odometry\n",
    "\n",
    "Solutions for the week 11 exercises. We follow the lecture terminology on calibrated two-view geometry and visual odometry.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085717e",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11.1\n",
    "\n",
    "Detect SIFT keypoints and descriptors in the first three images and match them between consecutive frames. We limit the number of SIFT features to 2000 and store keypoints as 2D numpy arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563f971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "data_dir = Path('exercises/ex11_data')\n",
    "K = np.loadtxt(data_dir / 'K.txt')\n",
    "\n",
    "# Load the first three images\n",
    "im0 = cv2.imread(str(data_dir / '000001.png'), cv2.IMREAD_GRAYSCALE)\n",
    "im1 = cv2.imread(str(data_dir / '000002.png'), cv2.IMREAD_GRAYSCALE)\n",
    "im2 = cv2.imread(str(data_dir / '000003.png'), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Detect SIFT features (at most 2000)\n",
    "sift = cv2.SIFT_create(nfeatures=2000)\n",
    "kp0, des0 = sift.detectAndCompute(im0, None)\n",
    "kp1, des1 = sift.detectAndCompute(im1, None)\n",
    "kp2, des2 = sift.detectAndCompute(im2, None)\n",
    "\n",
    "# Convert keypoints to arrays of 2D points\n",
    "kp0 = np.array([k.pt for k in kp0])\n",
    "kp1 = np.array([k.pt for k in kp1])\n",
    "kp2 = np.array([k.pt for k in kp2])\n",
    "\n",
    "# Match SIFT features between consecutive frames\n",
    "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
    "matches01 = bf.match(des0, des1)\n",
    "matches12 = bf.match(des1, des2)\n",
    "\n",
    "# Convert matches to arrays of indices\n",
    "matches01 = np.array([(m.queryIdx, m.trainIdx) for m in matches01])\n",
    "matches12 = np.array([(m.queryIdx, m.trainIdx) for m in matches12])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613b03ae",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11.2\n",
    "\n",
    "Estimate the essential matrix between the first two images using RANSAC with `cv2.findEssentialMat`. Decompose it with `cv2.recoverPose` and retain only matches that are inliers and lie in front of both cameras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb471df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Estimate essential matrix and recover relative pose\n",
    "E, mask_E = cv2.findEssentialMat(kp0[matches01[:,0]], kp1[matches01[:,1]], K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "_, R1, t1, mask_pose = cv2.recoverPose(E, kp0[matches01[:,0]], kp1[matches01[:,1]], K, mask=mask_E)\n",
    "\n",
    "# Keep matches that are inliers and in front of both cameras\n",
    "mask = mask_E.ravel().astype(bool) & mask_pose.ravel().astype(bool)\n",
    "matches01 = matches01[mask]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ae956a",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11.3\n",
    "\n",
    "Find feature tracks across the first three frames so that `points0[i]`, `points1[i]`, and `points2[i]` denote the same scene point.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3187e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, idx01, idx12 = np.intersect1d(matches01[:,1], matches12[:,0], return_indices=True)\n",
    "points0 = kp0[matches01[idx01][:,0]]\n",
    "points1 = kp1[matches01[idx01][:,1]]\n",
    "points2 = kp2[matches12[idx12][:,1]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f0769f",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11.4\n",
    "\n",
    "Triangulate 3‑D points from the first two images and estimate the pose of the third image with `cv2.solvePnPRansac`. Visualize the inlier 3‑D points together with the camera centres (the camera centre is given by −R^T t).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780ad7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Projection matrices for views 0 and 1\n",
    "P0 = K @ np.hstack([np.eye(3), np.zeros((3,1))])\n",
    "P1 = K @ np.hstack([R1, t1])\n",
    "\n",
    "# Triangulate homogeneous points and convert to Euclidean coordinates\n",
    "hom_Q = cv2.triangulatePoints(P0, P1, points0.T, points1.T)\n",
    "Q = (hom_Q[:3] / hom_Q[3]).T\n",
    "\n",
    "# Estimate pose of third image with PnP RANSAC\n",
    "retval, rvec2, tvec2, inliers = cv2.solvePnPRansac(Q, points2, K, distCoeffs=np.zeros(5))\n",
    "R2, _ = cv2.Rodrigues(rvec2)\n",
    "\n",
    "# Helper for camera centre\n",
    "camera_center = lambda R, t: (-R.T @ t).ravel()\n",
    "\n",
    "# Visualize\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "ax.scatter(*Q[inliers.flatten()].T, s=5)\n",
    "ax.scatter(*camera_center(np.eye(3), np.zeros((3,1))), c='r')\n",
    "ax.scatter(*camera_center(R1, t1), c='g')\n",
    "ax.scatter(*camera_center(R2, tvec2), c='b')\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); ax.set_zlabel('Z');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdfbc22",
   "metadata": {},
   "source": [
    "\n",
    "## Exercise 11.5\n",
    "\n",
    "Generalize the visual odometry pipeline to the complete sequence by iterating over all images. We maintain lists of rotations and translations (`Rs`, `ts`) and triangulate new points for each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eeb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Iterate through all images in the folder\n",
    "image_files = sorted(data_dir.glob('*.png'))\n",
    "Rs = [np.eye(3), R1]\n",
    "ts = [np.zeros((3,1)), t1]\n",
    "all_points = []\n",
    "\n",
    "for i in range(2, len(image_files)):\n",
    "    img_prev = cv2.imread(str(image_files[i-1]), cv2.IMREAD_GRAYSCALE)\n",
    "    img_curr = cv2.imread(str(image_files[i]), cv2.IMREAD_GRAYSCALE)\n",
    "    kp_prev, des_prev = sift.detectAndCompute(img_prev, None)\n",
    "    kp_curr, des_curr = sift.detectAndCompute(img_curr, None)\n",
    "    kp_prev = np.array([k.pt for k in kp_prev])\n",
    "    kp_curr = np.array([k.pt for k in kp_curr])\n",
    "    matches = bf.match(des_prev, des_curr)\n",
    "    matches = np.array([(m.queryIdx, m.trainIdx) for m in matches])\n",
    "    E, mask_E = cv2.findEssentialMat(kp_prev[matches[:,0]], kp_curr[matches[:,1]], K, method=cv2.RANSAC, prob=0.999, threshold=1.0)\n",
    "    _, R, t, mask_pose = cv2.recoverPose(E, kp_prev[matches[:,0]], kp_curr[matches[:,1]], K, mask=mask_E)\n",
    "    mask = mask_E.ravel().astype(bool) & mask_pose.ravel().astype(bool)\n",
    "    matches = matches[mask]\n",
    "    pts_prev = kp_prev[matches[:,0]]\n",
    "    pts_curr = kp_curr[matches[:,1]]\n",
    "    P_prev = K @ np.hstack([Rs[-1], ts[-1]])\n",
    "    P_curr = K @ np.hstack([Rs[-1] @ R, ts[-1] + t])\n",
    "    hom = cv2.triangulatePoints(P_prev, P_curr, pts_prev.T, pts_curr.T)\n",
    "    Q_new = (hom[:3] / hom[3]).T\n",
    "    all_points.append(Q_new)\n",
    "    Rs.append(Rs[-1] @ R)\n",
    "    ts.append(ts[-1] + t)\n",
    "\n",
    "# Stack all 3-D points\n",
    "all_points = np.vstack(all_points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f97d61",
   "metadata": {},
   "source": [
    "\n",
    "## Optional extensions\n",
    "\n",
    "- **Exercise 11.6:** Triangulate each feature using all frames where it was observed.\n",
    "- **Exercise 11.7:** Capture your own calibrated image sequence and apply the pipeline.\n",
    "- **Exercise 11.8:** Perform bundle adjustment over all poses and 3‑D points for refinement.\n",
    "- **Exercise 11.9:** Evaluate the method on the KITTI sequence 09 with provided ground truth.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
