All right. 

Speaker 1: I think this is as full screen as we're going to get the slides. 

Speaker 2: Welcome to the final lecture in computer vision. I hope you're ready. You've been preparing all semester for this. 

Speaker 1: So today we're talking about 3D structured light scanning. And we are recording and live streaming this lecture. And I would still like two people to volunteer to give me some final feedback after the lecture. So who will that be this week? 

Speaker 2: Yes, we have two. Thank you. All right. 

Speaker 1: So after today's lecture, you should be able to explain what laser line scanning is. And you should be able to analyze and use great coding coding. And you should be able to analyze and use phase shift encoding. So what does that mean? 

We will find out once we've been through all of these things. And at the end, I give you some information about the exam and you can of course ask any open questions you may have. So in general, the topic of photogrammetry. So we already did like slam structure of motion. So we know kind of the concept of taking a lot of pictures of a scene and then we can use slam to find the actual camera poses that have captured these images. And we can find some 3D points in this scene. But this only gives us a very, very sparse 3D representation of the scene because it's only where or we detected a shift feature in some of the images. So there are algorithms for doing dense 3D estimation. 

So when you have two images and know their relative pose, then you can estimate the depth of everything in that image. But that is a difficult problem because you have to be able to kind of match every point to every other point. So it's not super accurate depending on the scene. But it turns out, yeah, I mean, it works okay in daylight and if you have textured objects, but if you don't have good light or if you don't have a textured object such as just a blackboard with nothing on it, then it's really difficult to match every single point on this blackboard to the exact same point in the other image to be able to triangulate that point correctly. So we're not going to dive further into this path, but instead spend today talking about structured light, which is kind of like cheating for getting a good 3D scan. 

Because it's what if we don't have to use the visual appearance of the blackboard itself to match every point to a point in the other image, but what if we illuminate the scene with some light source of our own choosing? And the first light source we're going to talk about is a laser line scanner. So this is some apparatus that will display a laser line onto the world and we can control where this laser line is shown. So the setup for this, it's been used a lot historically, is we have this, the laser emits a 3D plane of laser light and this plane will intersect the object or the world somewhere and then we can see the laser hitting the object that will give a very bright red line in our image and that's projected to the camera. So the camera, in the camera we can very easily identify where is this line of high intensity and because we've calibrated both the camera and the laser line, then when we see the laser line in the camera, then that's a point in the camera that gives us a line and we have the equation for the 3D laser plane we're currently displaying and then we compute the 3D intersection between the line and the laser line. So we can see the laser of the pixel where we see the laser and the 3D laser plane and that gives us a 3D point on the object. And then we can rotate the laser plane a tiny little bit and capture a new image, find where does the laser intersect the object now and then rinse and repeat and each time we move the laser we get a new line of 3D points on our object. So this requires that we calibrate our camera laser setup so that when we have the laser at a certain angle we know the equation for the laser plane and more annoyingly it's slow because each time we capture a full 2D image with our camera we only get a single line of 3D points. So we get one line of triangulated points per image we capture. So if we want to scan a whole object we can do capture quite a lot of images but we get really nice results from this and they've used it to scan like statues in Rome and so on. But the premise behind this, the underlying concept is what we will be building on with the rest of the techniques I explained to you today. 

So is this clear to you how this works? Okay because it's basically the same thing but we will make our laser planes smarter. Because we're not really using the rest of the image with the laser we're just using the part that was illuminated by the laser so what if we couldn't encode the entire surface at the same time? And what if we do something like we see in this image we take some kind of white light source shine it into some crazy optical things so we get like different color displayed on the different parts of the object. And then we can see the color and then we know exactly where that light is coming from. So we should be able to do that but when we try to display some color things onto the object then the object itself has color and it can really interfere with what we're reading. 

So something with color is not really recommended to have the color carry information because it can easily be obscured by the color of the object itself. But we can do another continuous encoding which is sinusoidal phase shifting which we will talk about in a while. But for now we talk about binary monochrome patterns so if I have a projector like a regular projector showing these slides I could use this projector to show a single vertical line of white pixels and everything else black that would be my laser plane. But then the question is could I do something interesting with the rest of the projector pixels to make more efficient use of I actually have a projector. And the way we do that is with or discrete encoding called great coding. 

But here is just some more explanations on the continuous schemes but we'll skip over this and talk about the concept behind discrete encoding schemes. And the main thing is that we have now we show not a single image where we have a single white line but we show a sequence of images with our projector. And then in each pixel we look at which sequence of intensities did this pixel receive. So if I show let's say I show four images with the projector then each pixel now has four observations what the intensity was in each of these four images. And then we can identify where was this pixel in the projector's field of view depending on the pattern we projected to it. 

And it kind of makes more sense when we look at a single example of this which is the binary encoding. So the thing we have here on the left that is the image we would show with the projector. So it's an image that is entirely white on the left hand side and entirely black on the right hand side. We project that onto the world and then for robustness sake we also predict the inverse image onto the world that we can subtract these two images from each other. And then we have a really strong signal as to whether this was illuminated or not by the projector. And then we can see if it was illuminated by the projector we know this pixel must be left of this dotted red line. And if it was not illuminated it must be to the right of the red dotted line. So now we've been able to say is the pixel left or right of the middle of the projector. 

But as this is called binary encoding we can add more bits to our encoding. So now instead of only showing one frame we show two different patterns. First we show one with white on the left, white on the right, that's this one. 

And then the second thing I show with my projector onto the object I want to 3D scan is frame number two. That is white, black, white, black. And now if I see a pixel in my camera that is white in the first image and white in the second image. I know it isn't this quarter of the projector's field of view. If it's black in the first image and white in the second image I know it isn't this quarter of the projector's field of view. 

And then I can keep adding subdivisions of my image so I show more and more high frequent binary patterns with my projector. And there is a question. The question was what is the purpose of the inverted one? And the purpose of the inverted pattern is just to obtain increased robustness. Because if I just have an image with projector light it can be difficult to distinguish. Is this just a white piece of paper or is this actually something that's bright from projector light? So by having both the regular projector pattern and the inverted one, then when you subtract these then you kind of take the object's own brightness mostly out of the equation. But now with this we have, I know every pixel here is in the left quarter of the projector. And then at some point in my camera's field of view I will have a change of the code. It will go from being white white to being white black in some pixel. And where the pattern in the camera changes from white white to white black. I know that this, the pixel in between those lies exactly on this line here. And this line here that is a line projected from the projector so that is a kind of a virtual laser plane. And here we are able to separate the image into four regions with just two frames. 

And if we add more we can do finer and finer subdivisions and then we display the sequence of images. Read out was the light, low or dark in this. And then for each pixel we can see it was let's say white black white white black white black black. And then we know we are exactly here in the image, in the projector's field of view. So each time we add a new frame with this we subdivide the volume that we're looking at once more. So if we capture n frames like n different patterns with the projector we get two to the power of n unique regions. So instead of just sweeping a single line of illuminated pixels we would need n frames to get n unique borders. 

But here we get two to the n minus one unique borders. So we quickly subdivide the thing we're looking at when we project more and more patterns. So if we have a projector that's full HD then we have 1920 pixels in the width. We can do a log two of the width so we can do 10.9 bits or that means we can do 10 frames in total. So it's actually we would need to project 10 frames and then 10 inverted frames. So if we just show 20 different projector patterns onto an object we can measure which of these tiny tiny pixel intervals of the projectors field of view that this 3D point is in. And for each of the borders that corresponds to a virtual laser plane and we compute the point laser plane intersection if it's a calibrated projector. 

Is that clear? So we have more lasers laser planes in software than frames we show because it grows exponentially. But the encoding that we use so far is a binary encoding. So this is if you count the region you're in down here region zero one two three four and then you could take this number and convert it to binary. That would give you exactly what the pattern should be in each of the frames. So this is purely counting in binary. 

But for this utilization of it binary actually has some problems. Because let's say we are here. We want to be here at the dotted red border. 

But something happens the light is reflecting in a weird way something is kind of shiny. So we do a wrong reading on pattern number four. This we should have read it to be a one. But accidentally we read this to be a zero because of some noise. So now the border we actually believe we're in is the one over here because when I only changed this from a zero to a one. 

I jump all the way here. Like if you do a bit flip in your binary number the final number can change quite a lot. And this is a very bad thing because that'll be a very wrong 3D point. 

But if we do this instead so you can see it looks kind of the same but different. And this here is another way of encoding numbers in zeros and ones. This is called gray coding coding. 

And the property that great coding coding has with binary doesn't. It's is that each time I increment my number by one or in this case each time I moved between move to the next region. I only change a single bit in my binary representation of the number. I always change one bit. So that means if I only read a single bit wrong I will only be off by one. So this is a much more robust encoding to use when we're doing this in the real world. 

Any questions? So we only change the bits each time we change a border. So it's much more robust and it uses the exact same number of frames as a binary pattern. So that's pretty nice. And that's what we can do with binary patterns but now we move into kind of my favorite of the pattern encodings that you can do with a projector. 

I guess you have to have done it a bit too much to have a favorite but let's not. It's called phase shift encoding. So instead of only using completely off or completely on with our projector what if we like vary the intensity as we move along the X coordinate of the projector. 

So the pattern that we would display onto the world will look something like this. So and this is just shown as a function here. We have the intensity of the projector as a function of the projectors X coordinate. And the mathematical equation for this pattern that we see here is given by the theta of X. 

It's a function I've defined where it's zero on the very, very left hand side and two pi on the very, very right hand side of the projector. And then we multiply this by N which is the number of periods we have in our pattern. So it starts from zero cosine is one and it goes down once, twice, three, four, five, six, seven, eight. So N is eight in this case because we have eight periods. 

Is there a question? The X is the X coordinate of the projector. So if it's a full HD projector it would go from zero to 1900 and 19 or one in 1920. But mainly we will just think about it in terms of we re-parameterize the projector to have a coordinate system that is theta that goes from zero to two pi. That is how we talk about the projectors coordinate system. 

Then we also resolution independent. So we take the projectors X coordinate from zero to two pi multiplied by N. Then we have this pattern signal that goes from zero to eight times two pi or N. And then we take the cosine of it so we get this wavy up and down. But the cosine goes between minus one and one and we don't get half the technology to display negative light. 

So we divide this by two and that a half so we go from zero to one. Which is again what you see up here. So this is a monochrome pattern. There's no color in it. 

We don't have problems if the object itself has color. And we have something that's continuous. So each point in the image will have a unique virtual plane corresponding to it. Once we know what the face is, we're not actually limited to the number of pixels in the projector. And even though the projector can only display discrete intensities and has a discrete number of pixels, because we are in the real world, if the projector is just a tiny bit out of focus, then we can actually display a continuous signal by defocusing the projector. I will drop the X notation and just refer to the X coordinate of the projector as theta. 

And it goes from zero to two pi. So now we have to wrap our heads around what's happening here. So now I'm showing different images that I will project here as a small slice of each image. So the first image is the one I showed you already before. But then I change the image that I predict a tiny bit, in that I no longer have n times theta, but I also have this part, which is 2 pi times f over s. And f, that is the number pattern I'm showing currently. So f increases all the way down here, and then s is the number of shifts that we're doing. Because it's called phase shifting. So you can see the entire line we have up here, the first pattern. If we shift this entire pattern a tiny bit to one side, then we shift the entire pattern a tiny bit again, and we shift it one or two, three, four, five, six, seven, eight times. And once we've shifted it eight times, if we would shift it a ninth time, we would be back to the beginning. 

So s is eight, and f goes from zero to seven. So the thing we see in the figure here is each small strip here represents a full pattern that I project onto the world. And then the thing we have in the vertical dotted lines is we're now looking at a specific pixel in the camera, or like a specific part of the scene, what light does this point see? Because now if I only look at a specific point in the world, but I look at it as a function of the pattern I'm displaying, now I get a sinusoid. 

So I shift the projector sinusoidal patterns, and then when looking over the different images in these pixels, I get a new sinusoidal pattern. And we can see here that the red and the orange and the blue parts, they have slightly different patterns, just fade onto them. They have different phases, these three sinusoids. And we note that each of them have only a single period, even though we have n being much higher. So then we could compute the phase of that pattern. 

So each pixel is now a sinusoid when we look at it as a function of the number of patterns I'm displaying. Is there any questions? Up until now. It's a lot of things to wrap your head around. Okay. So we... we managed to change the projector's pattern and entire wavelengths for one period we do that in S steps. So S is the number of shifts we have. And each projector x-coordinate corresponds to a virtual laser plane, so we would like to know, once we've measured, All of the information in a specific pixel, we would like to know which virtual laser plane does this correspond to, aka which x-coordinate of the projector illuminated this specific camera pixel. And we can find this if we fit a sinusoid to it and recover the face of that sinusoid. And you can use these squares to do this, or you can use the fast Fourier transform. 

And the fast Fourier transform is fast, so we will use that. So if we take the fast Fourier transform of the observations in a single pixel as a function of the pattern, then the first number will just be the number of shifts or the mean, but the second one will be a complex number and the angle of this complex number will be the face of the sinusoid. So we just take the angle of the second component of the FFT and we were hoping that this would let us measure the x-coordinate of the projector, but the thing we get out is something that looks like the thing on the right. So we increase all the way up to 2 pi and then we go back down and then we go all the way up to 2 pi and back down. So we've been able to measure something about where we are, where this point is with respect to the projector's x-coordinate, but it's wrapped between 0 and 2 pi, it doesn't go all the way up to 8 times 2 pi. So we don't actually know uniquely where we are in the projector's x-coordinate, so we don't know which unique laser plane we are in, we just know if we find a face that is lower than we know that we are either here, here, here, here, here, here. 

So we have n different virtual laser planes to choose between and we need to find out exactly which of these virtual laser planes this point is intersected by. And I will give you a break in a few minutes, don't worry. 

Speaker 2: But I want to cover the heterodyne principle first, which is the trick that lets us do this. So we don't only display one set of sinusoid. We show first the thing I showed you before, we have like let's say a number in, in one period across the projector, and we measure the angle of that, this theta, theta 1, with the FFT. 

And then we have a different set of patterns with slightly more periods called n2. And we measure the angle that we obtained from that. And if we choose n2 to be equal to n1 plus 1, or in general just such that they are co-prime, but this is a very easy way to do that, we can recover the global X coordinate of the projector, theta, by subtracting the phase that we get from the primary pattern from the secondary pattern. So if we take theta 2 minus theta 1, this is just these two guys up here that we measured modulo 2 pi. 

So this entire thing here is modulo 2 pi. We can replace the real equation for theta 1 and theta 2. So this is n2 times theta, n1 times theta. 

And because n2 is just one more, then this entire equation here gives us theta modulo 2 pi. So by displaying a second set of sinusoidal patterns onto the object, we've been able to measure the global X coordinate of the projector, which we call here the phase Q. So this looks something like this. We have the first pattern and we have the second theta and they go up and down between 0 and 2 pi. 

And when we subtract these exact red and blue signals from each other and do modulo 2 pi on it, then we get this nice, smooth, increasing signal and we've measured the predictor's X coordinate. And with that, I will let you have a short break until 20 to 2, unless you have questions. Seems not to be the case. Otherwise, I'm here. 

All right, we're back. So before the break, we computed the phase Q and we were very happy that we managed to compute the exact X coordinate of the projector as single pixel was illuminated by when we were doing the phase shifting with two different patterns. But I did a bit of foreshadowing by calling it the phase Q. So it's not going to be good enough for doing actual measurements because the real world contains noise. But luckily, we can throw some math on top. 

That's not very complex. That reduces the noise by quite a lot. And I will tell you why we can do that. But first of all, if we don't have any noise, sure, you can use the phase Q. It'll be perfectly fine. 

But if you're doing things in the real world, then you always have some kind of noise. So we have the phase Q as we did before, which is just our secondary phase. And then we subtract the primary phase. And then we introduce a new variable called the order, which computes how many times the primary phase has wrapped around. And that's, I mean, when we have this thing here, then each time we come all the way up to 2 pi and jump back down to zero, then we've wrapped around once, then we've wrapped around twice, three times, and so on. 

And we introduced the order to measure how many times we've wrapped around. So we take n1 times the phase Q. So that should be equal to theta 1. These two things should be equal, except that this one wraps around and this one doesn't, because the phase Q goes from zero to 2 pi. 

So there's no wrapping here. So the only difference between these will be 2 pi times n1. And then we divide this whole thing by 2 pi, and this gives us a relatively nice signal that goes from zero, and it jumps up to one, and it jumps up to two. 

And then there's a bit of noise. So we throw some brackets around it to round this number to an integer. And this rounding gives us the robustness to noise. And now we can re-estimate our actual projector X-coordinate, this theta estimate, by taking 2 pi of the order and then adding theta 1 to that, the primary phase, and dividing this whole thing by n1, because this here goes from zero to 2 pi, zero to 2 pi, jumps up and down. But when we add this thing to it, so the order goes from zero, one, two. We multiply it by 2 pi, so it goes zero, 2 pi, 4 pi. And when we add these two things together, we get a smooth signal that goes zero all the way up to 8 pi. Or n1 times pi. But because we want something that actually gives us an X-coordinate in the projector, we divide the whole thing by n1, so this estimate of theta will then go from zero to 2 pi. And we have modulo 2 pi on everything in case there is some noise that makes us rub around. 

But why does that make sense? Well, we can do a bit of write some numbers out and or some math out and see what happens. So let's assume that we know the true primary phase and the true secondary phase. But the phase we measure is the primary phase plus epsilon 1 and the secondary phase plus epsilon 2. 

So there's some noise on it. And the phase Q, we get this, that's the true phase, minus epsilon 1 plus epsilon 2, because we have epsilon 1 and epsilon 2, they are part of this linearly. And if we then substitute these equations into the thing we did here on the previous page, then inside the order we have the primary phase with some noise on it, but because of this rounding operation, that noise disappears. So the epsilon is gone from here. 

So we actually have a clean estimate of the order. And then when we plug that into here, we have no noise here, some noise in the primary phase, but then we divide that by n1. So the actual noise in our estimate of the phase will be epsilon divided by n1. So now we've reduced the noise in our measurements by n1, and n1 can be a big number, like 40. So we've made the noise in the system 40 times less. 

So this is a really good thing to do. And it looks like this in practice, and this is something I plotted from today's exercise. And here I'm showing we're 3D scanning an object, and I have the left camera, and I'm looking at row number 400, all pixels that row number 400 sees. 

On the very left here, we see some noise. So this was probably not illuminated by the camera. And then we have mostly an increasing signal, and then there's something that wasn't illuminated by the projector as well. 

But it's mostly a smooth signal, but we see that there's some noise up here. This is the phase Q. And the blue thing in the background, that's theta 1, and we can see this jumps up and down and up and down. And then we can compute the order that I talked about before, and we can see that actually some of this noise isn't present in the order before rounding. But once we round it, this just becomes a completely straight orange line. And then we can recompute the phase Q, or we can recompute the phase, this estimated phase. And I show that as this orange line, and we can see the noise here is gone. 

Now we just have a clean signal. So you can refer back to these if you want to kind of like debug some of your code. So the way we actually do 3D scanning with phase shifting coding is usually to project two different sets of sinusoid with different number of periods in them, such as 40 and periods in one and 41 periods in the other. And then we shift the pattern with 40 periods, and then we shift the pattern with 41 periods. And then for each pixel, we compute the wrapped phase of the primary pattern and the wrapped phase of the secondary pattern. And then we calculate the phase Q by taking the difference of the primary pattern and the secondary pattern. And then we calculate the primary order. 

And then we estimate the actual pattern. And then when we have theta, we have the phase, and we have a calibrated projector. We can find the corresponding projector plane. 

So which virtual laser plane does this correspond to? And then we can translate the pixel by taking the intersection of the pixel ray and the predictor's plane. And this means that we can actually potentially do one 3D measurement, one triangulation per pixel in the camera. 

And this is easy to make more precise. We can just add more projector frames, then we get more signal. And the thing about one projection per camera pixel is quite powerful, because there can be a big disparity. You can have like a 20 megapixel camera and a 2 megapixel projector. But as I said before, as soon as the projector is a little bit out of focus, you actually have an analog signal generator. And the real resolution of the projector doesn't matter. 

And then you can measure 20 million points from the 20 megapixels in your camera by just showing like 10 or 20 different projected patterns onto the world. Ah, there are any questions? Okay. So, usually we need to calibrate the projector. 

There are some ways of doing that, but we need to find correspondences between the projector and then we can estimate the projection matrix of the projector. And it gets complicated. I've done it. I wouldn't recommend it. But we can do something else instead of actually needing to calibrate the projector. We can just use two cameras. So, have one camera here and one camera here, and then have a projector in the middle. So, everything else is the same as before, except if we just calibrate the cameras and then estimate their relative poses and their intrinsics and their distortion, then we don't need to calibrate the projector because we purely use the projector as a signal generator. So, the projector shows all of these patterns and it encodes everything on the surface, but instead of using now intersection with a virtual projector plane, then we see I imagine now I'm in camera zero, and in some pixel I've measured the projector phase of 0.23. Then, because this whole thing is calibrated, I can find for this pixel what is the epipolar line in the other camera. 

I know the point I'm looking for must be along this epipolar line. So, in the other camera, I walk all along the epipolar line looking for the pixel that is closest to the encoded phase 0.23 because that must be illuminated by the same X coordinate of the projector. So, we don't need to know exactly where the projector is, and the projector doesn't need to be super accurate. 

But then we only need to calibrate the cameras, and that's much easier. And this requires that if we imagine the projector planes, they should not be parallel with the epipolar planes. So, we usually have the projector project patterns that are constant along this direction and shift in this direction and then have the camera separated or orthogonal to that. So, today's exercise is on structured light with phase shifting. So, what you will do during the exercise is the image rectification and phase decoding and unwrapping, and then you will do some masking to find out which pixels were not illuminated by the projector, and then you will match these pixels to each other, find out when I look along the epipolar line in the other image, which one corresponds to it, and then you will triangulate the 3D points. And you'll be working on this dataset we scanned in our lab, which is from the Natural History Museum. At some point, they were like, this baby T-Rex skull is visiting us. 

Could you maybe 3D scan it? We were like, sure. So, it came out to our lab, and then we took some pictures of it. 

That was quite cool. And the way that you search along the epipolar lines, for that we use rectified cameras, which you will do with a single open-CV function, but I'll just explain to you what it's doing, which is you have your raw images in the beginning, they have lens distortion, all sorts of things, and then you understood them, and then we rectify the images. And when we rectify the images, this makes the searching operation much easier, because rectified images have parallel epipolar lines. And what this means is that once I have a point in one image, I don't need to think about what is the epipolar line in the other image and search along it, then I know if this point was in row 15 of this camera, I know the corresponding point must be on row 15 in the other camera, because the images have been rectified such that they have parallel epipolar lines. So, that just speeds up the searching quite a lot. 

And you're just matching a single scalar, this phase, so you're not matching some high-dimensional signals, so it's also efficient. And I can tab over here and show you with some of the colors on it, that this is the point cloud that you can, I think maybe add down same images to be nice to you. But this is not a mesh, this is just a point cloud with lots of tiny, tiny colored points, because you get a 3D point for each pixel in the camera. 

So, it's a very easy, comparatively easy, compared to other bracelets of 3D scanning, to get very high resolution and accurate 3D scans of objects. So, now I will talk about small wrapping up things, but that was it for the technical content. If you liked this course or you like images and you would like to work more with images, here is a list of other courses I suggest you could take. There is the advanced image analysis course, which also runs in the spring, so it just wrapped up this Wednesday. But if you have the opportunity to take that, it's a very nice course. And then in the fall, there is introduction to deep learning and computer vision, which is a course where you get introduced to deep learning on images from the foundation. And then once you've taken that, you might as well take advanced deep learning and computer vision, where you get introduced to more advanced concepts like vision transformers and diffusion models, and all sorts of actually state-of-the-art things. But there is less 3D in both of these courses. 

That's kind of the 3D aspect in computer vision. It's mostly covered here. So after this lecture, you should now be able to explain laser line scanning, and you should be able to analyze and use gray coding coding, and you should be able to analyze and use phase shifting coding. And there will also be an exam, which I assume you're aware of. And you can find the time and place of the exam by going to exam-plan .dtu.dk. 

I believe it's from 9 in the morning when I checked. And it is a multiple-choice exam, and it is following the same format as the exams I've uploaded to DTU Learned. You will not be punished for answering incorrectly, but you will just get zero points for that question. And there is also a lot of possible answers for the questions. 

So guessing will not be worth that many points. The questions are a mix of understanding questions and calculation questions. So you actually need to have a computer that can run things that are similar to the exercises. But the questions are made such that if you have solved the exercises, then you should be able to relatively easily answer most of the exam questions. So you can use your computer and all your notes and all your code, and you cannot access the internet during the exam, and you should not use any large language models or generative AI during the exam. And since you don't have access to the internet, you can just download, click, the documentation of OpenCV onto your computer. So you have that available in a searchable format. 

If you run into some function you would like to use. Are there any questions about the exam in general or something unclear? Something you would like elaborated? Yes? Yes. 

That is a very good point. It is a four-hours exam. Okay. And then we have had some weekly quizzes. And we have arrived at a winner. So let's check the very exciting leaderboard. Did you learn this keeping up the suspense? So, Elhu, could you please come down here? And could you give her a round of applause? And then we have the very high end present here to compensate for my lack of use of the blackboard. And some chocolate as well. 

Yes. I hope you... I mean, I can't guarantee how you will do at the exam, but I expect it will be good, because it's very similar to the quizzes. So once again, please give a round of applause. And with that, thank you so much. It's been a pleasure teaching you. I wish you good luck at the exam. And if you want to get feedback, please come down here afterwards. 