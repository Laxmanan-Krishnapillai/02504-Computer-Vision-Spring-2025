Speaker 1: All right, the time is now 13.0000. And we don't have any technical issues this week, I hope. So let's jump into the lecture. And this week we'll be talking about simple features. And is the sound okay for you guys? Perfect. And you're being noticed that we're recording live streaming and hopefully this week you can actually live stream it. I've done a different thing. 

And same procedure as last week. I would like two people of you to volunteer right now to give me a bit of informal feedback after the lecture. Just anything that comes to mind how you thought about the lecture, you can tell me afterwards. 

So who would like to do that today? Thank you very much. We have two people. 

Wonderful. And here are the learning objectives of today. So after this lecture you should be able to explain what the image correspondence problem is. You should be able to explain how we use image features, both key points, interest points, the center names. You should be able to understand and implement image filtering. You should be able to implement the Harris corner detection algorithm that's like the bulk of today's exercise. And you should be able to run Cany edge detection. And here we have some... Is that my cursor? 

Yes. The table of contents with clickable wings. But first, the image correspondence problem. And this is kind of a shift or an explanation to the chicken and egg part of this course. Because now, up until now, we've been looking at some points and we have some theoretical cameras with some projection matrices. And these points exist. 

They come from somewhere. And then if we see the same point in multiple camera, then we can triangulate where exactly this point is in 3D. But what are points exactly? How can we see the same points in multiple cameras? How do we actually do this in practice? 

That's what we're going to spend the next few weeks on. So let's say I have these two different images of the same scene. So it's the same thing I'm looking at, but they're taken front on and then way over to the side. And then I've run some algorithm here that has found some points illustrated in green. And then with blue, we draw a straight line from the same point in one image to the same point in the other image. So the blue lines here indicate these two green points correspond to the same point in 3D space, even though they are projected to different places in the images. So the image correspondence problem is kind of, what is this point in one image? When I have that, how do I find the same point in the other image? That's basically image correspondence. So when I have two images, if a thing is visible in one image, it will have a correspondence in the other image. 

You should be able to find exactly this point in the image taken from here and an image taken from here. But if I choose this point in the middle of the blackboard and I have an image here and I have an image here, it's going to be very difficult for me to locate the exact same point in the other image because the blackboard is just flat. There is no texture on it. I haven't drawn something interesting on the blackboard. So finding a specific point on something that has almost uniform color is really hard to identify the exact same point in the other image. So we don't want to choose something very boring. So we want to find points that we can uniquely identify. 

So if I now show you some examples of these LEGO bricks taken from slightly different angles, then I want to just give you one minute to talk to your neighbor about what kind of points do you think we could identify that we will be able to find again in another image. Please do that now. All right. Would anyone like to share with me what insights you've arrived at in your discussion? 

It's a very open-ended question, so at this point there are no wrong answers. If you really want to see the shapes on the right side of the car, the round things on top of the LEGO bricks are uniform on everything. So you can also change that. But the corners, it might be a little buggy because, for example, the red brick, on the first picture you cannot see the backside, but on the third picture you can. So it might not detect that one. 

Okay. And for the people listening online that can't hear what's being said, I'll just repeat that it was just to look at the corners of the LEGO bricks, so kind of these places here, and potentially also the things on top like these round ones. And then we have the problems that, depending on the perspective, like here I can see this corner quite clearly, but when the image is rotated to here it looks very different. So we may not be able to identify the same corner, even though it's present in both images. And I have gone ahead and overlaid some corners on these images, detected with the Harris corner detector. And we can see that for some of them, let's say this pink brick here, we have quite nice that we find actually the same corners in all of the free images. 

Okay, we don't find this corner up here. I guess it's too close to the red one. But so there's, of course, some noise in the detection, and we also detect a lot of crazy things going on down here, where there's a lot of specularities going on, so the top here looks very different, because the LEGO logo is just reflecting the light source directly into the camera, which creates a lot of various corner detections. So we always have differences in our detections, but as long as we choose some kind of feature, such as a corner, that is characteristic, that we should be able to find another image, we should be good. And then we have to make sure that we choose something that will be robust towards the kinds of perturbations that happens when I take a camera and I move it around in the real world, so I can move my camera closer to something, and I can move it further away, and that changes how it looks. I can also take the camera and rotate it in place, and I can translate the camera. And all of these create different changes in appearance of the scene, and we need to be able to have something such as a corner that is invariant towards these changes as far as possible. 

Again, some of the corners of the LEGO bricks will not be visible when we change our perspective, but as long as some of them are, we should still be able to match some points. And here we just have an example. So these are different images. 

You can see the bush here has moved, so the camera has moved, but otherwise the cursor itself looks quite similar, and then we have a bit more extreme zooming and rotating everything. But we would like to be able to match features between all of these. And then we have some other problems that occlusion, it's not necessarily possible if I choose some point. This point can be hidden behind another point. 

You can't see my hand because it's hidden behind my other hand, so we're not guaranteed that we can see the same thing. And the lighting can change. It's not so much if the images are taken at the same time, but if we want to match things across time and space, then the change in the color of light can have a huge appearance, and we need to be invariant towards that. And if there are a lot of objects in the scene, we need to be able to recognize each object individually. 

And in order to... Yes, there's a question. Is the question whether the points I can find in one image and uniquely identify in another image, whether they are the same for all images? I don't think I understand the question quite. I mean, it depends what the other image is, which points you will be able to uniquely identify in the other image. 

It's not given beforehand. So the question is what it means that there's a lot more detections here. This just means that we have some noise in our detector, but luckily here, the points that we were able to detect over here of the corners, these corners were still detected. So because we're working on real data and there is noise, there will be a lot of false positives and points that are only detected in one image, that are not detected in the other image, and thus we can never match it because we didn't find it in the other image. 

Yes. Yeah, this is just the output of the detection, not the output after matching things to each other. We're just talking about detection for now to simplify things. And the way that we then try to match points to each other, we're not going to talk about today, but because the thing we're talking about today is kind of how do we find the points. And then in two weeks from now, we'll talk about how do we actually describe what the image around this point looks like in a nice, unique language that is invariant to all of this moving the camera, changing the lighting, rotating the camera. 

There's a really nice framework for that that's been around for quite a number of years since quite robust called SIFT. But today we're just talking about how do we find points. And then there are a lot of synonyms and I may use terms interchangeably, but I just put it on the slide for you. The things we find in an image when we detect things, just a single image, the point itself can be called a key point or an interest point or a feature point. So this is just an XY coordinate in the image where I think I can uniquely identify this point if I have a similar image. And then we have descriptors or key point descriptors, interest point descriptors, feature descriptors. 

These are the language we use for what does the area around this point in the image look like. And this is usually put into a vector and then we just find similar vectors in the other image and if the vectors are similar, we say this could be the same point. And the simplest feature descriptor would just be to take, okay, what are the pixels around this point? Let's put them into a vector and then we can measure if this is the same vector in another image. So if these have high similarity, the dot product will be large. But in general, this works really bad because it's not invariant to rotation or scale or translation. 

So if there's a little bit of transformation between the two images, this can work, but I wouldn't recommend it in practice. That's why we talk more in depth in two weeks about how to actually describe the appearance. So that was kind of the overview of what is image correspondence and now we go into the how can we find some kind of simple features. 

But before we can get to that, we have to revisit some basics. That is image convolution and just catch up on this in case you're in doubt because you need it a lot in the exercise today. So image convolution is just defined as this integral, but we're working with images, everything discrete. So we do it as this sum and we have some kind of filter here. This is a free by free filter. 

It contains values. The way I do the filtering or the convolution, I know there's a difference between filtering and convolution whether you rotate the filter, but in deep learning and also in discourse we don't care about the distinction. We just keep the filter at this, no rotation. We overlay this filter on the input image. We multiply the numbers that are overlaid. 

So these here are pixel intensities. We multiply 15 with 1.16 and we multiply 4 with 2.16 and so on. And then we sum the wrong up and then we get the output and we put that in the center of where the patch was overlaid. And then we slide the window over once more, do other modifications, sum, and then we get that out. We get an image out of the convolution operation. 

I'm going kind of quickly over this because it's kind of a prerequisite for the course. And yeah, I said, we just use filtering and convolution interchangeably. And convolution commutes. 

So convolving the image, the kernel with the image and convolving the image with the kernel is the same. And here is the definition of the two-dimensional caution filter that we can plug on if we want to blur the image. And very, very nice property when we're working with images is that the isotropic caution, so when we have the identity or a diagonal covariance matrix, we can separate the caution out. So we can write the 2D caution as a product of a one-dimensional caution vector and a one-dimensional caution vector. And then the outer product is the full 2D caution. So this means we can compute the full 2D caution by having the caution as a column vector and then convolving it with the caution as a row vector. This gives us the full 2D caution. And then when we convolve the image with this full 2D caution, we get the blurred image. But we can change around our parentheses because it's also associative. 

So we can put over here first. We convolve with the caution as a row vector, a column vector, row vector, yes. And then we convolve with the caution as a column vector. And the nice thing about this is that this has, let's say we have n elements in our caution, then this is complexity n squared, and this is complexity 2n. So it's way faster when we get kind of a larger caution. 

And when we want to convolve images with caution, which we want to do a lot today, ideally we would need to have an infinitely big filter because the caution has infinite support. It never goes all the way to zero. But we don't like infinite big numbers when we're computing with them. So we do some kind of truncation and say when I have the standard deviation of my caution, then I can go to, if I go to plus minus free standard deviations, then because of statistics, I know that I have 99.7% of all of the mass of the caution inside what I've actually computed. So a good filter size is just going to plus minus free standard deviations. And then that's an approximation of a caution filter, but it's good enough. And now we'll jump into Visual Studio Code and just do some of these things. So it's less confusing in today's exercise because you'll be using this subfilter 2D OpenCV function quite a bit. 

Are there any questions before I jump? Very good. So first we import NumPy and OpenCV. And then I add these things here so I can run everything. And my interactive kernel hasn't died, so I'm very happy about that. And I read in some image I've stored on my computer. 

And I also want to show the image and therefore I import Matplotlib. And then I check this, did this work? And then it takes a long time. And this got confused. And I need to set my Matplotlib back to inline. I changed some settings before this. 

And okay. Now an image comes out, but this is a very, very blue person. So it's because OpenCV loads images in as not red, green, blue, but as blue, green, red, because of extremely legacy reasons. 

It's a very old piece of software. So we flip the order of the channels and then we have something more reasonable. And then I've set it in another exercise as a hint, but every time you load in an image, do yourself a favor and convert it to floats. Because for some of the exercises later in the course, you're just going to run into weird errors if you have the image as integers, also in today's exercise. It uses more memory and so on, but for the exercises in this course, just always convert it to float. And I divide by 255, so I bring it back in the 0, 1 range and we still have our image. 

Okay. Now I would like to create some kind of Gaussian filter. So what I want is sigma for this Gaussian filter. Let's say sigma is 5. And then how far out do I want to go on each side? So let's say I want to go to plus minus 3 standard deviations. 

And this may not be an integer, so we round it up. And then I need to decide which x values do I compute my kernel in. And then we go from minus D to D and we check if this is what we wanted. And it's asymmetrical because I only went to D and not D plus 1. So now if I check x, it goes all the way from minus 15 to 15 and then 0 is in the middle. And then we can compute a Gaussian filter by taking minus x squared and divide by 2 sigma squared. And then you're like, okay, that's a bit... not see what was the definition of the Gaussian. 

That looked like this. It has this whole 1 over square root of 2 pi sigma squared. But I didn't write that because I'm lazy. Because we're doing an approximation of the Gaussian. So we're not getting all the probability mass in, but I know that the Gaussian is a density function and I want my approximation to sum to 1. So I just do this approximation and then I divide it so everything sums to 1. And then I want to look at what does G look like? 

Does it look like a Gaussian? It does absolutely not. Yeah. Let's square x and see if that works. 

Yeah, that looks a bit nicer. So if we don't do this normalization and actually implement the factor, then I would have something that sums to 0.997. And then when I blur an image, it actually gets a tiny bit darker because the Gaussian doesn't sum to 1. So even if we're approximating the Gaussian, we always want it to sum to 1. So now we have a Gaussian row or column. It doesn't really have a shape. 

It's just one-dimensional. So then we can try to apply this to our image. We can create a Gaussian version of the image and say cv2 filter2d. Let's say what if we take the image and this is something about the output, the data type of the output and we just leave it to minus 1 so it defaults to something and then we put in g and then we show what this looks like. 

So now we're just applying our one-dimensional Gaussian to the image and that did not go over well. And then we have a wonderful... Oh, I'm im showing t, not mt. My computer is a bit slowing when it's also recording. Okay, and then we have an image that looks almost like the image we had before. We can't see any difference because this is a quite high-resolution image. 

So let's turn up sigma to 50 so something will happen. And then we apply this bigger, way, way bigger kernel to the image. I should just have down sampled the image. Okay, and then we have something that looks like it's moving very fast but it's only been blurred in one direction because we've only applied a one-dimensional kernel to it. 

So we could do the auto product and get the 2d kernel or do filter 2d twice but we have this handy function built into openCV called cv2.sep filter2d where we can actually give it what is the horizontal kernel, what is the vertical kernel. We can just get some documentation of the function with the help command to turn up. Then we can see the first is the image and then the thing for the destination image and then we have the kernel for x and the kernel for y. I'm not actually sure why there's two inputs. 

I think this should work. So like the first we apply this filter in the x direction and then after that we apply our one-dimensional filter in the y direction. And now it has to do twice as much filtering but now we have an image that has been blurred in both directions without computing the 2d filter and only using a single function. 

So because you'll be doing a lot of convolutions with cautions, this is kind of nice to know. Are there any questions before I jump back to the slides? Very good. 

Yes. So we would like to find derivatives of an image. I would like to take the derivative in the x direction but what does it mean to take a derivative of an image? Because I mean I know that there are a lot of different things I know a bit about math. And then we have functions, and we can take derivatives of those. But an image is not a function. An image is just a collection of RGB values. So what does it make mean to take a derivative of an image? 

It doesn't really make so much sense, but we can do some cheating and make it make sense. So if we make the image not a collection of discrete values, but apply some math and make the image continuous, then we can actually take the derivative in the x-direction or the y-direction or whatever we want. So let's say we have a Gaussian, and now we have our image. We blur the image a bit, and then we get the blurred image. Now the blurred image, we can actually mathematically compute the value anywhere, because when we blur it with the Gaussian, then it becomes continuous, because the Gaussian itself is continuous. We're just working with a sampled version of the Gaussian. And it turns out to be even simpler than that, because we can, let's say, we want to take the partial derivative in the x-direction. 

We do this to our blurred image. We look here at the whole expression, and we take the derivative into the parentheses, and this here, that's our column vector. That's not a function of x, but this here is the row vector, and that changes as a function of x. And then we can apply this derivative to the Gaussian kernel, and then we have not here a Gaussian kernel in the x-direction, but the derivative of a Gaussian kernel in the x-direction. So actually taking the derivative of an image can be done by convolving with a Gaussian, where we've taken the derivative of the Gaussian. 

But so that kind of makes sense, if you ask me, but the thing that's a bit less intuitive or surprising, or the price we paid in order to be able to do this was we took the derivative of the entire image. So if I just take the Gaussian that's been taken the derivative of, the derivative Gaussian in the x-direction, and convolve that with the image, but don't blur the image in the other direction, then I've done invalid math, it's illegal. So we must always apply some kind of Gaussian in the x-direction and some kind of Gaussian in the y-direction. And when we want to know what is the image taking derivative in the x-direction, then the Gaussian, there's some kind of Gaussian we apply in the x-direction will be at the derived Gaussian, or like the derivative of the Gaussian. So we always have some kind of Gaussian on x, some kind of Gaussian on y. And here I've just been nice to you and written up the formula for the derivative of the Gaussian, because you can just take this and compute the derivative. It's really, really friendly. 

Yes, and because we can actually reuse in our implementation of the derivative, we can reuse that we've already computed the Gaussian, so that's why it's a simple expression to implement. And with that, I would like to take a five minute break, so let's meet back here 20 to 2. Hello, welcome back. 

Now it is time for Harris' Connors. There's a lot of text on one slide. So, Connors and a bit of spoiler blobs in parentheses, which we'll look at later. A nice feature is because they're easy to describe and detect. So now we introduce this quantity delta i, which is a function of some x, y coordinate in the image, and delta x and delta y. And this measures how much the intensity of the image changes when I move delta x and delta y away from my point x, y. So I'm at some point in the image, and then I go delta x, delta y away from this point, and I see how much did the image intensity change. And Harris' Connors are then detected as points where we have a locally maximum change from some small shift delta x, delta y. So a corner is a local area of the image where this intensity change metric delta i is this intensity change metric delta i squared is large, no matter how we choose delta x and delta y. 

So no matter how I try to move away from some point in the image, I want the image intensity to change a lot. And to improve robustness towards noise, then this cornerness should be true for a local area, not just a single point. So therefore we apply a Gaussian blur to this measure here. We can involve it with a Gaussian. 

We can involve our delta i that measures how much changes locally with this Gaussian. Which gives us the corner measure c. And here I've just written out what the definition of delta i was. That's the image minus the image when we move a bit away from x, y. And then I can take this part here where we have the image where we evaluated at x plus delta x and y plus delta y. 

We can replace this with a Taylor approximation. First order, so it's in x and y direction. So it's the intensity at x comma y minus the derivative of the image in the x direction at x, y times delta x and the same for delta y. 

So this is a Taylor expansion or a Taylor approximation of this. And then we introduce some shorthand because I do not want to write this out multiple times. So this is just ix and this is iy. 

The image where we've taken the derivative in the x direction evaluated at x comma y. And that's also a nice variable name to use later on. And then we can take this thing here where we have the ix times delta x, iy times delta y. We can write this up as a row and a column vector multiplied together. So we have the image gradients here and we have the deltas here. 

And yes, we're cheating a bit with the notation again because ix is evaluated at x, y, but we just omitted. It makes it brief. So this is really kind of the value at x comma y, but later on we can just look at the entire image. So our corner checking measure, if we recall, it was this. We want this, when we blur with the Gaussian, the square difference between the image intensity somewhere and the image intensity a bit away from that point. And we've now approximated this with a Taylor expansion. 

We can plug that in. And ix comma y goes away because that's also part of this guy, so they cancel each other out. And then we're left with the thing from before. And because this is squared, we can multiply it out and do the multiplication. So we have now a small column row vector with delta x, delta y, multiplied onto this 2D matrix where we have the derivative of the image in the x direction squared as this top left element. And this is the image in the x direction. The derivative in the x direction multiplied with the image taken the derivative in the y direction. It's not the image taken the derivative in the x and y direction. It's the product of two first order derivative images. And then the column vector with delta x and delta y on the right hand side. And everything is convolved with the Gaussian. So then we can also take the convolution with the Gaussian inside and blur all of these elements. And then we end up with this 2 by 2 matrix that we call c of x comma y. Now I introduce again that it's a function of x and y. 

And this is also known as the structure tensor that pops up in other fields like analysis of 3D microstructure and so on. Okay. And now we try to think back a few slides. What happened there? 

There I was talking about. We have some cornerness measure. So this cornerness measure should be large regardless of how we choose delta x and delta y. And we were able to approximate our cornerness measure as this whole thing here. 

So it's delta x delta y multiplied onto the structure tensor and then also multiplied from the other side. So this is our cornerness measure. But I could sit down and plug in a lot of delta x and delta y, but I don't want to do that. 

How can I know if the cornerness measure will be large no matter how I choose delta x and delta y? So this is something you've learned maybe too many years ago, so you've already forgot it. But what happens when I multiply a vector onto a matrix and then I take the dot product with the same vector again? So let's say what if delta x and delta y is an eigenvector of the structure tensor? So then suddenly I'm prompted to think about eigenvalues because if this is an eigenvector, then the thing I get out is the eigenvalue times the eigenvector. And then if I give the eigenvector norm 1, then the eigenvector dot product with the eigenvector itself will be just 1. So the value of this will be the eigenvalue. And if both eigenvalues are large, then this will always be large. If one of the eigenvalues is small, it will be small sometimes, the closer I am to that eigenvalue. So I can check the eigenvalues of c instead. And we can also do some other tricks because just checking the eigenvalues becomes a bit ad hoc. 

So we have some other ad hoc things on top. When we compute the determinant, that for a 2 by 2 matrix, that's the product of the eigenvalues. And just here for notation, I've called the elements of the structure tensor for a, c and b. So this is the formula for the determinant. This is the formula for the trace, which is identical to the sum of the two eigenvalues. And then Harris, the guy himself who invented the thing, introduced this concept r, which is the determinant of the structure tensor minus k times the squared trace of the structure tensor. 

And what does that mean? So we can multiply it out and it's the product of the two eigenvalues minus k times the squared sum. Or in terms of the elements of the matrix, it's this. But that's difficult for me to visualize. 

I'm not that smart. So instead, I used this nice drawing of it where what we have here as these contours are iso values of this corner, and this measure r. So we said we want both eigenvalues to be large. And then we have a corner. But it turns out we actually forgot a crucial thing. Because we could also have an edge. So let's say I have an image here. 

I have a corner here. And this is the same intensity. But then if I have right here in the image, I'm going to have a really large eigenvalue here. And maybe if there is like this line is quickly, the other eigenvalue could also be large. 

So just taking the sum of their product is going to be not necessarily robust. I also need to take the ratio of the two eigenvalues into account. Because if one eigenvalue is much, much greater than the other, that means the image changes much, much more in one direction than it does in another, then the point I'm looking at is no longer a corner, but it's an edge. So there's a much greater change in one direction than there is in another direction. 

And in order for me to be on a corner, I need to make sure that there's approximately the same change in one direction as there is in the other direction. And I don't want to put absolute thresholds on the eigenvalues. Because then it could also be that there's some corner that doesn't stand out so much compared to the background, but it's like the ratios of the two eigenvalues with respect to each other that we're interested in. And that's what this cornerness metric R captures. 

So K is this hyper-frearmature in the function, and it's typically set to 0.06. And when we do that, then we end up with this. So if we are down here, then both eigenvalues are small. It's a flat area, nothing is happening here. When lambda2 is really, really big, it's an edge. Or when lambda2 is big and lambda1 is small, it's an edge, and vice versa. But when both are kind of in the same ballpark and we are up here, then we say it's a corner, and we can then threshold R to find out. So if R is negative, one eigenvalue is much greater than the other, and it's large and positive for large eigenvalues, and it's small and positive for small eigenvalues. So we can apply some kind of thresholding to our image on this cornerness metric R that we've computed by computing the 2D structure tensor at every pixel in the image. 

So for every pixel, we have a 2D matrix, and we compute the eigen... or we compute the cornerness metric R based on these 2D matrices per pixel. Are there any questions? Or have I said too many confusing words? Or is crystal clear? 

Yes? Yes, I want the lambdas to be approximately the same. I want when I move in one direction, I want approximately the same amount of change as when I move in the other direction. So now I need to threshold this cornerness metric R using some threshold, and I typically choose the threshold to be between... I take the max of the entire image to find out what is the most cornerness going on here when I choose between 0.1 and 0.8 times the max value. That's my toe, that's my threshold. So this gives me an image where I've identified the corners, or does it? 

That's one final step that we need to go through. Because I've done the blurring, so this is a really salient corner in the image. So let's say we have our pixel grid on top of this beautiful corner, then we may actually have that this one is above tau, this one is above tau, this one is above tau. So because of the blurring, many of them are actually above the threshold. So the corner detection algorithm would give us like nine corners, but there's only really one. 

It's just because a lot of the values are greater than our threshold, but we only want the one that is the actual corner, the biggest of these values. So in order to do that, we need kind of this post-processing algorithm called non-maximum suppression. So we threshold the image and we only keep the things that are local maxima in the image. 

And formally, that is, we want to find the local maximum, that is, a pixel is only a local maximum when it's greater than all of its neighbors. So the easy way to implement this is to initialize a new array, mask or m, that we initialize as a Boolean array and this has everywhere where this should be the cornerness measure, not the image itself. So this is everywhere where the cornerness measure is above our threshold tau, like these are candidate corners. And then I take and compare the original image of the cornerness on a note, okay, this is non-maximum suppression for general image, not for the cornerness measure. I compare the original cornerness measure image, not the thresholded one, I compare with a neighbor. So let's say I look at my neighbor to the right and I ask, am I greater than my neighbor to the right? 

Do I have a bigger value? If I'm not greater than my neighbor on the right, then I know this point cannot be a local maximum. So therefore, I set all pixels in the mask to zero or to false when they are not greater than the neighbor. And then I repeat this for all of my four or eight neighbors, depending on how I define the neighborhood. 

So I would typically want to use all eight neighbors. And then when I've checked set to zero everywhere where the original image is not greater than all of its neighbors and set those to zero, then I just find the coordinates that are left in them that are still one, because these are both above the threshold and they are local maximum in the image. And then when you implement the check towards, is this greater than its neighbor? You need to make sure that you have some greater than or equal, because if you have two values next to each other that are exactly the same value and you enforce that one of them must be greater than the other one, then you can actually miss a local maximum. So when I check, let's say my image at x, y, this should be greater than my image at x minus one, y. And it should be greater than my image at x plus one, y. So if I try to enforce this whole thing, I can end up with not finding something if these two values are identical. So therefore I need to always in one of the directions have greater than or equal. So I end up keeping at least one of the values that are equal. And that's it for Harris' corners. 

Yes? This is for an neighborhood. So the neighbors of your point. So you could either choose to check when you're in an image. Then there are two kind of obvious neighborhoods. Yeah, either just up, down, left, right. But then you could have these two have identical values. 

And you can have some corners here. So you would typically check all eight neighbors in the image. Yeah, because this one you check if it's greater than all of its neighbors. And then that's when you're an image in 2D. And then if you're in a later week of the course, then you have this and you're suddenly in 3D, then you also need to check your neighbors above and below. So you can get to reuse your non-maximum refreshing code from today in a later week. 

Oh, yes. So the question is whether only one value is set to one in the neighborhood? So you can only have one local maximum within the neighborhood. That's kind of the definition of a local maximum. It's something that's greater than everything else in the neighborhood. And then you also get to play a bit with canny edges in today's exercise, but not implement. 

So this is a bit more fluffy explanation of it. If we're not interested in detecting corners, but in detecting edges, we can use the canny edge detector. And how do we measure is something is an edge. 

We do that by looking at the gradient magnitude of the edge. Now I've reintroduced the x, y subcret or the functions. So it's the squared gradient magnitude next plus this, the squared gradient and y. That gives us a gradient magnitude. 

So just everywhere in the image, what's the length of the gradient? And then I can threshold this magnitude. But if I just threshold it, I may get some kind of weird line. I detect this part of the edge, but then it just drops below my threshold for here. 

And then I detect some the other edge over here. But there's a tiny hole in this detected edge because the intensity just happened to drop below the threshold there. So therefore, canny uses two thresholds and a seed and grow threshold. So first, what we have here is kind of like, this is something that looks like edges in the image that have a certain amount of corner or edge measure gradient magnitude. So I take everything that's part greater than the first threshold, a big threshold. 

I say this must certainly be edge. And then the second step is called grow, where everything that I found in my first iteration, that's greater than g1, I look, is there anything in my neighborhood that is still greater than this lower threshold tau 2? So therefore, in this image, I would be able to detect all of this line because this is included in the seed stage. And then there's something that's greater than tau 2 all the way here. But nothing on this line, this edge is greater than tau 1, so it doesn't get found. 

So this is the conceptual output of canny edges in this case. And it's nice, it's trusted. It's a good, it's a good edge detection algorithm. And in today's exercise, you will be implementing the Harris corner detector, and you will be trying out canny's edge detection. And after you finish implementing it, try on your own images, like draw some features on a piece of paper and see how it works to get some insight into it. So now you should be able to explain the image correspondence problem. You should be able to explain the use of image features, key points, interest points, and you should be able to understand and implement image filtering. You should be able to implement Harris corner detection, and you should be able to run canny edge detection. So with that, I wish you good luck at the exercise, and if the feedback persons could come down here. Thank you. 