Is the sound okay for you? Ish, you want more sound? More sound? More? More, more? 

Speaker 1: Better? Yes. Okay. Hello. It is me. I would like to wish myself a warm welcome back. Thank you, myself. I am Morten, Riga Hanemose, and it is I who is usually teaching this course, but so far you've gotten to experience Arnast and Teodor, and I've heard that they did an amazing job on the first four lectures, but it's me who's giving the lectures from now on, and I hope that you are ready for that. I'll just give you a few words about myself. So I have a picture of myself, and I'm an assistant professor here at D2Compute in the section for visual computing. I am a colleague of Arnast and Teodor. 

My background is in kind of this computer vision and differentiable rendering, and I work with a lot of different applications of this in camera calibration, 3D scanning, human pose estimation, and a surprising amount of skin cancer as well. And about the course, some things have not been kept quite up to date, in my absence, but just to clarify, there are these weekly quizzes on DGULearn that consist the exam as a multiple choice exam. So I've taken the questions from the 2023 exam and cut them up into which week does this question belong to, and then made them into these weekly quizzes. So if you complete all of the quizzes, then you will have completed the entire exam from 2023. And you can still do the quizzes from the previous weeks. 

I certainly hope so. If I understand DGULearn correctly, even though the deadline is exceeded, if you can't, just tell me and I'll fix it. And I also put this... Where do we have... You can see how you're doing. If you put your name in the first week, then we have this overview of the people that have answered the quizzes and which answers they got correct or wrong. 

So some people still have everything right. And please, along the course, let me know if you have any questions, if it's about the course or about something that I'm saying right now in the lecture, or just anything, feel free to interrupt me or send me an email or talk to me after the lecture. I'm here to help you learn the best. And as in the previous weeks, this is your notice that what I'm saying is being recorded, and if you ask a question that may appear on the recording, you can access the recordings from the DGULearn page. 

Is it not being streamed? No, I can see that. Okay, then I'll change my procedure for next week. 

I don't think I can change it while it's running. And then, a bit of departure from how things have been running the previous weeks. Then, I think so, at least, I would like two feedback persons. So I would like right now for two of you to volunteer after this lecture, you come down here and just tell me what should I do different, what you like, what didn't you like. 

It's completely voluntary, it's not dangerous, it's just an opportunity for you to speak your mind about what happened in today's lecture or the course in general. And I'm waiting for you to volunteer yourselves now. We have one volunteer, thank you. 

And we have a second one, thank you so much. So please stick around for a bit instead of going straight to the exercises afterwards. And now, for the actual content of today. What will we be learning about? We'll be able to perform nonlinear optimization in computer vision after this lecture. We should also be able to explain the principle behind the living by macOS algorithm and we should be able to compute the Jacobian of the function. And we should be able to reason about different ways we can parameterize rotations in optimization. 

And here's the table of contents with some clicky links, if that's your thing. But the first thing we talk about is this nonlinear optimization. Because it turns out in computer vision we encounter a lot of problems that we can actually write as a problem of this type. 

So we have some function g of x. And this could be something we're trying to fit. It could be a homography that we're trying to estimate. And then we have the parameters of the homography put into the vexa x. And then the function takes out our input points and it maps them through the homography. And then y is our observation of where these points should be. 

And g of x tells us where did the points map to. Because with everything we've done so far, we've used these linear algorithms computed by doing the SVD, we get an estimate, but it's not the least squares estimate. So we can make things a bit more accurate by adding this on top. And we can also do this camera calibration if we want to add lens distortion on top or pose estimation or as we'll do in the exercise today. 

And use as an example throughout the lecture triangulation. So when I have two cameras that are viewing the same point in 3D, I want to find out, I've seen the point in 2D, where is this point in 3D? And in that case, the x is our unknown where the point in 3D is. And g of x is then the projection function that gives us where does this point project to in 2D in each of the two cameras. And y is where I've observed the point. 

Is this clear? All right. So we rewrite it a bit with some math. 

We just absorb g minus y. This is always the same. We don't want to write it out every time. So we just rename this entity to f. And the thing that we're minimizing or the thing we want to optimize, we want to minimize the sum of the squared errors. I like each term in this vector here is an error. And we want to minimize the square to norm. That is the sum of each element of f of x squared. And we can also write this as the inner product of f with itself. 

Because when we take the vector and the vector and multiply them together and get a single number out, that's the inner product. So the living by Mark-Ward algorithm that we use for this is an iterative algorithm. So we solved the problem in an iterative fashion. And now we imagine ourself being placed in at iteration k of running this algorithm. 

So right now our best guess of x, we call this xk at the k-fiteration. And then we have our function f. It's a bit difficult to work with. So we do replace our function with a first-order approximation of the function. So we do a Taylor expansion of the function around the current estimate xk. Which is we'll be able to compute f of xk plus some small delta, a vector that moves us a little bit away from xk. We can approximate this as the value at f of xk plus the Jacobian of f times delta. 

So this is just a first-order Taylor expansion, but of a multivariate function. Because f takes a vector in and gives a vector out. And j is then the Jacobian, which contains all first-order derivatives of x at xk. And how can we find the Jacobian of f when we only really know things about g? But it has the same Jacobian as g, because the only difference between f and g is that we've subtracted a constant y. So all the derivatives of these two are the same. So we can actually look at the Jacobian of g instead. 

So we're all clear on how it kind of looks like. The Jacobian, so we have r function f, that we can write this nice short way. We can also write it out bigger, that it takes x1, x2, x3 up to xn in the vector in. And then the vector comes out with now f1, f2 up to fm. So we have n inputs and m outputs. So now we can write the Jacobian as the partial derivative of f1 with respect to x1 in the very upper left corner, and then it's an m times n matrix. 

So it contains all first-order derivatives of f or g, because they're the same. So now we can go a step back and say, OK, what did we want to look at? We want to look at this actual sum of squared errors. At xk plus this, we walk a little bit of distance delta away from xk. We don't yet know what delta is, but we hope to find out. And we could approximate this f of xk plus delta. That's our first-order Taylor expansion, and we could do it as the inner product. 

So the approximated error here, that's why we have a tilde on the E. This approximated error is the inner product. And then we can multiply out these parentheses. So we have f of xk multiplied by f of xk. That's this first term, and then we have this one multiplied by this one, and this one multiplied by this one. 

So we get two of the Jacobian times this delta transposed times f. So that's another inner product. And then we have the final this one multiplied by this one. And we can rearrange the terms. So this last one becomes delta times the Jacobian transposed times the Jacobian times delta. And this is a second-order approximation of E using only first-order derivatives of f. So what does y is that so? 

We can see here we have in this term the delta vector appears twice. So it's second-order, but because the approximation we did of f was a first-order approximation, but we have a nice closed-form solution to go from f to E, where we square everything, then we can give a very good approximation of E, which is the thing we actually want to minimize, when we have access to this underlying function f. So if we only knew E itself but didn't know that E was computed as a sum of squared errors, then we couldn't do all these tricks. We're just utilizing that we're solving this very specific sum of squared error problem, and then we use derivatives of the underlying thing before we square it. 

And that's kind of the trick. So now we have a good approximation of our error that we want to minimize. And how do we minimize something? We can take the derivative. 

So we take our approximated error term and take the derivative with respect to delta. So what do we have here? This is a constant. 

It goes away. This one has two delta. So the delta disappears and we'll just live with two Jacobian times f of x, and this one has a delta over here and a delta over here. So it's kind of like x squared. So we get two times x or two delta times this Jacobian transpose times the Jacobian. And in this case, the Jacobian transpose times the Jacobian is an approximation of the Hessian of f. Just a fun fact. 

So we have taken the derivative of our error function, and then we can set the derivative equal to zero and solve for delta to find what is the optimum of this, at least what is the optimum of the approximated function. So we set it equal to zero and then we rearrange the terms. So we throw this guy over on the other side and divide by two. And now we have a matrix times an unknown vector is equal to a known vector. And we can solve this with least squares. And then we find delta, the optimal step that we should take at xk, given that our approximation is correct. So we solve for delta and then we find our next xk, so xk plus one, by taking our current best estimate of x and walking delta away from it. And then we do this over and over again and we converge to the optimum. 

Most of the time. Because the approximation that we're doing in epsilon hat is better the closer we are to the actual minimum. And when we're very, very far away from the minimum, the approximation is less good or really bad. 

So what can we do about it? We can fall back to the most trusted optimization method of all time, gradient descent. So we just take some step in the gradient direction. And if we take a negative step in the negative gradient direction, we'll always get to a minimum. 

So the neat thing it turns out is that if we add inside before, this was the equation system that we were solving to find delta to give us the optimal step. But if we on this left hand side here, instead of only having the matrix as being the Jacobian transpose times the Jacobian, if we add a little regularization term with lambda times an identity matrix, then if lambda becomes really, really, really big, then this entire matrix is just almost an identity matrix. And then the delta we get out of solving the system will be a step in the negative gradient direction with step size like 1 over lambda. So we can smoothly interpolate between completely trusting or second-order approximation of the error term when lambda is 0 and when we make lambda bigger and bigger and bigger, then we go towards only doing gradient descent with smaller and smaller steps. And then you can throw some heuristics on top that you try taking a step with your current value of lambda, and then you check, okay, what is the error, the actual error at my new xk plus 1? What's the actual error here? If the actual error got worse, then we took a bad step, then we go back to xk and we double lambda, and then we try again. 

And if the error decreases, then we took a good step, and then we can lower lambda a little bit so we trust our approximation a little bit more the next time. And this algorithm just is very widely used in optimization and especially in computer vision because there's just so many problems of this form that arises. So there are so many problems that are least-squared problems in computer vision and by doing explicit utilization that the problem we're actually doing is something where we're squaring each term and then minimizing this sum of squares, we can utilize how we formulated the problem and that we have the underlying thing before we squared it, and we can take the derivatives before they get squared to get a second-order method for the squared problem. 

And that's kind of nice because then we don't have to worry about second-order derivatives which are even more of a headache than first-order derivatives. So, do we have any questions about the first part? Very well. Or yes? Yes? 

No, the... Okay, so the question is what is E exactly? And the E is just how well does our current guess fit the data? 

So it completely depends on the problem. If it's triangulation, then... and I have two cameras, then my Y will be the X coordinate in camera 1, Y coordinate in camera 1, X coordinate in camera 2, Y coordinate in camera 2. So Y is everything I know and G of X is then what does the model say that the value should be? And then E is what's the square difference between what I think it should be and what it becomes? 

Yes? So the question was whether lambda is the step size. The lambda is kind of a regularization parameter. So if the lambda is zero, then I take the step that is entirely determined by my approximation. 

And if lambda is quite big or the closer lambda is to infinity, the more the step goes towards gradient descent, the negative gradient direction of E. Okay. And in everything I just talked about, I just lightly danced over the Jacobian, because where does it come from? Who gives us the numbers inside the Jacobian when we have some kind of problem? 

How do we actually compute this guy? And okay, the Jacobian is just a lot of first-order derivatives put inside a matrix, and there's kind of free schools of doing this. You can do analytical gradients. You can do automatic differentiation, which has two sub-species. 

There is reverse mode automatic differentiation, which is widely used to train neural networks, where it's called back propagation. And then you have forward mode automatic differentiation, which is called dual numbers, and they have different properties in terms of how many things you want the derivatives of and what your output space is, and then you have finite differences. And I will talk more about analytical gradients and finite differences now. And if you have any questions about automatic differentiation, feel free to ask me in the break. So what do I mean when I just say analytical gradients? You sit down, write up your problem, and you take out your pen and paper, or maple, or senpai or something, and then you compute the derivative, and then you implement it in the computer. 

That's it. It requires some finesse, and you can do some mistakes, but I have the example first before the... Let's do the pros and cons first. So if you actually implement it, it's very fast to compute, and they're extremely accurate because there's no error. 

But all the work lies in... It's complicated to implement them and to derive them, but luckily many functions in OpenCV return the Jacobian in addition to the value itself. So that's nice, but you would usually only implement the Jacobian if you're trying to do production quality code, or I have to optimize problems of the same type many, many, many times, and it's worth putting in this effort. And then here is... We will now go through an example where we do analytical gradients of triangulation of a point. So we want to now triangulate a point in 3D, so the thing we want to find, or x, is the 3D coordinate of the point. 

We've seen this 3D point in cameras. So our f is written up like this. Q tilde 1 is our observed. It's tilde because there's some noise when we're observing the point. This is where we saw the point in camera 1. 

And then we have here Q, that's our 3D point. Then we put it through the pi inverse function, so we convert the 3D point to a 4D point in homogeneous coordinate, a four-dimensional vector, which is a 3D point in homogeneous coordinates. We multiply it onto the projection matrix for camera 1, and then we put it through pi again, so we get out the inhomogeneous 2D point projected to camera 1. And we subtract this from where we observed the point, so this gives us the first two elements of our f vector. That is, how far off is the projection in camera 1? 

And then we do the same thing for all of our in cameras. So in this case, when we're doing the least squares problem for triangulating a point seen in in cameras, then the function f returns a vector that has length 2n, because we have both x and y in each camera that we've observed the point in, and or Q, or which we called x before. The thing we're trying to optimize has three parameters, because that's how many degrees of freedom our point has. And this also means that the Jacobian has size 2n times 3. 

So how do we compute the Jacobian analytically? We do a little bit of shorthand. We just do this for one camera, so we drop the subscript on the projection matrix, and we introduce a shorthand for the homogeneous 3D point as Qh. 

And Qh is also seen here. It's just with a 1 on the end. And then we introduce some more notation, which is the rows of the projection matrix. We just give them this subscript. 

So we have low case P12 and 3. This forms the projection matrix. And over on the left side here, we have the projected 2D point in homogeneous coordinates with some unknown scale, that is s. So we need to divide by s to get the actual x and y out where this point projects to. 

Okay? And we saw here, okay, s is given by P3 times Qh. So we can get the x-coordinate of this projection out by dividing by s, which is the last row of the projection matrix times the homogeneous 3D point. So the numerator is now the first row of the projection matrix divided by the homogeneous 3D point. So this is Sx divided by S, as we saw here. And then we can even write all of this out instead of having these nice matrix or vector products. We can write out all of the terms individually. 

P11 times x plus P12 times y. And then we can look at, okay, what is a single element of our Jacobian? What is the partial derivative? What's the derivative of the x projection with respect to the x coordinate of the 3D point? And that's just taking the derivative of this thing here. And then we get out, okay, all of these don't have any factors of x, so they're constants. 

It's only this one, so P11. And then divided by P3 of qh minus pi, this whole thing over here. And this is kind of, I'm just showing you this. But it's just, there's nothing magical about it. It's just computing derivatives. It's hard science. 

There's no creativity needed. Just consistency. So very fast, very bothersome, but worth it sometimes. That was analytical gradients. Now, for the other thing I promised to talk about, finite differences. They're kind of like the evil twin of analytical gradients. Let's do the pros and cons first. They're very convenient. They're very easy. 

You can basically just get them out like this. They're much lower to compute. They have lots of unnecessary computation. There's some hyper-feramity we introduced that we need to choose. And then they're not accurate. I mean, they're an approximation. 

It says so in the name. So I would only use these if you're not concerned about speed or robustness. So for example, in today's exercise, that's what I would go for. OK. And how do we do it? 

That's just very straightforward. We now look at only a univariate function. So if just put, gets a scalar in and puts a scalar out, and then we look at the value of x plus h, and then we do a first-order Taylor expansion at the point where we put the point x, which is given by this thing. So it's the value at f of x plus the derivative of f at x times h. And we can isolate for the gradient. 

So we can rearrange this. And then we find that the derivative at x of f of x at the point x is equal to f at x plus h minus f of x divided by h. So in other words, if we want to know what the gradient is at x, then we can evaluate f of x and a little bit away from f of x, and then divide by how far we went away. And then we get an approximation of what the slope was at that point. It's like a very intuitive way of computing gradients. 

And it also works. And this is called forward differences because you do plus h. So you also have backward differences where you do minus h. And this whole thing has an error of o of h, because it's a first-order Taylor approximation that is kind of the driving mathematical force behind it. And we can also do a second-order Taylor approximation and then evaluate the second-order Taylor approximation at f of x plus h and f of x minus h, and then rearrange the terms. And then we get out this other thing where if we evaluate a function at f of x plus h and f of x minus h, and then divide by 2h, because now we are 2h apart, then we get a better approximation because this is based on a second-order Taylor approximation. This is called central differences because the two things we evaluated are kind of centered around f of x. 

It's more accurate, but it requires two new evaluations of f. And you may notice that both of them require two evaluations of f. But when we are doing living by Markov, we have a really, really big vector, or we have a really, really big x. Then each time we want to compute the derivative of an element in x, we need to recompute something. But this here, where x is unchanged, we could use this for every single element of x. So this only requires one evaluation of f per value in x, whereas this one requires two, so it's twice as slow, but also more accurate. 

So it kind of depends what you need. And this principle, I did everything unibariate because we can only apply it a single parameter at a time. So we need to evaluate f once or twice for each element I have in the vector x, and this can be quite a lot. And you can do some tricks to reduce the number of evaluations if you know which elements kind of don't affect each other. So I can change this element of x, and it will not affect these outputs, and then you can change two elements of x at the same time. But that's some trickery I will not get into here. And then we have this parameter. 

It's h. What do we do? A good choice is just to say some fixed percentage, like 10 to the minus 4 times x. Then we're always within nice numerical precision. Other issues with this. Let's see. If we always choose the percentage of the number, then if that number is zero, then our step size is zero, and then we need a special if statement to handle that. Ask me how I know, because I had some code that was giving me a very, very weird error, and I couldn't figure out why. 

So if you are not in the mood to implement analytical gradients, just go for finer differences, and if it's not good enough, then you can spend the time on your analytical gradients. And now, before I talk any further, we should do a short break of five minutes. So let's meet back here 10 minutes to one. And this watch is a few minutes behind. All right. 

So let's jump back into it for something different but related. Because in many of these problems that we are interested in, in computer vision, that we can write in a least, want to solve as a nonlinear least-quash problem, it could, for example, be a camera calibration where we want to find the lens distortion. But then we need to be able to represent the position of the camera, but also, crucially, the rotation of the camera, or if we're doing post-estimation of some object. We also need to represent the rotation of the object. And it turns out that rotations in optimization is kind of this whole thing, whether it's good to know some things about it. Because they're kind of deceptive, because we usually, in computer vision, represent these as three-by-three matrices, so we can multiply the amount of things and get it rotated out. 

But we also know in our hearts that they have just three degrees of freedom. So how can we parameterize a rotation matrix? How should we take this three-by-three rotation matrix and put it into the vector x, so we have something that we can optimize? If we just flatten it and put all of these nine numbers into our vector x, the optimization will very quickly move away from something that's a valid optimization matrix, and it'll start scaling the object and squishing and doing all kinds of weird things to it. 

We don't want that. One of the things we can do is called Euler angles, which only uses three numbers, and it's kind of like in, you take the first, you take the amount of rotation around x, and then the amount of rotation around y, and then the amount of rotation around z. And you make a matrix that rotates around x, around y, and around z, and you multiply them together, and then you have your rotation matrix parameterized by three numbers. But this, as you may have heard, if you have heard or like reading about the Apollo missions, this can suffer from gimbal lock, because this is the same thing that happens when you have a physical gyroscope that you want to use to track the rotation of your rocket going to the moon. 

If two of these gyroscopes align, then suddenly you can no longer optimize well with this. And I will show you, actually what it looks like. I've made this little website, Fingering. So here we have a z-pad, and then we can rotate it around x, we can rotate it around y, and we can rotate it around z. So we can rotate it in three different directions. So if our optimization algorithm changes any of these parameters, the rotation is changed meaningfully. But you can also click the link in the slides if you want to play around afterwards. 

But let's say we've been moved here. So our x rotation has rotated us minus pi, and then in the y we've been rotated kind of pi half and also pi half in the z. So now this also looks fine. If I rotate y a little bit, we rotate this direction. If I rotate z a little bit, I rotate kind of up and down. And if I rotate x, I also rotate kind of up and down. So this means in the current configuration of the x, y, and z rotations, I cannot change a single of these three parameters to obtain kind of this rotation because the Euler angles have become gimbal locked right now. 

And for this reason, and kind of like if you get close to the gimbal lock, you have this weird singularity and it's hard to get out of. So we don't really like using Euler angles for optimization because I can get into this that I lose a degree of freedom and no matter which parameter I change, I can't really rotate in the way I want to at that point. But we can do a trick if we still want to use Euler angles, which is if we make sure the rotation that we're optimizing is close to the identity rotation. So each of the three angles is kind of close to zero. 

We're far away from gimbal lock and we don't have to worry about it. So if we have a good guess, which I guess I should have said long ago in this lecture, we're only doing nonlinear optimization when we actually have a good starting guess of x. We do some linear algorithms to get a pretty good estimate of x, and then we throw it over to the nonlinear optimization. We don't want to start just optimizing from noise, from a random guess. So we actually have usually a good guess of what the rotation should be. So if we then compute the final rotation matrix as an identity rotation starting with zero in each of the three rotations multiplied by our initial guess, then we probably won't move into the gimbal lock territory during the optimization if the initial guess is good enough. 

And if we're doing finer differences, then we don't want to start from zero because then we multiply our parameter h by zero. So we could start by having all of them be rotated to pi. So it's still the identity. That's kind of Euler angles. And then we have axis angle. And here this is another way of representing a rotation with three or four numbers because in axis angle we represent a rotation as we have a rotation x as v. And then we have an amount of degrees that we're rotating the object around this axis in 3D. And then if I want to rotate the other way, then I can just have the negative axes. And if the axis has length one, then I can actually divide this axis with length one by the amount I want to rotate. 

No. Why do I want to divide? I want to multiply it. I want to multiply it by the angle I'm rotating. 

And then I can store it in three elements. And if the rotation then becomes zero, then I'm rotating zero degrees and then it's zero, zero, zero. And this is coincidentally also how OpenCV gives all rotations to you that it calls RVEC. 

And you can convert these RVECs to three by three rotation matrices with the CV2-Rotrigues function. And this has a singularity at zero, but it works well in most cases. And we can go back here and just kind of refresh this guy. And here we have our axis angle rotation. And then we can just play around with it and it looks... It also looks kind of okay, but it suffers from some of the same problems that is not really ideal for optimization because when I... You can see if I just change RZ, I'm just changing something linearly, but if you look at the rotation of the object, it's kind of funky. So the space is not really super linear, super clean to work in. So the final thing I'm going to talk about is quaternions, which are kind of the gold standard for representing rotations. Quaternions use four numbers to represent a rotation, but these four numbers, when they're squared, they sum to one. They were invented by some guy in the 1700s as an extension of the complex numbers. And nobody had anything to use them for until people in the 70s found out, hey, these are really good at representing rotations in a computer. 

So they were useless for a very, very long time. And they're super smooth and they have no problems when we use them, except that we have to make sure that they still sum to one. So we have to normalize the quaternion in each step. And the nice thing about a quaternion is kind of when I move here, when I just change a single number, okay, you can see that here the script is normalizing the other numbers, so they still sum to one. But when I only change a single number, I'm always rotating in kind of the same direction. So if I linearly interpolate between two quaternions, I get the rotation I would expect between these two quaternions. So quaternions are also used in computer graphics to interpolate between rotations, where Euler angles and axis angle have these when I interpolate between different points, it behaves in an unexpected way. So if it's a possibility for you in the framework you're using to optimize with quaternions, do that. Otherwise, you can resort to the tricks I described. And you don't want to implement everything from the bottom, but luckily there's many implementations out there. You can do Limba Mark-Watt in series, this is a C++ library that uses dual numbers, which means you don't have to implement any derivatives, it just uses templating and C++ thing and does all the derivatives behind the hood. 

Very cool. And it has quaternion parameterizations. You can use scipy.optimized least squares as we do in today's exercise. 

This is very easy to use and you can just ask it to do finite differences for me and it will do it. Or you can feed it then, vertical gradients if you have those. And then finally, you can use Jax. Jax doesn't implement Limba Mark-Watt per se, but it's very commonly used in machine learning and it has different ways of computing automatic gradients both in forward and backward mode. 

So I just wanted to give it a shout out. Now that we were talking about gradients. And then to wrap things a bit up, I have some final points on camera calibration. I know we talked a lot about camera calibration last week, but here are some more tips and tricks. 

I'll talk about what can we do instead of checkerboards? How do we do subpixel estimation? How can we overfit? How do we deal with overfitting? And what is bundle adjustment? So with a normal checkerboard, like you saw pictures of last week that we used to drive the entire calibration process, the open CV detection algorithm is very sensitive and wants to be able to see the entire checkerboard before it can detect it. And this means that covering your entire image plane with detected corner points can be hard because if you try to go close to the edge of the frame, it's very easy to get the checkerboard outside of the frame. 

So an alternative to this, which I would recommend if you're doing camera calibration in practice, is called a Rucoboard. And this is a checkerboard covered in a Rucomarkers, which are kind of, they're not QR codes, they're a Rucomarkers. But it's the same principle that if the algorithm now just sees a subset of this, because we have these data matrices, each of these are uniquely identifiable, and then we know exactly which corner we've seen, even if the entire checkerboard is not visible. And then the things we talked about were also just how do we find, we've detected the checkerboard, we kind of know where the corners are, but in order for the calibration to be accurate, we kind of need to know very precisely is the corner here or here or here or here. 

And for this, there is a function in open CV that kind of looks at the corner around the neighborhood called CV2 corner subpics. And I'm not sure I would recommend it based on my choice of emoji here. So what can we do instead? One algorithm I would recommend was presented by Schubsel, where I did my survey, the citation went away. But basically here they generate n random duty vectors, which are displacements from the true corner, and then they optimize a homography. And this homography maps from the checkerboard to the camera. And the only objective or last term as the kids call it nowadays is that you have the homography of my point here, where the homography is 0, 0 at the corner. So if I go in my vector off from the corner and then compute the image value here, this should be the same intensity of the image as if I go in minus that direction from the corner. So we're just saying that if I map this corner to the image, or if I rectify it such that the corner is at 0, 0, then the intensity should be the same whether I go in a random direction here or in the opposite direction, because it's symmetrical. And then they optimize the homography with nonlinear optimization. 

And then you can read out what is 0, 0 in this homography. And they get some really nice results from it, but it's quite slow. And then when you're doing calibration in practice, then as with all machine learning, if you introduce a more complicated model, then your model has more expressive power, and you can better fit to the data you're given. So if you add another distortion coefficient to your model, you will get a lower reproduction error, because you're fitting the data better. But this is only for the images that you actually use for the calibration, and you're not certain that this generalizes and actually gives you a better calibration, because you may just be overfitting to the data you have, or maybe you don't have any points near the edges of the frame and your distortion coefficients can do really crazy things. So it won't necessarily generalize. 

And therefore, if you're doing camera calibration in practice, I propose that you use cross-validation, just good machine learning practice, have a set of checkerboards that you use to fit the calibration, and then on a second set of checkerboards that are not part of the calibration, you can estimate their poses and compute the reproduction error, and then use this as a target of how complex of a distortion model can I include. Yes, and this is what I just described. And then the final point, what is bundle adjustment? Because when I want to do a really good calibration, it turns out that all of the imperfections of the real world really become an actual problem. And it's very difficult to make a perfectly flat checkerboard. I will, after the lecture and the feedback, go and print some checkerboards on A4 paper that you would use to do your own calibrations, but because it's just printed on a piece of paper, it kind of lies on the table and doesn't want to be flat. And therefore, we can solve this with optimization. 

So we have a pretty good guess that the checkerboard is probably flat, but we can optimize for every single position, 3D position of the corners on the checkerboard, and then we can actually estimate what is the shape of the checkerboard by just throwing everything into a big optimization problem and solving it with Limbermaquad. Are there any questions regarding that? I know there was a lot of info in the short time, but I also want to release you so you can go to the exercises. But you should now be able to perform nonlinear optimization in computer vision, and you should be able to explain the principle behind Limbermaquad. You should be able to compute the Jacobian of a function in a few different ways, even. And you should be able to reason about different parameterizations of rotations in optimization. And if there are no questions, which it seems there are not, I will give you some info about the exercise, which is I will go after the feedback and print out these checkerboards, give them to you, and you can use your very own cameras that you have in your pocket to take some pictures of these and then calibrate the camera in your phone and get a calibration out of this. 

And you can use either the openCV built-in camera calibration functions, or you can use the functions that you built last week. But that was it. Thank you so much. Enjoy the exercise. And please come down here, feedback persons. Thank you. 