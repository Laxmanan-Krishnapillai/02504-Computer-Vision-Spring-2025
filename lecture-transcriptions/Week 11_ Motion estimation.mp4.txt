Speaker 1: Hello, welcome back. I'm so glad you could make it. Today we're talking about visual odometry. And before I get further into that, I just let you know that we're recording and live streaming this lecture. And if you would be so kind, I would like two people to volunteer and give me feedback after the lecture. Who would like to volunteer this week? We have one. 

We have two. Thank you. So, after today's lecture, you should be able to choose the correct decomposition of the essential matrix, explain why the scale of the translation is unknown, explain what the perspective endpoint problem is, and to implement a simple visual odometry algorithm. But before we go into all of that, we will put ourselves in the mindset of a little, or actually kind of big, rover on Mars. So, I have my wheels and I'm driving around, I give them input, I want to move forward for one meter. 

But is that actually what's happening? What if my one set of wheels is stuck? How do I find out if that's the case and I'm just going in circles? How far have I actually moved? Is there a rock that I'm about to drive into? 

I would like to steer around that rock. Things like that are hugely important. And that is the thing that we can do with visual odometry. 

So, this is just one of the applications of it. But basically, visual odometry slam answers two questions. How is this specific camera moving through the world? Is it going forward? Is it going left? What is the trajectory it's following? And what is the shape of the world this camera is looking at? Can we estimate what the world looks like as we move through the world? And this has a plethora of applications. 

Such as a master over or the master over in your living room, a little robotic vacuum cleaner or an autonomous car or estimating the pose of a virtual reality headset without external tracking devices. So, this is widely applied as well. And then there are some synonyms. 

I'll be using the term somewhat interchangeably. So, the lecture is called visual odometry. But slam, which is an abbreviation for simultaneous localization and mapping, is very similar to this. And so, a structure for motion. And all these three things are kind of the same in that they both estimate a 3D map of the world and estimate the pose of the camera that has captured the images. 

But they just have emphasis on different parts of it. So, in structure for motion, the goal of that is not so much to estimate the camera's path but to estimate the structure of the things you're looking at, like what does the world look like. And in slam, you're more interested in the localization. 

So, they're doing very similar things because you solve both problems at the same time. And then, I say we're doing... We have a single camera, typically, that's moving through the world or master over is driving forward. But when we think of that in computer vision terminology, we rephrase it because as this camera is moving through the world, it captures a discrete set of pictures. So, it takes a picture at this location and then it moves a bit. It takes a new picture at this location and a new picture at this location. And then, to translate that into our computer vision lingo, we think of these as separate cameras located at different points in space. And they just coincidentally happen to have the same camera intrinsic parameters. So, we can estimate those once and then we have them for all of those. 

So, for the rest of the lecture, I'm not... When I say camera one and camera two, then it's actually typically the same camera but placed at different points in time, different places in space. Is that clear? 

Good. And when we do visual auditory, there's one thing we can never really find out, which is the scale of T, kind of, how big is the translation from one camera to another. And I will first give a mathematical argument for what this is, for why this is the case, and then a more heuristic argument. So, the thing we can find when we have two cameras, we can estimate their essential matrix. And the essential matrix is given by the rotation times the cross-up operator of T. And this means that if we take the true translation and multiply it onto the essential matrix from the right-hand side, then we are in effect... 

I found a piece of chalk. We are in effect computing R times Tx times T. But then this part here, this is equal to the cross-product of T with itself, because this is transforming the cross-product into a matrix. And the cross-product of a vector with itself, these two vectors are parallel, so it will always be zero, even if we apply some scale to the translation that we are multiplying onto the essential matrix. 

So, no matter how long we believe the translation is, this will always give zero. But it also means that the way that we can estimate the translation, because this was when we knew the decomposition was like this, but in reality we only had the essential matrix itself. And then we can find the translation by seeing what vector do I multiply onto the essential matrix from the right-hand side that gives zero. That is the translation. 

But this translation can be multiplied by an arbitrary scalar, and it will still give zero. So therefore we can't find out exactly how far apart these two cameras are. We can only find out the direction between them. And then a more conceptual reason, if I show you a series of images taken by different cameras, you don't actually know how big the thing you are looking at is. 

It could be a real car, or it could be a toy car. You don't know if the camera has moved one centimeter, or one meter, or one kilometer. It all depends on what the thing you're looking at is, and we can't estimate the absolute scale of the world. We can only estimate the relative scale of the world when we do visual odometry. 

So we can estimate the shape of everything, but only up to some unknown scale. Is that clear? Good. 

So, now we move on to decomposing the essential matrix. So, I have my series of images, and I just look at the first two images now. And I estimate some computing features, I match them, and I want to have a robust image. I can estimate the estimation of the relative pose. So I use Ransom on top of it, and now I can choose to estimate the fundamental matrix between these two, or if I know their intrinsic parameters, which I probably do because I calibrate this camera in advance, I can estimate the essential matrix, because the essential matrix requires knowledge of the camera's intrinsics. I want to estimate the essential matrix with Ransack, and now you should ask yourself or the person next to you to answer these two questions. 

You have about 30 seconds, and you will talk about how many degrees of freedom does the essential matrix have, and how many degrees of freedom does a single data point, that is a pair of corresponding points in camera 1 and camera 2, how many degrees of freedom does that fix? Talk to each other. Thank you. 

Okay. Do we have any ideas for how many degrees of freedoms we have in an essential matrix? Anyone would like to suggest an answer? Should we do a show of hands? Start from lowest. Any one, two, four, five. We have some fives. 

Six, seven. A majority of fives, and I agree with the five, because we have the essential matrix consists of the rotation and the translation. So we have three degrees of freedom for the rotation and the x, y, c of the translation. So you would think that there would be six, but because I can multiply the translation with an arbitrary scale, I only have five degrees of freedom. So I lose one to the translation scale. 

So I have five degrees of freedom, and then when I have a single data point, how many degrees of freedom does it fix? Let's do a show of hands for how many things. I'm going to say one, and I'm going to say two. So how many things is one? How many things is two? An overwhelming majority for two. And I'm not sure I agree. 

So let's look at it. When we have the essential matrix, we can take a point in one camera, multiply it onto the camera matrix, multiply that onto the essential matrix, multiply it onto the inverse camera matrix, and then we have the epipolar line in that camera. And when we have the epipolar line, we can only measure the distance from the line. We can't measure the x and the y coordinate. 

In other words, when I have my pair of corresponding points, so let's say it's q1 and the essential matrix and q2, I should probably transpose this one. This whole thing is equal to zero. This is the single constraint that comes out of the essential matrix. And this zero here is a scalar, it's not a vector. So for this reason, I only have a single equation each time I have a pair of corresponding 2D points in each camera. 

So I have 5 degrees of freedom, and each time I have a data point, I am able to fix one degree of freedom. Is it clear? Or any questions? So we arrive at needing 5 data points, which pairs of corresponding points, or points that we believe are corresponding in these two cameras. And for the first time in this course, I want you to estimate something and there's not a nice linear algorithm to do it. So you can't do this one with SBD. 

I'm so sorry. There is NISTAS 5-point algorithm that... I mean, you just write this equation down for all 5 points, and then you solve it, but then you end up with a tenth-degree polynomial that you have to find the roots of, and that's not exactly something we can do closed-form. So this is implemented for us in OpenCV, if we just use the built-in functions to estimate the essential matrix. 

So we're going to do that. So now I still have my camera moving along here, and I've now been able to use RANSAC to estimate the essential matrix between the image taken here and the image taken here. But I only have the essential matrix, and I would like to have the rotation and translation between this camera and this camera. 

So if this is 0, 0, 0, and this is placed at R and T, I would like to estimate those. Can we find those from the essential matrix? And the answer is, yeah, kind of. So it is an ill-posed problem to decompose the essential matrix. We get two possible rotations, either the rotation or the rotation transposed, and the sign of the translation is also unknown, because the scale that we multiply onto the translation here could also be negative, and it still gives 0. So we don't know which way the translation points, and we don't know which of the rotations to use. So there are four possible poses for the second camera, because two rotations, two translations. Question. That's more complicated to explain, because it involves going into how we actually decompose the essential matrix. 

So it's not something I'm going to cover here, but we can talk more about it if you're interested. So I check all my four possible combinations of R and T, and then I choose the one that results in most of the points I've seen being in front of the two cameras. And that sounds a bit weird, so if we look at it, here is a visualization of all the four possible combinations. So this one here is the correct one, because the point that both cameras see, this is a corresponding point, then when we have the R and T for the second camera, we can triangulate this point, find its 3D position, and then we check, is this 3D point in front of the first camera? Yes, is it in front of the second camera? Yes. 

This is another of the R and T combinations we get from the decomposition. So we check, is it in front of this one? No, is it in front of this one? 

Yes. So it's only in front of one camera, so that means that you have basic knowledge of how cameras work, they can't see 3D points that are behind them. So therefore this is just a mathematical quirk, and we can rule out this possibility. But then we do this chiralogy check for all the points that we have, and then the combination that has the most points in front of the camera is the one we choose as the true pose. Alright, so back to visual or geometry, the thing we're trying to do. We have now been able to take the first camera, we fixed this camera in translation zero, rotation identity, and then we can find the pose of the second camera. 

That's great. But how, if we now have a third camera over here, how can we estimate the pose of this camera? If we use the essential matrix trick again as we did between these two cameras, then we will again get a new arbitrarily scaled translation, because the scale of this translation is unknown, it could be a millimeter, it could be a meter, and the scale of the second translation will be unknown, it could be a millimeter, it could be a meter. 

So we can't combine these two translations if we use the essential matrix both times. So we need to do something different to go from camera 2 to camera 3. But what if we could use The translation between camera one and camera two to fix the scale of the translation we estimate from camera two to camera three. So at least the translations that we estimate will be consistent with each other. 

So we only have a single global unknown of what the unit of the world is. And the way we can do this is once we have the first two cameras, they've seen some of the same 3D points. That's how we were able to match features between them. When we know the pose of both cameras, because we just estimated it, we can triangulate every single point they've seen in 3D. So now we have 3D points for all the matching points between camera one and camera two. And then if we do some careful bookkeeping in Python, we can say, OK, of these points that I found here, which of them have I also matched to camera three? 

So the 3D points that I've seen in camera one, or the points that I've seen both in camera one and camera two and camera three. I have now estimated their 3D position from camera one and two. So now the way we find the pose of camera three is to solve the problem using the 3D points. So we take the 3D points we just computed and use those in combination with where they are in camera three, the 2D points there. And then we estimate the pose of the third camera purely from 3D points. 

And if there are no or are there any questions about that, I'll talk more about how we actually estimate a pose of camera from 3D points, but is it clear why we want to do it? Cool. Then I will give you a five minute break so you can clear your head and we meet back here half past one. 

OK. It's now time to talk about the perspective endpoint problem. So previously on computer vision, we talked about we have the 3D points and we want to estimate the pose of the camera from those. 

And that is exactly what the perspective endpoint problem is. So the problem is I would like to estimate the pose of a calibrated camera. So I know the intrinsics, the camera matrix and the distortion parameters. I would like to estimate the pose of this camera from N corresponding 3D points. So I have the 3D points and I have the 2D points in my camera. 

Finding the pose of the camera is then solving the PNP problem. And you've already done this in week four, but for an uncalibrated camera with the PST function. And it's also called camera resectioning for an uncalibrated camera. But again, there you did it for an uncalibrated camera. And when you do it for an uncalibrated camera, you need more points because you don't assume any knowledge of the camera intrinsics. And if we reduce the number of degrees of freedom by fixing the camera intrinsics, we can both get a more accurate estimate. And we can also reduce the number of iterations we need to run Ransack for. So if it's possible to have some kind of guess of the intrinsics, we always prefer to do that. 

Because kind of the naive solution to this is doing what you didn't week for. You find the projection matrix, some estimate of it at least. And then I can take my known camera matrix and multiply the inverse version of that on the left hand side from the camera matrix. And then you recall that the projection matrix is just given by the camera matrix times R and C. So if I multiply with the inverse camera matrix from the left, I should just get R and C out. And then I have my camera pose. But if I do it that way, R is likely not even a true rotation matrix because that's not enforced anywhere. And this requires way too many points. 

So it's noisy. And yeah, I wouldn't recommend doing this under any circumstances. But I would recommend that you ask yourself, how many points do I need to do this? 

So we can just talk through this one together. How many degrees of freedom does it have? Okay, so I guess it's kind of similar to what we were doing with the essential matrix. We want to find the pose of the camera. So the rotation has three degrees of freedom. The translation has three degrees of freedom. But this time we don't lose any degrees of freedom to scale because we have the known 3D points. 

So if we change the scale of T, the solution is actually wrong. So we can find the scale of the translation here. So we have six degrees of freedom. 

Free from the rotation, free from the translation. But how many degrees of freedom does a single data point fix? And then we ask ourselves, what is a data point? A data point in this case is a point correspondence, a 3D point and a 2D point. And when I have a 3D point and I have a guess of the rotation and translation, and I know the intrinsic, I can compute the projection matrix, I can project the point to the camera, and then I have an estimated 2D point. And I have the measured or the 2D point I was giving from the feature. So for each data point, I have an equation for the x coordinate of my point in the camera, and an equation for the y coordinate in the point in the camera. So each data point fixes x and y, so it fixes two degrees of freedom. So even though this problem has six degrees of freedom, I only need three data points to solve the problem. So the minimal case here is also known as P3P, because n is equal to 3 in that case, which is a separate research problem that people are interested in because it's so important for RANSAC to be able to do this many times over, very fast on hardware. And I'm not going to explain to you exactly how you solve it, but just say it's a bit like all those problems you were given in high school, but with more dimensions. Like in high school, you're like, okay, here's a triangle, and then you have like this angle, and you know the length of this side, and you know the length of this side, and I do find the rest of the numbers. And it's the same thing, but in 3D. 

So the free... Here it's the visualization, so x0, that's the camera, and then we have the vectors looking out from that. So like the direction in space out from the camera is fixed by the point, but we don't know how the camera is rotated, so we can only compute the relative angles between two points. And then these down here, those are the 3D points I've seen. So we can compute the lengths of these sides here, the pairwise distances between all 3D points, and then when I have a 2D point and my camera entrance is, I can compute which direction from the camera that corresponds to, and then you can compute the angles between these vectors, the free pairwise angles. 

And then you can draw this beautiful shape, and then you just need to estimate all of the lengths in this. And that's solving the P3P problem. And it turns out just like with the essential matrix, that you don't get a unique solution when you solve this, but you get four different possible solutions. So in practice, we choose a fourth corresponding point, or a fourth data point that we use to check amongst the four solutions I've computed, which of these four gives the lowest error with respect to this extra correspondence I include. So we say it requires 3 plus 1 point, because this plus 1 point is only used for deciding which of the four that we actually go ahead with. 

And just as with this, many algorithms exist and they're implemented in OpenCV for you, and you can drizzle a bit of ransack on top of it to make the estimation of the pose of the third camera robust. There is a question. The question is, is a data point a pair of points or one point? And a data point is a pair of points, and a data point is the thing that we're operating on in our ransack paradigm. So when we were just fitting lines in 2D, then a data point was a single 2D point, but now we're fitting camera poses, and the data points in this space is a pair of 3D and 2D points that I believe are corresponding. And it's three of these data points that I need. 

I need three point correspondences. Plus one. All right. So now we need to be able to also plot some things once we estimate them. And then it's very good to remind you of this because I guarantee that otherwise half of you would run into this problem during the exercise. So here I use kind of a similar naming scheme as I did with the homographies. I say very explicitly what this transformation matrix does. And that is, we have a camera and we always describe its pose as being R and T. But T is not the position of the camera, and R is not the orientation. Or because what R and T give us is the mapping that allows us to go from world space to the camera's coordinate system. So if you have a point here in the world, a homogeneous 3D point in the world's coordinate system, and I multiply it onto this, I now have this 3D point in the camera's coordinate system. So if I invert my transformation matrix, then I get the matrix that goes from the camera's coordinate system to the world's coordinate system. 

And that is this thing where now the rotation is inverse, and the thing out here where the translation usually is, is now minus the rotation transpose times the translation. So this means I can now say what is the position of my camera in the world's coordinate system. I can obtain that by saying what is the position of the camera in the camera's coordinate system? Well, the camera's coordinate system, the camera is at 0, 0, 0. So I can take 0, 0, 0, 1, because it's homogeneous coordinates, and multiply it onto this transformation matrix that maps from camera to world, and then I get where is the camera in the world's coordinate system, and then I get out just this guy up here. So the orientation of the camera is the transposed rotation, and the position of the camera is minus the rotation transposed times the translation. 

And I will admit that this is hugely counter-intuitive, because it's very nice to think that the position of the camera is just t. But if you plot it like that, you're not going to be joined at the results. So keep that in mind. And is it clear why we need to do this? Okay, so now we talked about how to go from camera 1 to camera 2, and camera 2 to camera 3. And then we just need to outline the whole algorithm, so we can take and use the essential or fundamental matrix, but preferably the essential to find the pose of the second camera. We triangulate freely points between the first and the second camera, and then we use PNP to estimate the pose of the third camera. And then we now know the pose of the second and the third camera. 

So now we can estimate 3D points between the second and the third camera, and then we identify which features have I seen in camera 2, 3 and 4, and then I take and find the pose of the fourth camera based on the 3D points that I triangulated between the second and the third camera. And then this process of using a rolling window of three cameras repeats, indefinitely, until you run out of images. And then thus you're able to estimate the pose of every single image. And each time you estimate the poses, you also estimate the 3D points. 

So we have actually accomplished the goal in SLAM that we also not only find the pose of the cameras, we also find information about what does the world look like in front of us. Am I about to drive into a rock? Because then I would find 3D points on the rock that I'm about to drive into. And then you can do some more things on top of that, such as finding the points that you've actually seen in a lot of cameras and then doing some optimization or bundle adjustment to make sure that these points actually project to the same point, like that the same point in 3D has a consistent estimate. 

If you've seen it in more than three images, maybe you've seen it in 20 images, then you could take that into account when you estimate the point in 3D. But that requires more bookkeeping and is more advanced to implement. So you just start with implementing up to step four. 

So as I just alluded to, if you see the same feature across a lot of frames, it makes sense to track this feature again and again and not recompute its 3D position, but maybe fix the 3D position the first time you estimated it or update your estimate, but also use the previous frames you've seen it in to reduce the amount of drift you can do. And then there are more advanced topics in SLAM, such as loop closure. And here we have a visualization of what that means. So if I'm the camera, I've gone around here, I'm the green path, and then I walk around the room. But then as I walk along this path, tiny errors can accumulate. And then as I come here, I can identify by looking at what SIF features have I seen previously. I can have some additional stuff on top of my algorithm checking against previous frames and see, ooh, the 3D points I'm seeing here, they look a lot like the 3D points I saw 300, 400 frames ago. And then I can find the pose between this frame I saw 400 frames ago and then I can recompute the poses of everything along this path and then we can find out, ooh, I'm actually back at the same point where I started, but doing this requires, again, a lot of, like, then you need to optimize along this entire path here. 

So that's something that takes a bigger software library, I would say. So today's exercise is a bigger exercise, so it's something you'll be doing both this week and next week. And next week there is no lecture. 

There was something with Trackman, but I got canceled due to scheduling reasons. So next time you just have the opportunity of looking at today's exercise or if there are any previous weeks you need to catch up on, you also have the opportunity to do that. But basically you do the steps I've outlined here. You estimate the essential matrix between camera one and camera two. You estimate the 3D points and then you use Solve PNP to find the pose of the third camera and so on and so forth. So now you should be able to choose the correct decomposition of the essential matrix. You should be able to explain why the scale of the translation is unknown. 

You should be able to explain the perspective endpoint problem and you should be able to implement a simple visual autonometry algorithm. And I've just told you this in words, but it's also in the slides. So there's no lecture next Friday at 13. 

So in two weeks from now we meet back here at 13 for the final week. But next time you just do exercises. All right. Good luck with this week and next week's exercise. Thank you. 