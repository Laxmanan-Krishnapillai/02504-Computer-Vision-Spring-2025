All right. 

Speaker 1: Yes. Wonderful. Hello. Today we're talking about the exciting topic of geometry constraint feature matching. And then we have to ask ourselves, what does that mean? But we'll get to that. But first, we're recording and live streaming this lecture. So anything you ask about could potentially appear in the recording. 

The mics aren't that good. And I would like two people to volunteer to give me feedback after the lecture. So who will that be today? We have one. 

And we have a second one. Thank you very much. So after today's lecture, you should be able to explain and implement the eight point algorithm for estimating the fundamental matrix. You should also be able to explain and implement estimating the fundamental matrix with RANSAC. 

And then as I promised when we talked about RANSAC, I'll talk a bit about how to choose the threshold for RANSAC using the chi-square distribution. So we've covered a lot of topics in this course. And now in this week, we are going to use some of the topics from previous weeks and build on top of them. 

Revolutionary. I mean, we've also done that in some previous weeks. But now we're drawing back specifically on stereo geometry that you were taught about by ANAS, I believe, in the third week. And then some kind of features we'll be using SIFT. 

And then we use RANSAC to make sense of these features, to match them correctly. So this is kind of a conceptual view of what we want to do, or like an actual view. So the thing we see here on the left, this is an image. And there is also another view of the same scene where the camera has been moved. And we've computed SIFT features in the one view and SIFT features in the other view. And what the image is showing is a little big blue dot, slightly bigger blue dot for where the feature is. 

And then a long blue line indicating the offset to the pixel coordinate in the other frame. And when we look at this image on the left, it looks like a mess. The lines are going in all different directions, because even though we've matched the SIFT features, like I described last week, so we only match them if they are the best matches to each other in one direction and the best matches to each other in the other direction, we still get a lot of false positive matches that even though the local area looked similar in the descriptive language of the SIFT feature, they shouldn't have been matched. 

And then the thing we have on the right, we have applied Ransack with the fundamental matrix, which is what I'll talk about today. And with this, we've cleaned up the matches. So here, we can see that all of the arrows on the building are kind of going in the same direction. 

And then the arrows here out on the bush, they're very long. So I mean, there are still a few outliers in here, but overall this is a much, much cleaner set of matches that we can actually use to do further processing. And actually relate the camera geometry of the two cameras that have taken these pictures and triangulate the points in 3D and all the things that we would like to be able to do. 

And we do this by removing the outlier matches with Ransack just as we did when we fitted straight lines to 2D points with Ransack. So I realized it is a while ago it was week three, approximately six weeks. So therefore, I have decided to recap what the fundamental matrix is. The fundamental matrix describes both the camera entrance six and the relation between the two cameras. So when R and T is the relative of the two cameras, rotation and relative translation between the two cameras, then we could compute the essential matrix by taking the cross-up operator of the translation, multiplying that onto the rotation that gave us the essential matrix. And the essential matrix only describes the relative post between the two cameras. And then we multiply on the inverse camera matrix of camera one from the right. And the transposed inverse camera matrix of camera two from the left onto the essential matrix. And this gives us the fundamental matrix. And what could we do with the fundamental matrix? We could do one thing and one thing only, but a very, very useful thing. 

We could say, OK, well, I guess we could do other things. But we could say that if Q2 and Q1 are a pair of corresponding points, so Q1 in camera one, Q2 in camera two, are 2D observations of the same point in 3D, then when we multiply them onto the fundamental matrix from each side as shown here, this will equal zero. So the fundamental matrix is a way of expressing that two corresponding points lie on their corresponding epipolar lines. Because the thing that's going on here is when we compute, for example, Q1 times f, like just the right part here, then what we get is the epipolar line in camera two that Q2 should lie on. And vice versa, when we compute Q2 transpose times f, we get the epipolar line in camera one that Q1 should lie on. And we check whether a point lies on a line by multiplying the point with the line. If it's zero, they lie on the line. OK. Are there any questions to this? 

All right. Then we will estimate the fundamental matrix. We did, just on the last slide, we defined the relation that Q1, Q2 multiply onto the fundamental matrix. 

Fundamental matrix should give zero. And that's kind of the only thing we know about Q1 and Q2 if they're corresponding. But now we say we have a big collection of points. It's not only Q1 and Q2, but now it's Q1i, the ife point in the first camera, and Q2i, the ife point in the second camera. And we believe these are corresponding still. And then we can multiply them onto each other. And because these are Qs, these are homogeneous 2D points that are represented with three numbers, the last is one, and they have an x and a y coordinate. And we can also write out the full fundamental matrix. 

And then we have something that looks very similar to what we did with the homography matrix when we wanted to estimate that. So we take the expression that should be equal to zero, and then we write it all out. And then we gather all the terms that belong to F in a vector and all the terms that don't belong to F in another vector. And we call that vector B. And then it turns out that the elements of B must be as follows in order for the two things to have equality. And that's just sitting down and writing it out to show this. 

And in this case, we also have, and luckily a nice formula, for computing Bi, so the B vector for the i correspondence, because it is just taking the outer product of the two 2D points and then flattening that into a vector, which you can verify by looking at the vector above. But that's mainly nice when you have to implement it in code. Wink, wink. So in order to estimate the fundamental matrix, when we have many of these point correspondences, we compute the B vector for each of them, we stack them into a tall B matrix, and then say it with me, we use the singular value decomposition to find the singular vector corresponding to the smaller singular value, which solves the minimization problem that this thing here, like, that B times the F in vector form should equal 0, subject to F having norm 1, because if we set F to norm 0, then we could just make the whole thing 0. That's the trivial solution we're not interested in. 

All right. So if we have any number of points, we can estimate the fundamental matrix in this way. Or at least any number of points big enough for this work, because this works only if we have at least eight points, because the fundamental matrix has nine numbers, and its scale invariant, that's why we can fix the norm of F to B1, and because each pair of 2D corresponding points fixes a degree of freedom in our estimation of F, then eight points plus the scale degree of freedom is enough to estimate the fundamental matrix. We call this the eight-point algorithm. But how many degrees of freedom does the fundamental matrix have? I have a strong memory that someone in week three told me that it was seven degrees of freedom. 

But why was that? Well, inside the computation of the fundamental matrix, there was the essential matrix. And inside the computation of the essential matrix, we used the cross-up operator of T. And because this matrix here, the cross-up operator of T, is rank-deficient and only has rank 2, that means F is also rank-deficient, so we know that the determinant of F is also zero. 

So this is an additional constraint that we could impose to the system. So F has only seven degrees of freedom, and we should be able to find it from only seven point correspondences. So if we take and do create the B matrix with only seven point correspondences, solve the system, then we will find two vectors solutions that both have a singular value of zero. So these two solutions are equally good. 

And then we can find a linear combination by solving for alpha, such that the linear combination of these two gives us a fundamental matrix that has determinant zero. And this is the seven point algorithm. So this is a few more steps to go through, but allows you to estimate it from seven points instead of eight points. 

But now, I think you need to think a bit and talk to your neighbor about why do we actually want to estimate this from seven points instead of eight points. Just give you 30 seconds to do this so you can kickstart your brains a bit. Thank you. Okay. You've had some time to talk with yourself. It was a very required discussion. 

So does any group have suggestions that they want to share? Or a singular person. Also welcome. Okay, so the suggestion is that it takes less computational power to do it for seven. And this is true. I believe I'll have to double check. But it's not exactly the reason we're interested in it. So many times throughout this course, I will ask you, how many degrees of freedom does this have? 

How many degrees of freedom does this have? And it's because we are interested in doing ransack. And when we apply ransack to something, we want to fit the given model to exactly the minimum number of data points required for this exact model. And why was it that we wanted to fit to the exact minimum number of points? Because in ransack, we know that we have some data points there inliers and some data points there outliers. And we need to fit a model to only inliers at some point in order for the algorithm to believe that we've succeeded in our task. And the more points we sample, the higher is our likelihood of sampling at least one outlier which renders everything useless. So if we can sample seven points at a time instead of eight, then we have a much higher probability of fitting a model to only inliers because it's more likely to sample seven points that are only inliers instead of sampling eight points that are only inliers. Is that clear? 

Okay. So we can estimate the fundamental matrix from eight points quite easily by creating the B matrix, but it's also possible to do it from just seven. And it's okay if you do it from eight in the exercise because you just need to run your code for more iterations in order to hit upon one of the times when you sample only eight inliers. But if you were doing this in production, you would use the seven-point algorithm. 

Okay. Now, we actually want to use Ransack. We need some way of measuring how a data point fits a specific model. And we can recall what we did for lines. Here we had the line in 2D, and then we had some data point. We couldn't measure the distance from the point to the line. But now we're talking about fundamental matrices and 2D point correspondences. And how do we measure these distances? 

First of all, we should probably be on the same page. What is a data point when I'm talking about fitting a fundamental matrix? I say it needs seven, has seven degrees of freedom. We need seven data points to fit it. 

What is a data point when I'm fitting a fundamental matrix? This is a question. I'm going to ask you a question for you. 

Or for me. So, at this point is a thing that I can use to compute a single B vector. So it is two d points that I believe are corresponding to each other in one camera and in the other camera. 

So I need seven data points. I need seven 2d points in camera one and seven 2d points in camera two. And I believe that they are paired in this way in order to be able to fit the fundamental matrix. And now the question that we need to answer is, given a fitted fundamental matrix and a single data point. 

So one point in camera one and one point in camera two, how do I measure how well these align with a certain fundamental matrix? That is the question we're trying to answer. And, okay, if these two points are corresponding, then what can we say? 

We know that when we multiply it together with the fundamental matrix, we get zero. So they fit the model well because they come from the model. But what happens if they are not corresponding? 

We haven't thought about this so far, so let's think about it now. So we have the first one, q2i, transpose, multiply it onto the fundamental matrix. This gives us the epipolar line of this point in camera one. So we can call this line one. 

And then likewise we can take the fundamental matrix q1i. And this gives us line two, the line in the other camera. And if we then denote the terms of the lines as a, b and c and we normalize these lines such that the first two elements of L1 and the first two elements of L2 squared and summed together have norm one, then we can measure the distance from the point of line by multiplying it onto the line. 

But we are not guaranteed that these two have norm one. So what we kind of want to measure is to take what is the distance from q1 to the epipolar line in camera one. And what is the distance from q2i to the epipolar line in camera two. So we need to compute normalized versions of these lines. So we can just call that, yeah, we have this line two i and line one i. And then we can compute if we call the elements L1, L2i, A, L2i, B and L2i, C. So now I've written out the elements of the fundamental, the epipolar line in camera two. 

So in order to compute the distance from the point in camera two to the epipolar line in camera two, I can take the product between the line in camera two, L2i, multiply it with the point in camera two, q2i. And then I need to divide this whole thing with constant such that these two squared sum to one. So I normalize by the square root of L2i, A squared plus L2i, B squared. So this whole expression here, which differs just a bit from the slide because I introduced the L2i shorthand to represent the epipolar line, measures the distance from q2 to the epipolar line in camera two. And then we do the same thing, but take q1 to get, we take q2 to get the epipolar line in camera one and measure the distance from q1 to that line. And that gives us these two distances here. So for a given fundamental matrix, we can measure the distance from the two epipolar lines in this way. And then we could just add these two distances together. And that gives us the squared symmetric epipolar distance. 

And the only thing that happened here was that in addition to summing the two together, we also squared them because we only need to use this to compare distances. So we don't actually care about the square root. And if we can avoid taking a square root, things are faster. 

We only need to compute it if it's above or below some threshold. We can square the distance, we can square the threshold. But then it turns out in practice, there is another similar looking, very related distance called Samson's distance that performs slightly better in practice. So if we contrast and compare the two formulas, here we have one fraction where we compute the normalization constant for one line and the normalization constant for the other line. But in Samson's distance, everything is just in a single fraction and we sum all of them together. 

And the only explanation I can give you is people have tried this in practice and found it performs slightly better. And this is also a square distance like the other ones, so we also need to square our thresholds. So this is the answer to the question, how can we compute the distance from a data point, that is two points that we hope are corresponding, to a given fundamental matrix. And we get our given fundamental matrix by sampling eight random data points, fitting the fundamental matrix to these, and then we can measure the distance from all data points to this fundamental matrix. Are there any questions to that part before I move on to how we can threshold our RANDSEC? Yes. So the question is what the points I'm sampling are, and I'm sampling seven data points, and at data point in this setting is a pair of a point in one image and a point in the other image that I believe are corresponding. 

So a single data point is an x-y coordinate here and an x-y coordinate here that I believe should belong together, because they have been matched by whatever feature descriptor I use, probably SIFT. Good to clear that up. Okay. So this part is not so long. Do you want a break before I go through it? You're just strong for me today. All right. How do I choose the threshold when I do RANDSEC? 

I don't know. Before we just left it as a hyper-parameter and say this is something you can choose yourself, but if we introduce some assumptions, we can use math and something will plop out of the end that will tell us this is how we could set our threshold. So if we assume that our inliers have an error that follows a normal distribution, they are normally distributed, which is a quite reasonable assumption in general. Then we need to think about what the dimensionality of the error is, and now things get a bit complicated in order to understand what I mean by the dimensionality of an error. 

So we'll go through some examples. When we're talking about fitting a fundamental matrix or an essential matrix, then the error we're measuring, the way we find out if a pair of data points fit well to our current model is by looking at, we have, here's our point in one camera, and we have this epipolar line, then we measure the distance from the point to the line. So we only measure the error in one dimension. We don't know if the point was over here, we would have the same distance to the line. So it's only orthogonal to the line that we actually measure the error, so it's a one-dimensional error that we are measuring. 

Whereas if we are fitting a homography with RANSAC, then let's say we've observed our point here, and we have a homography that fits to our data, so we can take the point in camera two and map it to camera one, and then whoop, it arrives here in the image. And then we can measure what is the distance between these two points, but now we're not only measuring the distance between the points, but we can measure what is the distance in X and what is the distance in Y. So we are measuring a two-dimensional error, because we know not only the distance between the points, but actually the distance in X and the distance in Y, so it's more specific. Because our assumption is that the error of inliers is normally distributed, so we are assuming that there's a normal distribution on the X coordinate and a normal distribution on the Y coordinate, which is why we need to think about this dimensionality of the error. And again, if we're doing post-estimation or camera calibration, then we also have from our well-known checkerboard, this projects to a specific 2D point, we have a specific observation of this 2D point, and we can measure both the X and the Y error. And again, in the case with the epipolar lines, we can't do this. We only know how far we are away from the line. We can never know the error in any other dimension. 

Is this anything you would like to ask about? Okay, then we just introduce a new term and call this the co-dimension of the problem, instead of calling it the dimensionality of the error. Now we call it M, and this is the co-dimension of the problem. So, now we assume that inliers are normally distributed and the error of each sample will thus follow an M-dimensional normal distribution with a standard deviation of sigma. And when we're working with a squared error, so we don't need to take square roots, then the summed and squared error follows a chi-square distribution. By definition, because the chi-square distribution is literally what if we have an M-dimensional vector of the random standard gaussians or gaussians with sigma, what is then the norm of this vector? So what is the norm of the error? That's what the chi-square distribution describes for us. And for this reason, we also always work with squared distances because then we don't need to take the square root. 

So, now I just need to choose... Actually, I introduced one assumption and then we need to choose something else. So I need to choose my standard deviation, but my standard deviation is more interpretable because I can say, okay, where did my points come from, where they clicked by a human annotator, what is the standard deviation of how humans click it or how will the sift perform, that's something I can measure. And then I choose a confidence level and say, how certain do I want to be that I find a true-in liar? So if I say 95%, then I'm asking the question, please give me a threshold such that if I apply this threshold, it will identify 95% of true-in liars. And then I look up in some table or use R, I don't know what the kids do nowadays. 

And then this... When I looked up in my CDF table for the Christchurch distribution with my given code dimension and my chosen confidence interval, outcomes and number, such as 3.84. And then I say, my squared threshold for estimating a fundamental matrix, is 3.84 times the squared standard deviation of the 2D points. And where did the 3.84 come from? That comes from some table I can look up, where I've shown you here some useful values. So you have your confidence interval or how confident you want to be here and then the code dimension of the problem here. And then when you know these, you can look up and then you get a number that you can plug in to your system. So now you just need to, instead of choose an arbitrary threshold, which can be very difficult to decide and is more arbitrary for these abstract problems of a homography matrix or a fundamental matrix, we can now translate this into, what is the standard deviation of my points? 

Yes, we have a question. Now this is my beautiful notation stating that it's m in the columns here and it's 1 minus p in the headers. So 1 minus p is just the p value of the distribution and then this is 1 minus p because it's our confidence threshold. It's like p.05 is 95%. 

There's, yes. So the question is whether you should take the square root of this threshold and the answer is no. We always work with the squared distance because when we measure distances here, then in order to measure the distance between these two points, then we have the difference and next we have the difference in y. 

We square them, we add them together and then we could take the square root to get the actual Euclidean distance but it's faster to not do the square root, we just have the squared Euclidean distance and therefore all of our math just computes what is the squared threshold that we use together with the squared Euclidean distance or the squared symmetrical distance like I showed a few slides before. All right. Now you should be able to explain and implement the eight-point algorithm for estimating a fundamental matrix and explain and implement estimating the fundamental matrix with RANSAC and you should be able to choose the threshold for RANSAC with the chi-square distribution. And I wish you good luck at the exercise and if you're a feedback person, please come down here. Thank you. You're too kind. 