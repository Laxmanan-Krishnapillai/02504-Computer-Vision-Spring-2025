Speaker 1: All right. Welcome to the worst full screen looking slide you've ever seen with a weird bar on the top, where we're just going to roll with it. Today we're talking about image stitching. It's a sound tool. Okay. So you can watch this lecture on the internet if you want to. And I would like two people to talk to me after lecture to just give me some feedback. 

Thank you. And after today's lecture, you should be able to explain and implement RANDSAC to fit homographies on images. And you should be able to understand the challenges involved in stitching panoramas. So again, like last week, we're going to take some previous topics of the course and then build on top of them to be able to do something new. And the new thing we're doing today is also kind of similar to the thing we did last week. So last week, we took the fundamental matrix and used RANDSAC to fit these on images by finding SIF features in both images and then matching and finding only those that were consistent with a certain fundamental matrix. 

But this week, we're going to do the same thing but with homographies. So find SIF images in the two features. SIF features in the two images match those. 

Use RANDSAC with a homography to find the homography of the best maps between these two images. And this allows us to do panorama stitching or image stitching, which is kind of this thing. So we have some camera and then I take an image here and then I rotate my camera. 

I don't translate it. And then I take a different image and then I can do all these things and then map the two images together and get a wider view than in just a single image. And then, I mean, we are drawing on previous cost material. So what is this previous cost material actually? What was it that these homographies were doing? 

What does a homography actually describe? Does anyone here recall that? And would like to share it? Yes? Yes. So it's a different point in one camera. We can know exactly where that point is in the other camera if we assume that the two cameras are looking at some kind of plane. 

It could be the wall over here or the blackboard down here. As long as we know the two cameras are looking at the same plane and we have the homography, then given a point in one camera, we can find the exact point in the other camera. But I just showed you a picture of some mountains on the previous slide. And as far as I am aware, mountains are not a plane. So why are we using a homography at all? Because this is not what I have taught you. 

I am saying we can use a homography to do this. But why? So this is kind of different to think about. But if we imagine that the world is kind of put on a... 

It's not the real world. It's just a picture of the world on a painting really, really far away or really close to me. But as long as I only have one eye, like a camera, and I don't move my eye from left to right, I can't distinguish whether I am looking at the real world that has depth and everything or if I am looking at a picture of the world that is completely flat and placed in some distance so it looks exactly like the world I am seeing. 

So we can assume that the world is showed on a plane and as long as we don't move the camera from left to right, this assumption allows us to model the image stitching problem with a homography. Does that make sense? Or you have to think about it a bit. But yeah, the key assumption is that the camera doesn't move and it's only rotation between the first image and the second image. So what if we are naughty and break this assumption? 

Then bad things happen because our assumption is no longer true. But actually if we just go to my desktop background, this is an image I took a while ago with my drone and I wasn't allowed to fly high enough to actually take this picture so I cheated and actually moved the drone around from left to right and I took four pictures and assumed that it was only rotating and then I used image stitching to do it. So if you look at some details in this image, then you see that the perspective of things have changed because the camera has been translated. So for example, if you look at this mast of this sailboat up here, it goes to the left, but the mast of this sailboat here goes in this direction and this one goes up because it's put together of four different images and it's not actually like one single image taken with true rotation. So if the camera moves, maybe if it just moves a little bit, you can still find the points, you can still find the matches, you can still stitch the images together, you're just going to have errors in the parts that have differing depths because you introduce some kind of perspective deformation when you move the camera. 

Okay. And in order for us to do Rensack, then just as last time, we need some way of measuring. When I have a pair of matching points, if or my points, they are actually an inliers and they are true, inliers, there's no noise, the error will be zero. But what happens if something is not matching? 

How can I measure how big the error is for a given homography? So I'm just going to find a piece of chalk. Beep, beep, beep. 

Very short, very nice. Okay. So just to keep a bit in mind, we have Q1, Q1. And this is just a normal 2D point. So the Q's, they are 2D. And then we have the P's. So the P. And that is still a 2D point, but homogeneous. 

Yes. Because we can apply the P points directly to the homography matrix. So that must mean that because the homography is three times three, P1 and P2 are also three-dimensional vectors. So they're the 2D points and homogeneous coordinates. And then by taking the Pi inverse function, we can convert from the inhomogeneous points to the homogeneous points. And again, if these two points map exactly to each other, we can take the inhomogeneous point in camera 2, put that through Pi inverse, which gives us the homogeneous point in camera 2. Then we multiply that onto the homography that gives us the homogeneous point in camera 1. And we put it through the Pi function to get the inhomogeneous point in camera 1. So there's no noise and we can map directly from Q2 to Q1 using this equation. Is that clear? 

Okay. But now we enter the real world where there's noise on everything. And in this case, it's the real world supported by some math. So we know exactly what the noise is. It's called epsilon. So in order to get the noisy point Q2, we call this Q tilde 2. And that's what we get by adding the exact amount of noise, epsilon 2 to Q2, and vice versa for Q1. And then we have the noisy versions of the homogeneous points as well by putting them through the Pi inverse function. So now we can simply measure what is the difference between the point we observe in both images. This gives us the error. So we can take in a lot of things going on here. So let's just write the same thing here. We have Q1 till this is our observed point. We know this. 

But we would actually like to take our Q2. I should put some dashes through them so they don't look like nines. If we take this and convert it to inhomogeneous coordinates with the Pi inverse function, we can map it onto the homography. And then we can take the Pi function of all of this to convert it back to an inhomogeneous point in camera 1. 

And then we can subtract these two from each other. So this has no noise on it. And we map it to camera 1. Then we compute the distance to the noisy version of the first point. And then we measure how far are these apart with the two norm. And then we just add also what the error was in camera 2. 

So the difference between Q2 tilde and Q2. Yeah, it's squared distances because we're always working with squared distances because it's faster to compute. And then this thing is the whole thing we need to measure. But can we actually measure this thing? 

No. Because we don't know what Q2 is because if we knew what the noise was, that had been added to go. We only know Q2 tilde. We only have the noisy observation. We don't know actually what the ground truth point is. So we can't take the ground truth point and map this to the other image. And I mean just to be clear, this whole thing here, that's just equal to Q1 without any noise. So the thing, it seems like this didn't really bring us anywhere. 

But it brought us to a realization that we can do something else instead. We can formulate this as an optimization problem. So because Q2 is unknown without noise, but we have the noisy version, we can minimize this entire expression here subject to or we can minimize Q2 such that we've, this whole thing is as small as possible. So we can estimate where is actually the true Q2 and because we have a direct mapping from Q2 to Q1 with this optimization problem, we're actually optimizing where is the optimal placement of the point that has the smallest distance to the two places we've observed the point. So this would be the ideal way to do it with this optimization problem. 

But it's a bit slow to need to compute an optimization problem every time I want to measure how far a given point is away from my homography. So we do an approximation instead. And this approximation is a bit simpler. So we, and here I cheat and reuse some of the previous formulations. 

So we take now P2 tilde. So this is the noisy point in camera two in inhomogeneous coordinates. So we can multiply that bad boy directly onto the homography and then use the pi function to get back into inhomogeneous coordinates. And then we measure how far is the noisy point when we map that to camera one. How far is that from the actual noisy point in camera one? 

So in this way we're not actually measuring the magnitude of epsilon as we would like to, but we're approximating it in a very tractable way. Is there, was that a question? Yes. I don't understand. So right there, P-2 is homogenous for inhomogeneous coordinates. Oh, I did it. 

So the, so P is a 2D homogenous, what? No, sorry, sorry. Yeah, this should have been Q. Was that the question? Yes, so this gives us where, if this was the true point, where would that point be in camera one? 

And then we do the same thing, but the other way around. So we take the true point or the, the noisy observed point in camera two. So the, the noisy was the tilde and then subtract that from where would the point from camera one map to the noisy point from camera one, where does that map to in camera two when we multiply it onto the homography? But this time we invert the homography because we're going from camera one to two instead of from camera two to one. Do I have enough parentheses? 

Yeah, it seems like I just need to finish my norm. So this whole thing is basically saying we don't have the true points, we only have them with noise, but we take the noisy point in image one map to image two, see how far it is from the noisy point there and take noise point in image one map that's famous too, see how far it is from the noisy point there. And this is the distance that we didn't use to measure for a given data point. 

The same trigger word I used last week. How far is a given homography? How well does a given homography fit this data point and a data point is a pair of two 2D points that we believe are corresponding? 

So this is the ingredient you need in order to be able to fit a homography with ransack. There's a question. So I'm actually saying two one and two three. Yes. 

All right. Yeah, and you already did the fitting of the homographies. And I mean I could just like how many how many degrees of freedom does a homography have? 

I can't believe I forgot to ask my favorite question. How many degrees of freedom does it have? I'm hearing six or eight? Nine? Any higher? 

Okay, there's some suggestions all over the place. So, the homography has nine numbers, so we can't go higher than nine. But the homography is also scaling variant because it takes a homogeneous point on one side and a homogeneous point on the other side, so I can multiply the homography by an arbitrary scalar like 75, and it's still the same homography. So we lose a degree of freedom to scale, but we can't lose any further degrees of freedom. So any eight randomly chosen numbers will still be valid, or any nine randomly chosen numbers that I normalize to a certain thing will still be a valid homography. So it has eight degrees of freedom. And that means because one 2D point in one image and one 2D point in the other image, so two 2D points, I believe are corresponding, are a single data point when I fit a homography. 

Or are they? Or can I actually with a homography say when I have a point in one image, I know the x and the y coordinate, and I multiply it onto the homography, and then I'm told where is this point in the other image. So I don't only have one equation, I have a coordinate for what the x is in the other image and what the y is in the other image. So a single data point here, a pair of corresponding points, fix two degrees of freedom because I have two equations. I have the x equation and the y equation. 

So I only need four corresponding points to fit a homography because that gives me two times four is the eight degrees of freedom I need. Is that clear? Okay. 

Yes. And now we have a homography and we need to do something with it to actually make the mountains move around. Let's say for example these two mountains. So we find our features, we match the features, and then here I show with a horizontal line if I have found two features to be an inlier data point. So these are the features I'm left with and we see that they all pretty much go in the same direction. So it's likely a correct homography that I've estimated here. And the way that we then apply a homography is very similar with what you did in week two when understanding images. So if you were confused back then, feel free to ask me or the TA's during the exercise on exactly what the details of that were. 

But it's the same principle. So we can use the homography to warp an image to a current field of view and we do it in the same way. We generate all x, y coordinates for all pixels in the image that we want to map it to. And then for each x, y coordinate I can map this to the image I want to warp to my current one. I can find out where is this x, y coordinate in my reference image and then I can compute the value at that point after mapping it in the image I actually have. And then I use some kind of bilinear interpolation to compute the value at every position. And when I have the value, then I put that value at that pixel location in my transformed image. 

And there is code for this provided. But I still expect you to understand what the code does. Or whether any questions before I move on. I'll be there for the first part of the exercise in case. 

Okay. So now I need to decide in which area I evaluate the homography. And like, I mean, I have my big, I have kind of one image here and my other image here. And then I need to figure out, like, this is my reference frame, this image, then I need to find out where, which x, y coordinate should I evaluate this image in order to get all of this image mapped to my reference image. So I could just make a really, really big canvas kind of, or a non-py array that contains all coordinates that encloses all of this and then map all images to this here. But that's kind of wasteful computation because I'm computing a lot of values that are just zeros. Like, when I'm warping this image, then everything that's not that image will be zero, which is like a very big part of the image. 

But it's very simple to implement. So the nicer and spotter thing to do is then to compute like a bounding box around the image that you want to warp, which you can do because you know the corners in the image coordinate system, that's just the size of the image. And then you can, with the homography or the inverse homography, depending on how you've defined them, you can find out where those four corners are in your reference image, the canvas where you're trying to map all the images to. And from those four corners, you can find the bounding box. And then you can save computing all of these zeros around here by only computing this square part of the image. 

And if you're doing a big panorama, then the more images you have, the more there's zeros, so the more this is necessary in order to actually get the code to work for much grande panoramas. Any questions? All right. So, here is an illustration of what I've just talked about. Would have been nice to show you. 

But here I've just done the same thing I said before. I just make a big bounding box around everything and warp everything to the same reference frame. So, here we have a lot of the pixels that are zero in each image. But then I can kind of add these two images on top of each other or take the average of the two images excluding the areas that are zero. And exactly how do I composite the image on top of each other? That's something you could spend also a lot of time on and doing that really optimally, which I will not go really that much into. But the easiest way is just to plop the most recent image on top of each other, so you just override if there's a new value here. You could take the average. 

He is just plopping them on top of each other. You could take the average like this. You could take the median. People are moving around so the average has ghosts of people if they were there in one frame not in the other one. The median helps a bit with that. But if you want to do something really nice, you can do a graph cut, which is a term from graph theory that you can apply to solve a macro random field that actually enables you to find how should I splice these images together such that the resulting difference between the images is the least apparent. The graph cut allows you to do that. 

But that's all I'm going to say about it. It's something you can go further in depth with is this excite shield. And if you have multiple images like I show here, I've taken a lot of images of this clip here and stitched them all together. And then the things illustrated here in red, like the figure, the line, the better these two images are connected, the more inliers I found between them. And then you can kind of construct a path from anywhere in this or choose one of these as your reference and then multiply homographies together in order to make a big homography that can go from anywhere to anywhere. 

And I should explain what I mean by that with this slide. So if we just think of the case where I have free images, then I do the ransack between image one and image two. I do the ransack between image two and image three. So I now get two homographies. One that I can use to map from image two to image one and one that I can use to map from image three to image two. So if I take my point in image two, I can multiply it onto this homography to get the point in image one. 

And I should delete this one. If I take the point in image three, I can multiply it onto the homography going from image three to image two and get the point in image two. And then I can combine these two equations and actually say, okay, this P two, that's the same as I have up here. So I can actually take P three, multiply that onto the two homography, then onto the two to one homography. And then I can pre-multiply these two homographies together because two three by three matrices multiplied with each other is a new three by three matrix and a new homography. So the product of these two homographies gives me the homography that takes a point in image three and maps it all the way to image one. 

And you can extend this principle as far as you need to for the number of images you have. And I should also say that this way of talking about homographies really helps you debugging when you're implementing these things. So if you give your homographies informative names and say age two to one, this tells you very explicitly in the code that I expect this homography to map points from image two to image one. And if you call them these explicit names, then you also have a very nice explicit name for the inverse of this homography because that's age one to two. And then if you're in this situation over here when you're trying to see, okay, which homography should I actually use to take the corners of the image two and map it to image one, then it's much more apparent when you give them human readable names. So I strongly recommend coding in a style similar to this. And then you can choose one image to be the base reference and you can just choose one image to be the center. And then you can also do joint optimization because in this graph here, if you just do the thing I said with products of homographies, then you have multiple ways of going from, let's say, if I choose this as my center, I can get the homography from this image to this image by either going up here and then up here or over here and over here or over here, here, here. So if I compute all of these, I will get slightly differing estimates. And the best thing would be to have to solve it as an optimization problem and have estimate with optimization the homography, the maps from this specific image to my canvas. Yes. 

And then you can impose the constraint that the homography estimated from this one to this one should be consistent with all the homographies you've estimated or should be as close as possible to it. And I'm happy to discuss more about this if you're curious. Yes. And then we are nearing the end of another quick lecture, but my final comments are about something that I've often wondered because I say the whole principle of this is that we don't translate the camera. We do pure rotations. So when I look at this, my illustration of the pinhole camera, which point should I rotate around? Is that the, let's just do a show of hands. Should it be the image sensor? 

Who thinks that? Should it be the pinhole? We have one vote for pinhole and zero votes for image sensor. So I, and I agree. 

Thank you. We should rotate around the pinhole because the pinhole is where all of the rays of our cameras intersect and it's only, it's actually the pinhole that the rotation and translation that we're talking about when we model a camera is modeled with respect to. But then what if my camera doesn't look like a sketch drawing, but it looks like this? Where is the pinhole actually? 

And that's a good question because it turns out it kind of depends. It can even actually be a point that's outside of the physical camera, like behind it, because the lenses inside the camera use glass to bend light and the camera itself is not a physical pinhole camera. So it's, it's something that if you want to get really a detail oriented about this, you need to actually try to rotate around different points and then find the one that minimizes the shift in perspective of the world. And you can get like special tripod heads that you can use to rotate around the specific point that you calibrate. 

But that's more in the nice to know area. Yes, and I already gave you this demo of the, of the desktop background. So I'm going to skip that and just say that there are a lot of things that I have not covered today because here you just, today you get to estimate homographies using a degree of freedom. But if we've calibrated the camera or then we have a pretty good idea of the principle point and the focal length and because the homography is computed from a rotation and a trend rotation and the camera matrices, if we know the focal length, we can reduce the degrees of freedom that we need to estimate the homography and thus when we need fewer degrees of freedom, we can sample fewer points and we can run ransack in fewer iterations, which will make our code significantly faster. And you can use other projection models if you kind of want to make the resulting image be able to cover more than 180 degrees. And you can do some kind of correction if the camera has changed the exposure between the different pictures you need to compensate for that. 

But that's beyond the scope. But for now, you should be able to explain and implement ransack to fit homographies and you should be able to understand the challenges that are involved in stitching panoramas. And are there any final questions before I send you to the exercise? Very well. Then enjoy today's exercise and thank you so much for listening. 