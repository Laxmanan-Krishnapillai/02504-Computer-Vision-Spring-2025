All right. Hello. Welcome back. It's yet another week. 

It's almost over. And we're here for our favorite part of it, the computer vision lecture. But first, I would like to jump to the DTU Learn page and just remind you that we have these weekly quizzes where each week I have two related exam questions based on the content that I'm going to be sharing. And if you answer those, then I update this leaderboard once a week and you can see how you're doing in relation to the other students. And you can still, if you want to, go in and answer the questions to the previous weeks and then I'll update for those as well if you don't see your name on here and that you would like to. But with that said, I will now jump into the lecture. 

And today we're talking about blobs and sift features. And we are, of course, recording and live streaming this. People are watching on the internet. And I would like two people to volunteer to give me some informal feedback after the lecture. 

Who will it be today? Thank you. Very good. So after today's lecture, you should be able to implement and use blob detection by using difference of questions. And you should be able to analyze and use sift features and feature matching. So those are the two things we talk about. 

And the way these two things are related is that sift is based on blob detection. So that's why we have these two things together. So what's the whole idea? 

Why are we here? We were talking about these images. I promised you a few weeks ago that I would say we have better methods of matching points between different images. So if I have this image on the left of a mountain and this image on the right on the mountain, then I want to be able to find some points at the points I find by detecting blobs. 

And I want to describe the local appearance around the point. And I do that with the sift descriptor, which will be covered today. So we need to kind of be invariant to all these different things. We need to be invariant to scale. So if I move the camera further away, I should still be able to find the same points. 

I need to be invariant to rotation if I rotate the camera and also somewhat to appearance changes if I change the perspective of the camera. But it turns out that we actually obtained this with sift. Some of it we specifically designed for that we are actually invariant towards rotation and scale. 

And then invariant towards appearance, that's just something that kind of arises from the way it's designed, but not something that we can like guarantee. And sift is by now, I guess relatively old, from 99 that it was iterated on a few times, 2003 and so on. But before this time, there was not really anything that worked well to match different points in different images and identify the same point. And then sift came onto the scene and really changed the landscape. That was the first time we kind of were able to deliver on some kind of promise of computer vision actually working well for real problems. And it's based on this scale space blob detection, difference of options that I'll be talking about in the first part. And then the first part of sift is that it detects blobs, like detects interest points, where in the image do we think there are points we can uniquely find again in another image. Then it finds out what is the orientation of each of these points. In this way we make it rotation and variant because we explicitly estimate the rotation of it. And then we describe the appearance of the area around each point. And sift is just one way of doing this. 

It was the first that worked kind of well and was popular, but others have come since that I also mentioned briefly at the end of the lecture. So if we recall a few weeks ago we were working with these Harish corners and Harish corners were these points in the image, these corners, where we say, okay, there's a big change in intensity in no matter which direction I go in, then the image intensity changes. And they are local and we can find them at kind of different scales by modifying the parameters that we blur the image with. We had these different scales of the gautions that we needed to blur the image with to detect the corners. And depending on what sigmas we set to these kernels, we detect the corners at different scales. 

And there are also rotation and variant. And Harish corners is kind of this, we look at the first order derivatives of the image, but blobs, they are kind of the second order derivatives of the image that we look at. So that's a bit fluffy and what is a blob even? 

A blob is this acronym for a binary large object. And you can think of it as a dark area in the image. So let's say my face is a bright area and it's surrounded by darker intensities on the blackboard. So if I stand in front of the blackboard, my face is a blob. And conversely, we also have dark areas surrounded by brighter intensities. So there's dark blobs and bright blobs, but we care about both of them. 

Okay, now we need to visit some math. The Hessian matrix. The Hessian is a matrix that contains all second order derivatives. So in this case, it's the Hessian matrix for a specific point in the image, namely the point x, y. So the first component of the Hessian matrix is if I take the derivative two times in the x direction. And the off diagonal elements is when I take the derivative in the x direction, then in the y direction. 

And this one is when I take two times in the y direction. And that's just, I describe what the notation means down here. So this is a collection of all our second order derivatives. And these second order derivatives measure kind of the curvature of the image. So if I have something bright, something that's higher up, if we think of the image, the intensity has kind of height. 

And the image has then a surface, something bright surrounded by something dark means that it's like a local hill in the landscape. And by looking at the second order derivatives, those are kind of the curvature. So at the top of the hill, it's curving downwards in all directions. 

Whereas if you're in a hole, like a dark blob, so it's something dark surrounded by something bright, then no matter where you look in the center of the hole, it's curving upwards towards the outside. And like we did with Harris corners, if we look at the eigenvalues of the Hessian, we can measure the principal curvature. So like what is the direction in which it's curving the most and the directional function also that. And the eigenvector corresponding to the largest eigenvalue, that is this lambda one, that is the principal direction, or is the how much it curves in the direction it's curving the most, i.e. the direction of most change. And then the second eigenvector is orthogonal to that, and the second eigenvalue measures thus how much does it curve in the other direction. 

So we can use these measures just as we did with the Harris corners sector. We can compute the determinant, we can compute the trace. And the determinant of these two, that's called the Gaussian curvature, and the trace of those is called the Laplacian. 

So we just sum the two principal curvatures, so how much does it curve in total at this point. And the Laplacian is the thing we use for blob detection. And if that's a bit abstract, we can also simply visualize what does the Laplacian look like in 1D. So if we have a one-dimensional signal, this is the filter you have to convolve your image or signal width in order to measure the Laplacian. So if I have my image and I convolve it with this Laplacian, then I measure, or I convolve it with this filter, then I have computed the Laplacian. 

And this is also called the Mexican Hap filter, because if you turn it around, it looks like that. And now we have a completely unrelated slide, but I have now drawn two Gaussians that have slightly different standard deviations. So one Gaussian here has standard deviation sigma, and the other Gaussian has standard deviation k times sigma. And now I decide I like these Gaussians and I subtract them from each other. So the thing I show here in blue, that was the Mexican hat, the Laplacian, and the thing I show in orange, that is what happens if I subtract the two Gaussians from each other. And the thing I want you to see here is that they kind of look similar. They're not the same, but it's the same thing that will give a response of this filter. And that means that instead of computing the Laplacian itself, we can approximate the Laplacian with a difference of Gaussian. 

And if I just explain how we do that, then I will explain why we do that. So that means I would like to compute the Laplacian of my image. I can approximate this as the difference of Gaussians. The difference of Gaussians is given by taking my one Gaussian kernel with k times sigma standard deviation, subtracting it from the other Gaussian kernel with sigma standard deviation, and convolving the image with this filter. But because of the properties of convolution, I can also multiply or take the convolution inside the parentheses and actually convolve the image with a Gaussian filter with k times sigma. And this other thing I convolve the image with, the Gaussian filter with just sigma. 

And then I compute the difference between the image blurred with two different Gaussians. And the reason for this is then when I need to compute the Laplacian, I just showed it to you before in 1D. But in 1D it looks nice, but in 2D it becomes this two-dimensional Mexican hat, but it is not separable. So we cannot do this thing we did with the Harris corners where we had the Gaussians and we used the separability, so we only convolved the image with one-dimensional kernels all the time. 

Because we can compute the image where we blurred with a 2D Gaussian by applying one Gaussian and another one-D Gaussian because the Gaussian is separable. But the Laplacian is not separable. The difference of Gaussians kernel itself is not separable. But when we separate it like this, we can do this operation by applying one-D Gaussian and another one-D Gaussian to blur the image. 

We can do this with two 1D Gaussians. So this means that we don't have something that grows with the square of the size of the filter, but it's actually linear in the filter size, which is tremendously faster when we have big images. So therefore we can now have a dog that is detecting some blobs. 

This leads to a question. Okay, so the question is why we can do this and whether we have to assume that the distribution is Gaussian, but we are not assuming that the distribution is Gaussian. The approximation itself is only what is the difference between the blue and the orange here. That shows the quality of our approximation because we are actually computing the response to the filter in orange here, but we would like to compute the response to the filter in blue. But they're close enough because the orange filter is so much more faster that we get that one instead of the blue one. But we're not actually assuming anything about whether the image follows Gaussian distribution. It's just that we're applying Gaussian blur to the image and then subtracting two versions of the image that have been blurred by different amounts from each other in order to obtain an image that has been convolved with the orange kernel. 

Good question. So, I mentioned also that we need to be invariant towards scale. So if we have an image that's taken from far away and we have an image that's taken closer up, we want to be able to find the same points. So when we're looking for points in our image, we don't only detect an x-y coordinate, we also detect an x-y coordinate and kind of how big is this blob. 

Are we talking about something that's two pixels wide or is it a hundred pixels wide? The feature that the SIFT detector has seen at this point. So in order to obtain scaling variance, we need to look at the image at different scales. And the way we did with the Harris corners, as I said before, we had these parameters that go into the Gaussian we're blurring the image with. And if we make these bigger or make these smaller, we can look at corners that exist on a very big scale or exist on a very small scale. 

And the same principle applies to plop detection. So we have our difference of cautions at the coordinate x, y with the standard deviation of sigma, which we get by subtracting our two Gaussian kernels from each other and convolving them with the image. That's not what we actually do because this kernel here is non separable, so we multiply it into the parentheses again and only convolve the image with the blurred kernel. And we blur the image and we blur the image with another sigma. And then we subtract these two images from each other. 

And then we introduce a shorthand here. We introduce L to determine the image that has been blurred with a kernel of this size. So the way we compute the difference of cautions here is by taking the image blurred with sigma at x, y and subtract the image blurred with k times sigma at x, y. That's what this L does. That just represents the image with different amounts of blur applied to it. And we have d that represents our actual difference of cautions. And then we compute a dog pyramid or a whole scale space of dogs. So we increase sigma a bit and we increase sigma and we increase sigma. We increase sigma by k each time and then we compute more and more images and we obtain a whole stack of these, which kind of looks like this. On the left we have a conceptual example on the right. 

We have some real values. The only difference is that here the scale goes up and here the scale goes down, but don't worry about that. So we can see in this way the things in the L, the images. Here it's a picture of a watch and as we go down we have sigma here and then we have... Can you even read these letters? 

Do you need new classes? It's fine because I can just tell you that it says k times sigma and then you will ignore that you can read that it says s when you download the PDF. So the first image here is convolved by sigma divided by k and then the next image, this is blurred with sigma, this is blurred with k times sigma, this is blurred with k squared times sigma, this is k to the power 3 sigma, this is k to the power 4 sigma. So we just increase the amount of blur we apply to the image to obtain this pyramid L. And then for each pair of images in our stack of blurred images we subtract them from each other and obtain a dog image, a difference of Gaussian's image. 

And we... Thus when we have a stack of these, each image can be used to compute two different dogs. And then we obtain a stack of dog images where a high or low response will correspond to a dark blob or a bright blob. So we're using the dog images to detect these blobs that is a dark area surrounded by something brighter. 

And that's also just the same thing visualized over here but with just grids instead of actual values. Are there any questions at this point? Tremendous. Okay. 

When I... oh, yes. So your question is what you should look for in the dog images. So the thing we obtain from the blurred images is just more and more blurred images. But when we look at the dog images we can look for local maxima or minima. So let's say what do we have here? We kind of have some... like each of these ticks on the... what are the hour markings on the watch. When I look at the image down here then for example this one, this is a local... can I zoom into this? 

We don't have the technology. But one of these hour mark ticks becomes a local... a local minima, local maxima in the image. So and then the point at which this is locally maximum, there we will have detected a dark blob. So one of these hour mark ticks on the watch is a dark blob and these become visible when we look in the dog space. 

But we talk more about exactly what we do with the dog images in just a little while. But before that I talk a bit more about how we compute them efficiently. So when I have two gaussians and they like each other very much and I convolve them with each other, then I obtain a new gaussian. And this means I could generate my scale space by instead of taking the sharp image and then blurring it with a gaussian that has k to the power of 25 times sigma and blurring the original image with this. I can actually take the image I just blurred and blur that a little bit more and apply a smaller gaussian to this and just blur it the right amount in order to get to k to the power of 25 times sigma that I wanted to. 

But still the new kernel that I need to blur with the size of that grows ever so slightly still. So if I choose as sift does k to be the cube root of 2, so to the power of a third, then after I have done this three times. So I multiply by sigma k k squared k to the power of 3 then k to the power of 3 is equal to 2 sigma. So after three blurrings I have doubled sigma. And after I've doubled sigma things have become pretty blurred out. I don't see many sharp details because everything is blurry. So sift does a computational trick and then it down samples the entire image once everything has become so blurry. So I don't need to store the full image anymore. And once I have then down sampled the image I can use the same gaussians that I used to do these free blurrings in order to blur the image again and then I can down sample the image again. 

So in this way the scale space of the gaussians is not only there not only stacks on top of each other but once I've done this three times I actually reduce the resolution and then it becomes a dog pyramid. But this implementational detail you don't need to worry about in the exercise today. But it's visualized here that we can compute. So this is our image just blurred with sigma k times sigma k squared k to the power of 3 a k to sigma. 

And then we need also this k to the power of 4 or k times 2 sigma in order to compute these four different gaussians. And then when we have this we can down sample everything and blur further from this image and then go deep deep into the dog space. Yes. Yes. 

Yes. So when we can take this image here that's been blurred with 2 sigma. And then we down sample that by a factor of 2 and that gives us the starting image here. And this image here has been blurred by k times 2 sigma or k to the power 4. So we can down sample that and then we already have this image in the pyramid. 

Because this image is just blurred by k more than this one. So when we computed both these two images we then sampled those and those give us the two next images for the dog stack or for the for the gaussian stack of images. And then the way that we actually detect blobs is we have or a lot of gaussian images stacked in the on top of each other. We have a lot of difference of gaussian images and in order for something to be a blob it has to be a local extremum in the dog space. So either a local maxima or a local minima. And what is a local maximum or a local minimum. 

That is just for this value here in dog space. Then I have to look at my eight neighbors in this difference of gaussian image and say am I greater than or less than all of them. Let's say I'm looking for a local maximum so I need to be greater than all of my eight neighbors in the same image. But then we also look at the difference of gaussian above and say am I also greater than these nine values in the image above me. 

And I'm also greater than these nine values in the image below me. And this is extremely similar to what you did with Harris corners except here you were just doing non maximum suppression in two dimensions. But now you're just going to be doing non maximum suppression in three dimensions. So we find the blobs by doing non maximum suppression to look for these local minima or local maxima in the three dimensional dog volume. 

That is how we detect the blobs with the same same concept as we use for Harris corners. Is that clear or questions. Yes. 

I'm so glad you asked. So the question is do we get smaller values in the difference of gaussian images when we increase sigma more. And it turns out that you can. If you did it with the leplation, then yes, then you would need to compensate for higher and higher values of sigma. But because we're using actually difference of cautions, then you can use the heat equation to show that the absolute intensity of the response does not diminish the more we increase sigma. So it's not a function of sigma. And it means we can use the same threshold for the entire difference of cautions pyramid. 

Okay. And then now we have found, we found some local minima or local maximum in our dark space. Let's say we found the point here. So we notice right down the x, y coordinate. 

Then we also write down which sigma did I find this point at? What was the scale of this point? Is this a really big blob or a really small blob? And then we do sub pixel localization of this blob feature. And you might say, why do we need to care about sub pixels? 

It's fine. I just find my blob at some integer location, one pixel off, that's going to be fine. But actually if we don't do this because in the sift, because we down sample the image, every time we've increased sigma, when we're very deep in the image and have a very big blob, then being off by one pixel is not just one pixel, but it's one pixel after we down sampled a lot. So it might be 32 or 64 pixels that this feature actually moves around in the original image. 

So therefore we care about sub pixel localizations. And the way we do it is we do a second order approximation of the dog around our local maximum by taking the derivative of our difference of function with respect to our vector x. And then we set the derivative of the whole thing to zero to find the local maximum. 

And then we solve for it and we get out and we need to plug in the second term in order to get to the actual minimum. And also if the absolute value of the difference of Gaussian response is below a certain threshold, such as 0.03 as they use in sift, we discard the point because it has such a low contrast that it's not deemed a reliable feature. Okay. We also need to discard other points along edges because we want this to be a blob that is well localized. And the thing we were looking at all along was just the sum of the two principal curvatures. So we only know that the sum of the principal curvatures is large. We don't know if there is like a huge curvature in this direction, but not really any in the other direction. And if there is not curvature in both directions, then the feature may as well be located over here or over here because we are actually located on an edge right now. And in order for something to be a blob, we want to have a big curvature change in both directions. 

We want a big curvature. And we again remember we were looking at the Hessian all along and the eigenvalues of the Hessian are the principal curvatures and we've approximated what their sum is. And then we actually compute the Hessian at this point in the image. And then just as we did with Harris corners in order to avoid cases where it's only one of the eigenvalues that was really big, we introduced these heuristics. So we take the square trace divided by the determinant of the Hessian. 

If this is less than some constant computed by r equals to 10, if it fulfills this threshold, we keep it because then we are not along an edge. And you can also see it starts out very elegant like we have. We're looking for just blobs. And then the further we get into it, then there's this, oh yeah. Here is 0.03, that's a threshold. And here's a threshold. 

So there went a lot, a lot of hand engineering into SIFT to get it to work really well. So therefore there are some of these things that are more, this is how it is. And if you ask why is it like this, the answer will be because the authors tried a lot of different settings and found this to work the best, which is not as satisfying an answer as you may look for. 

Except in some cases like with do we need to change the threshold, there's a bit more satisfying answer. And with all of that information solidly pushed inside your heads, I think you deserve a short break. So let's meet back here quarter to two, like a nine-ish minute break. 

All right. So now we have found out where we found a feature in the XY coordinate of our difference of quotients. We have a blob here and we also found the sigma. So we've made something that is translation invariant. We can find a blob no matter where it is in the image. We've made something that's scale invariant because we look across a lot of different scales in the dog space. But we also would like something more. We would like some rotation invariance. And here I've just shown you our kernel that we've actually applied to the image. 

This Mexican hat looking thing. And we have some kind of threshold and dark blobs will have a positive response in the difference of quotients. And bright blobs, something bright surrounded by something darker, will have a negative response. And we can kind of visualize this for ourselves by thinking of something that's very bright in the, let's say something that's very bright in the middle, has a positive value in the middle and positive times negative is negative. And here on the outside it's darker and here it's, this one is positive and positive times negative is also negative. So therefore a bright blob has a negative response and vice versa for a dark blob. Yes. And this is the figure out, really show, but I just moved around some slides because it made sense to see this one earlier in the presentation. So I'm not going to dwell on it now. But yes, we have made a scale invariant feature detector. 

Very, very nice. But we would also like some kind of rotation invariance. And the way we obtain this is by computing gradients in a small region around the blob. So we take the gradient in X and the gradient in Y of this L. So that's our upload image. And then we can measure the magnitude of the gradient. And if we look at what the gradient is in X and Y, we can put it into the arc of tangents too and measure the actual angle of the gradient at this point. And the way we compute the gradients here, we can cheat a bit because we've already blurred the image. So we approximate the gradients by just doing central differences on the blurred image itself. So subtract, go a pixel to the left and go a pixel to the right, compute the differences. That's the X gradient. It's good enough. 

Everything is blurry. And then we have this very concise sentence that explains how we do the orientation assignment of an interest point we've detected with SIFT. So we compute a circular histogram of gradient orientations. 

So what does that mean? We're looking at a small local area around the point. And for every point in this local area, we've computed the gradient magnitude. 

I hope it's the biggest gradient and we've computed the angle of the gradient at that point. And then we construct a circular histogram. So the bins we have corresponds to certain angles that will be placed into the same bin. So let's say if we have four bins, then everything between zero and 90 degrees will be placed into the same bin. And then depending on how big the gradient is, then it contributes more to that bin in the histogram. And in the case of SIFT, they use 36 bins and they apply a bit of smoothing. So each pixel doesn't only contribute to a single bin, but it also contributes a bit to the neighboring bins depending on the distance to it. And then the magnitude of the gradient is kind of what accumulates into the circular histogram. And then we look all around the bins in our 36 dimensional histogram and use the peak in the histogram to assign the orientation of the point. 

So that is to say we found something that's a dark or bright blob. And then the gradient direction that there is the most of. So if a lot of gradients are going this way, then we rotate the SIFT feature so it also points the way in which most of the local gradients are going. And this allows our SIFT feature to also be rotation invariant because we explicitly try to estimate how we should rotate this point in an objective way. Just follow the local dominant gradient direction. And what happens if we then have multiple peaks in our circular histogram of gradient orientations? 

Can we even have that? So let's say we are in the case where we have an image that's looking something like this. Okay, I guess this is a corner. But if there's something that has made us detect something blobby in this area. And then we have a lot of gradients going this way. But we also have a lot of gradients going this way. So when I draw or a circular histogram, then I will have kind of a peak in this direction and a peak in this direction. So both of these could be the biggest peak. 

So what do we do? Then the SIFT offers dictate to us that if there is a peak that is within 80% of the actual peak, then we say, yes, there are two SIFT features at this point. And then they just have different orientations. So we just look for both of them, create one feature that's oriented this way and create another SIFT feature that's oriented the other way. So we just look for both of them if there are two peaks that could kind of be the dominant one. Because in the other image we're not guaranteed that it's going to be this direction, that's the dominant one. And if we rotate it wrongly, then we are not going to be able to find the same point again. 

So we just look for both of them if we're kind of in doubt about the dominant orientation. So everything up to this point has been talking about the SIFT feature detector. Because SIFT is a two in one solution. It's not as with Harris corners, it's not just something that tells you here is something in the image you should look at. It's a detector and descriptor. And the detector does everything we've talked about so far. It finds where is a blob, where is something I found in the image I believe I could find in another image as well. And find out how is it rotated and at what scale it is. But now we come to the SIFT descriptor, which is how do I describe the local appearance of a point in my image in a language the computer will understand. 

And this kind of invariant to changes. And the way SIFT does this is with a histogram of local gradient orientations. And this example here is shown with a two by two grid of local gradient orientations. But using the scale I detect the point that and the orientation, then I normalize everything. 

So I'm now looking at it's at the same scale, everything's rotated correctly. And I place my grid onto the image here. So I have a two by two grid. And then I look at these four by four pixels within each of these grid cells. 

And let's just look at the upper left one here. Inside this I compute the gradient direction in each of my 16 pixels. The gradient direction and the gradient magnitude. And then just as before with the circular histogram, I now construct a circular histogram of the pixels that are within this top left part of my grid. So we can see here this circular histogram shows it has a large peak. Or I guess the peak is here in the down right direction. 

And that's because there is a lot. Okay, this one is point of Kining down right. This one is pointing down right. So this one is pointing just a bit down right. But there's the most of these are pointing down right. And there's not so many pointing down left. I guess there's only really this one is pointing a bit down left, but mostly left. 

That's why there's very little of this direction. So, and we also notice here this is a one, two, three, four, five, six, seven, eight dimensional circular histogram. Not the 36 dimensional we had before. So we have this histogram of local gradient orientations. In this example, it's shown as a two by two grid. So we have a circular histogram for each bin for each patch in this grid. But for the normal SIFT feature that we use, this is a four by four grid. 

I'll show you on the next slide. But we have this eight dimensional circle circular histogram within a four by four grid around or SIFT feature. And then the final step is that we take all of these values and all of these histograms and put them into a long vector. 

Now we get 128 dimensional vector. And we say this describes the appearance of my point. And we normalize it by the two norm. 

So we also a bit invariant to contrast. And then we have Taylor SIFT. So we can match features between these two different images of Taylor Swift. And we see here that the the SIFT features that I've matched are overlaid. And we can see that there are these four by four grids. 

And within each of these we have the circular histogram for this image. So here in this patch there was a lot going left and a lot going up. I guess the left is stemming from the here here. 

And the going up is stemming from kind of the neck. And this is the same in the other one. And I'm only I'm not showing all of the matches here. 

I'm only showing the best matches because otherwise it would be a very very crowded image to look at. So I should also say that because we're only looking at gradients to construct actually how we describe the appearance, then we're invariant towards absolute changes in intensity. If I add 10,000 to the image intensity, I'll get the same SIFT feature because I'm only looking at gradients. And if I multiply the image by 10,000, I will also get the same SIFT feature because my final step is normalizing by the norm of the SIFT feature. So any scale I've applied to the image is irrelevant and goes away. So this helps us obtain a bit of this invariance towards different lighting conditions. 

Here they just showing the illustrations of which ones match to the other ones. So the things we actually designed or SIFT feature to be invariant towards is the position because we look for blobs everywhere. The scale because we look for blobs at different scales in our dark space pyramid. 

Rotation because we consider all rotations and only choose the ones with a local dominant gradient direction. And any linear change of intensity because we could multiply the image intensity by a big number or add a big number. And we don't explicitly design this to be invariant towards perspective change, which is kind of the whole thing that it needs to be used for. 

But it turns out to be pretty good at it based on all these design choices. Okay, so now we've computed SIFT features in one image. Then we take a second image and put it into our SIFT algorithm and then we compute SIFT features in that one. And now we want to match SIFT features between these two images so we can finally find the same 2D point in two images. 

Which we've been talking about the whole of the course. So the way we do it is to compute the Euclidean distance between all features in image one and all features in image two and this gives us a distance matrix. And because we only need to compare distances and say, is this a better match than this other one, we don't actually need to compute the square root. 

And that's, then we can choose the best one. And then there's this weird trick that was introduced by paper called root SIFT. And this is a very simple transformation that you can apply to improve the performance of SIFT feature matching. So SIFT is a histogram and it's well known that the Euclidean distance is dominated if you have a very large value. So root SIFT is a transformation that enables you to measure the distance between SIFT features using the Hellinger kernel instead, which is better for histograms. And the nice thing is that it's easy to implement. 

So you, instead of just doing the L2 normalization I talked about before of the SIFT feature, you take your SIFT feature, then you L1 normalize it, and then you take the square root of each element and then you L2 normalize the resulting vector. And I'm not going to explain to you why this is the case. I'm just going to say, if you want to know why you can look at the paper. But it's those three steps. And then the final thing is you just use your Euclidean distance on this transformed vector again. So it's very few changes you have to make to a pipeline and it actually improves the performance a bit. So why not? I recommend. 

Okay. Now we know how to measure the distance between two SIFT features, but now we also need to know how to match them, how do we sort through all of these things. I could, for all features in image one, I could just find the closest feature in image two. 

And this is going to give me a lot of false matches. So therefore a very common thing to use is called cross checking, where I only keep the matches from image one to image two. If the point that this feature in image one is closest to an image two, then for this point in image two, I look at which point is it closest to an image one. And only if they are the closest to each other, then we keep this as a match. So that helps reduce the number of matches by a lot, especially if you have something that, that's where you have a brick wall and there's a lot of things going on. You find a lot of features on the brick wall, but they all look kind of similar. So you're not sure which one it is. 

That's the situation we want to avoid. We only want to actually match the things where this shift feature is only present once in the image, something that looks like this exact point. And another way of checking that is also what we call the ratio test. So for all points in image one, I find which point am I closest to in image two, shift feature distance. And then I find, okay, that was the point I was closest to. What about the point I'm the second closest to? 

What is the ratio between the point I'm closest to and the point I'm second closest to? And then I only keep the matches where this ratio is below something like 0.7, because this also helps us in the situation where I have a brick wall and I found a lot of features on this brick wall. They're pretty distinguishable, but all of them look the same because it's a brick wall. So if the one I'm closest to and the one I'm second closest to have basically the same distance, I can be pretty confident. I have no idea which one I'm actually matched with. So therefore only if the second one is a lot worse, I know that this is a unique point for this image as well. So the way I did this slide here was to use the ratio test and then tune the ratio really aggressively to only get a few matches. Because usually you get a lot, lot, lot more matches and that would just look messy on a slide. 

Can't have that. So a summary of SIFT. It enables us to do both detection and description. 

The way we detect the SIFT features are local extrema in the dog scale space, the stack of difference of cautions. And then we place a patch oriented along the dominant local gradient direction. Then we compute a lot of a four by four grid of histograms of gradients. And this allows us to match points between two images to be invariant towards scale, rotation, illumination and viewpoint. And we can also match partly visible objects as long as it's visible in the other image as well. Are there any questions to SIFT before I have some closing remarks on other feature matchings? 

Okay. SIFT is very widely used. It has 74,000 citations when I did the slide probably more now. And it was actually patented until 2020. So at some point in time, if you installed openCV, it didn't include SIFT and that was very annoying. 

But now if you can just install openCV in this area, it's very nice. And because SIFT was patented, a lot of competing different descriptors also grew up like SERV, and ORB, and BREAF, and BRISK and all these acronyms listed in order of citations. And they also have their advantages and disadvantages. Some of them are faster than SIFT and have better performance. 

But it's very similar in the same vein, I would say. And then there's this thing that has happened since in 1999 called deep learning. And this has created improved feature descriptors. But the improved performance is mostly an invariance to changing lighting. So if you have a trying to match features between a scene taken in the spring or in the fall or like a daytime and nighttime. 

But if you just have images taken from multiple views of the same scene at the same time with not any metallic or watery surfaces or so on, SIFT performs extremely, extremely well even though it's quite old and hand engineered. So one of these newer methods is called R2D2. That's the editor and descriptor. 

And SuperPoint is also editor and descriptor. But just a neural network that does the whole thing. So now you should be able to implement and use blob detection using difference of cautions. And you should be able to analyze and use SIFT features and feature matching. So in today's exercise, you build a blob detector and I should just delete the line about Matlab. 

Does anyone here have Matlab installed? No, good for you. Yeah, enjoy the exercise. And if the feedback person is good, come down. 