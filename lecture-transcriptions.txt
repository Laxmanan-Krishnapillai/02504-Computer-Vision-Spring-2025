Speaker 1: It's a pleasure to welcome you to 2450, as we call it, Introduction to Machine Learning and Data Mining. My name is Björn Sön Jensen, who will call me Björn or something to that effect. I'm an Associate Professor at DTU Compute, where I research machine learning methods and applications and stuff like audio, chemistry, and biology. So, of course, once in a while, I also get the pleasure to teach this course, which is about 500 to 550 people this semester. That always causes a bit of chaos as you figured out, especially when the auditorium doesn't work. Now, as it turns out, there's quite a lot of people involved in this course, and you'll also see one of my colleagues at one point. 

Okay, but let's get on to what we're supposed to deal with here. Hopefully, most of you would have seen this book. It's available on DTU Learn. You're supposed to read something, and we do expect that you actually read something before coming to the lecture. Otherwise, you are not going to learn much from me rambling for two hours about stuff that you don't have any idea about whatsoever. So, please make sure you do that. Also, do the homework problems that are on the web page. So, I'll actually use this opportunity to just show you the web page in case you haven't seen it. And you can Google that if you want. 

Okay, so it's this one. Hopefully, most of you would have seen that. You would have seen that we have tons of TAs, not tons of TAs, but we have enough TAs, I would say, to schedule in here, or schedule in here, and you can see there's some reading going on here. There's a homework problem, which happens to be a previous exam question, so all adapted at least. So, there should be plenty of material here for you to sort of prepare for each week, and hopefully it makes sense once we go through it. Now, every lecture, I will ask some of you to actually give some feedback on the lecture, on the style, the content. Also, the IT problems, that's fair. 

And today, there's no name. That means everyone is supposed to maybe give a little bit of feedback, but also go into, yeah, go into DTU Learn, find that survey, which is essentially a survey today. It asked you about where you seated today. Would you have wanted to have another seat? 

Had the opportunity been there? So, give feedback sort of on the practicalities of the course here. Okay, so at some point, your name will be here that week. 

Absolutely make sure to give some feedback on everything. Right, so we'll show you sort of an overview every week like this. So today, we are obviously in the introduction, and it should maybe have been called introduction and data because we squeezed some of the stuff we usually talk about next week into this week's lecture, which is great when we're already late, but so weird. Now, the following weeks here are going to be, let's say the basics. So these are the basic concepts and tools from mathematics, from statistics, probability, that you are supposed to know here. Not supposed to know, but that you need in order to do machine learning in the proper way. 

Or at least the way we think about the proper way. That means, to some extent, thinking about data in a vector space measuring similarity. It means linear algebra. So some of you would probably for the first time in your life see that linear algebra can be used for something useful. And then we'll see how that plays into what's called PCA, principle component analysis. 

Okay, then a tough week in week four. This is all about probabilities and probability density. So discrete and continuous random variables. We are going to review all of that in the context of machine learning. Because, as it turns out, all modern machine learning is, to some extent, based in these techniques. So you need to know about that. Okay, then there's a big block here of supervised learning. 

That's where you essentially observe something in the real world. You can think about an image of handwritten digits and you have the associated labels. So you know if this scribble is, let's draw a scribble. So you know if this is supposed to be a seven or whatever it's supposed to be, you actually have a ground truth label associated with it. Now from that input, you're now going to actually predict the actual class of that input. So in this case, we would probably want to classify that as a seven. 

It could also be an image of a cat or dog and you want to classify, is this, does this image belong to the category dog or cat? Okay, so just a few examples. And we're going to look at a bunch of methods here. And I'll come back to why we need more than one method in machine learning. 

As it turns out, there's no unified model that covers everything. So we need to learn different methods of different complexity. But we also vary, let's say, not concerned, but eager to tell you about how you actually evaluate these things properly. And this is as important as knowing some specific technique is that in machine learning we need to evaluate the stuff we are doing. 

And I'll come back to that a little bit today. So the evaluation, including some statistics in this week, will be needed to do what we want to do here. Then another block in the unsupervised learning setting here. So here you can imagine a huge pile of images of animals. You don't know whether or not a cat or dog, but at least we want to learn something, find patterns in that data. It could be group images together that are more similar to each other than there are to any other image in that dataset. Okay, that's what we call unsupervised learning. That's when you don't necessarily know the ground truth, but you're still interested in learning something about the dataset you have available. 

And I'll come back and mention a few other things there. It could be anomaly detection, which we can use mixed-year models for. It can be so-called association mining. I'll show examples of all of that today. In the third week, we're going to talk about the exam and a little bit more that we hope to squeeze in there about maybe effects and impact of what we are doing here. Let's see if we get to that. Great. Important point, there are a lot of you in this course that means you can help each other to a great extent. 

And for that, we have Piazza set up. And now I would have wanted to do this before we start the lecture, but as it turns out, I didn't. So I'll show you where to find the course and teach you learn. Hopefully, most of you would have already seen this. 

Whoops, not there. Okay, so here you have all the basic information I've already told you. So have a look at that. There was the welcome message we sent out the other day. And if you go at the top here, you'll see a discussion forum. This is where you'll find Piazza, Piazza depending a little bit on how you pronounce it. And in here, you will be able to post a problem or question without putting your name on it necessarily. 

It could be anything from, let's say, old exam questions to questions about code and stuff like that. And now, IG is messing up again. There we go. Okay, so really make sure to exploit this thing here because you have 500 other people looking at your question and may be able to help you. That becomes really important, not important, but helpful potentially when preparing for the exam. So please make use of this and think every time you're wanting to email me a question, would anyone else benefit from learning the answer to that question? 

If so, put it in there in some generic way that doesn't reveal who you are. All right, that was the Piazza stuff. We will put up videos, as you noticed, that I recorded stuff here on Zoom. So we will put those up, but that's not the intention. 

It's only as a backup. We want you here in the building on Tuesdays because you're engaging with, well, the lecture is a little bit, sort of one-way communication from my side, but at least the TA sessions. We have excellent TA, so make sure to engage with those. And then, yeah, you figured out via streaming to all kinds of weird places. 

The other auditorium in there, via streaming, via Zoom, and of course, you guys in here, you have the luxury of being here in person. Okay, so the plan for today, introduction, just a brief recap, not recap, but overview of what is machine learning and maybe a little bit historical notes here, where does it actually originate from and how does it fit in with the general other fields, I would say. Then there's a little bit of pre-test, and don't worry, you're not assessed on this, it's simply for us to know what you know. And this is perhaps to tweak the lectures and the presentation a little bit, depending on whether or not, well, if you don't know anything about statistics, maybe we should not start from assuming you know something about statistics, okay. We'll have a few breaks, and they will be shorter due to this IT problem, so expect like five minutes where you can run out and come back. And then I'll overview, basically, this supervised, unsupervised stuff, all the topics we talked about, and then we're going to see if we can squeeze in this section here. 

I might revert on that due to time, but let's see. Hopefully we'll get through that, because that leaves more time for the hard stuff coming in the next few weeks. Then from three, there's exercises, and these are not physical exercises, they are essentially programming and a little bit of pen and paper exercises for you to do, to prepare for, well, to actually just consolidate your knowledge and learn the material. All right, so the first part here, what is machine learning? And I'm going to come back to this part here, because this is essentially our holistic view of what machine learning is. It's not just about learning about algorithms and models and mathematics, it's about thinking about the whole pipeline going from data to some result that you can disseminate to your customer or your teacher, whoever it is. So there's a lot of stuff going on in here, and I'll come back to that a little bit. 

So a few learning objectives that you'll see every time. Okay, so back in time, where this machine learning business originate in the first place. So you've all heard about chat, GBT and deep seek and all of that. So I'm just going to bring you a little bit back into some of the first people who actually talked about machine learning and eventually coined the term. So many of you would have heard about the imitation game, and essentially that comes from Alan Turing, very famous and great mathematicians, computer scientists from, let's say the first part of the last century. So essentially he thought about a lot of things and some of you would have, you would know him from a universal computing, like computer science stuff. 

But as it turns out in this course, we are not going to talk too much about that at all actually. We're going to talk about another paper or idea he had even at that time in the 40s, namely that, let's say the prevalent idea at that time was that if we were to build machines, you think about what a machine would have looked like a computer at that time, it wasn't really sort of consolidated. But if we were to build these machines, what should it be able to do? The prevalent thought was that actually the machines would think and reasonably in a way that was similar to what we do in the human brain. 

So the processes should sort of be the same. However, he had this other idea, namely that instead of focusing on that, on the processes and what's going on in the brain, we should actually simply ask the question whether or not we can make a machine imitate a human. And that also comes back to learning like a child would do, like an infant would do. And again, here comes this learning that once you experience life, you learn stuff, and perhaps all machines should have those same properties. 

Now the term machine learning was not coined until some years later, as you can see here. And that was Samuels and he loved to play games. So he wrote a Chickas program. And at that time, there was something called a software program and you would have a device that could execute that. And he would say the machine learning is a field of study that gives computer the ability to learn without being explicitly programmed. 

Couple of important stuff here, learn. Okay, that means something adapting to something, but without being explicitly programmed. So many of you would have programmed in your life, more or less at least, but you know that, okay, I often have something called if then else rules. So if you want to implement logic like that, like simple rules or reasoning, then you would explicitly program them, for example, as if then else. So a rule could be if there's a lot of red pixels in my image, maybe it's a, I don't know, fire truck or something like that. 

Now you've explicitly programmed something. We want to learn these rules. We want to do that from experience. 

And that has sort of been consolidated into something a little bit more operational, I would say. So what Samuels did here was he played checkers and executed, played, let's say, the program against itself several thousand times. And then he basically learned the value of each position in terms of whether or not you win or lose at the end. Now, as I said, this has seemed a little bit, it's not particularly operational in that you can use these principles to go work on your own data. So over the, let's say the past 30 years or something, that has been made operational in the sense that, a very abstract, let's say, definition of what we want here is what Tom Mitchell here proposed in one in his book. And he said that computer program is said to learn from experience E with respect to some class of tasks, T, and in performance measure P. If it's performance at the task T as measured by P improves with experience E, blah, blah, blah. 

That's a mouthful. All it means is that, okay, we have a way of measuring performance that our, let's say, system or machine learning system does, it's on task. If we give it more data, it should be better at that task and improve the performance. 

That's all it's saying. So if you think about this checker's example from before, then, well, in the Samuel's setting, then playing 10,000 games would be the experience. Or if you play, let's say, 10,001, we would expect the system to be better if you actually learned something sensible from those experiences. Okay, and the task here, simply playing checkers. I'll come back to why it's so narrow in only solving one task in a bit. 

But then the performance is whether or not, how many times do we not lose here? All right, so the main point of these three slides is just to say there is a backstory to everything that's going on and all the hype that's going on. Some people have actually thought about this, tried to operationalize it a little bit. Now, it's your time to actually work a little bit and this is not to basically test you in any way. It's just an example of the quizzes that we will throw in throughout the course in the lectures, mainly for me to not talk all the time and for you to think a little bit about what's being said. So go to GCU Learn and I'll show you, if I can, go to GCU Learn in here somewhere and go to quizzes. I have no idea how it looks for you because I can't see that, oh, actually, maybe I can. But anyway, if you go to quizzes, you should be able to find quiz number one in lecture one. It should look roughly like this and put someone just nod or something, say that, yeah, you can see it just to make sure. 

Yeah, it works, it seems. Go in there, have a thing, we think about this stuff here and then try to give an answer here. It's really, really simple and the task is mainly for me to make sure that you learn a set up correctly and that you guys know where to find these quizzes. 

Sometimes we will look at your answers like overall statistics, but maybe not for this one. So, it's a couple of minutes and I'll run out and get some water and I'll come back. So, put in an answer and randomly guess. All right, put something in. It doesn't matter really what you put in. 

All I can say is that the correct answer is B here. I haven't looked at the statistics, I'll skip that for now. So, the first one here simply relates to the size of the data set, the size of your experience. Second one relates to actually the data you have available, namely images of eyes, cataracts, and the labels do they have cataract or not. 

Okay, the errors the programs make when trying to label the image that relates to the performance of the model and then we have something here that may be a little bit confusing, but this is simply information about the problem itself. It's not specific information you have about certain patients. That is what we call domain knowledge. I'll come back to why that is also important in machine learning, at least for now. All right, so as you'll see, we have the quiz. You'll see the solution in the presentation version of the slides when I upload it after. 

Lecture, if I forget, you remind me. All right, so if you look at machine learning, then there's a separate field or as a field. I would say it's hugely influenced by other fields and that is useful to know, especially when we think about what you need to know to do machine learning correctly. So if you think about machine learning and slash data mining, mostly machine learning as it turns out, then it's really influenced by statistics, by pattern recognition, AI, like Turing, for example. 

You would say he was in the AI business, except that the AI people tend to focus on algorithms, like let's say if some of you have taken the introduction to AI here at DTU, you would have learned about search methods on graphs and trees and stuff like that. So specific algorithms that can do something like that. We are also interested in that. Some people call it symbolic AI. We are also interested in that to some extent that we use these methods in our setting to let's say optimize things in our world. But so that goes into what we need to know here. 

Oh, actually you don't need to have taken that course, by the way, I should be careful. But it also relates to let's say database systems, how do we represent data in our computer? And then a bunch of other fields here that I haven't actually included, but that could be stuff like biology. You'll see neural networks and they are to some extent, at least that's let's say the anecdote, they are to some extent influenced by, or let's say inspired by the way a neuron works in your brain. But there's also physics going into all of this and then one important one, namely math. 

So linear algebra, multivariate calculus and all of that. I'll come back to all of this, but the interesting thing here is that there's so many things that basically goes into building a good machine learning system, that includes in particular the big companies that we all know and either love or hate. Okay, but the focus here in the center of all of this for the machine learning bit, is that we focus on the learning algorithms rather than these algorithmic ideas, like let's say specific search methods for some problem. 

We want the setup to be able to learn from experience like we just talked about. We don't wanna actually for every single problem we know, we don't wanna build in our knowledge about a particular disease, for example. Well, as it turns out, that might help in some cases, but that's not what we start out with necessarily. We start out with getting experience, getting data and learning from that. Okay, and then of course there's this notion or inherent notion of gradual improvements in learning. 

If you go through life and you get dumber and dumber as you go, assuming you are healthy, then maybe something's wrong. It turns out as you get more experience, you probably, about a particular task, you probably become better at that task. Okay, so there's some notion of time and then of course also the amount of data. The more data you have, the better you become at that task. 

Presumably, and that is basically the assumption here. This element here, there's also an element of, let's say, training time. If you have a fixed data set and you have a fixed so-called model and we'll come back to what a model is in machine learning, then if you train for longer, then maybe you get better. 

That happens in some cases, let's say for some models, but as it turns out, this notion of time can also hurt you a little bit that if you continue to optimize too long, you can actually overfit as we'll see as we go along. Okay, now, we focus on general and in the term general here, you should be interpreting this since that we don't wanna build a custom machine learning system set up for every particular problem. So if you go back and think about my example about handwritten digits, that's an image. The cat and dog set up, that's also an image, let's say classification problem. So maybe if I apply my generic machine learning algorithms, algorithm or method to the handwritten digits and it works well there, I would be able to, just with a new data set, be able to, I would like to be able to then simply close my eyes and be able to apply the same setup for the cat and dog example. That's what we mean by general algorithms here, general principles, not something where we need to build bespoke stuff for every single variant of the problem that we might encounter. That's the ambition here. 

That's where we'd like to go. We're not quite there yet, I would say, but for example, for images, there's some indication that there are some general principles that applies across different problems. All right, so machine learning relies on a lot of other stuff to do correctly. Now, maybe following up on this, you can say, okay, if we had a general algorithm in machine learning, you only need one week of teaching basically because there's only one thing that you need to learn about. 

That would be great, but unfortunately, that's not the case as I've alluded to. So if you look at, as this image here, so here you have essentially the amount of experience you have versus the error you make on unseen data. That means data you're wanting to predict on, essentially. That's data you haven't seen before. Now, of course, in order to evaluate this point here, you would actually need some ground truth data. So as it turns out, what happens is that we take a data set, we split it into a training and test set, and we evaluate it on the test set. Well, we train on the training set and we evaluate the performance of that trained model on the test set. 

So what's going on here? So we have experience at training set size. You can think about a number of images if you want. We have the test error here, or the so-called an estimate of the generalization error, which you'll become familiar with later. We see a couple of different things here. So we have three different, let's say, we can think about machine learning systems or models. One of them is not so flexible. The other one is, let's say, slightly more flexible, so medium flexibility. 

And then we have one that's very flexible. And a proxy for this thing is you can think about the number of parameters you have in your model. So the proxies you're going to work on, you might have maybe 100, maybe 1,000 parameters. 

Google and the likes and MEDA and OpenAI, they have billions of parameters, like 500 billion parameters. So you can think about one of them can probably learn more. It's more flexible in some sense than the others. 

But it's only a proxy. There are some theory behind this. But anyway, let's see what happens here. So for a given training set size, I look at what is actually the performance of the three models, and I see something interesting going on. If we take this sort of regime down here, it looks like the simple model. 

And here you can think about a straight line essentially that you're trying to fit to your data. It actually turns out it's the better one. So lower is better here. It's the better one if you only have, not a lot of data, let's call it that. Then in the middle sort of section here, that's this regime here, then actually this sort of medium flexible model turns out to be the better one. And then, well, as it turns out, when you get billions of data points and you pay for more data or whatever you do, again, like the big companies, maybe these extremely complicated, not complicated, I would say, but at least big models with a lot of parameters, they tend to do better. So unfortunately, we can't sort of, let's say stop after next week, just saying that we have one model that fits it all. We need to know about different models. And of course, you also need to know about how to actually produce a plot like this, for example. So how do we actually evaluate our models? 

That is exactly what we are going to spend the next 13 weeks on. So for example, here we talk about it in the supervised setting, you're going to learn about, let's say relatively simple models. These are so-called linear models. 

They can still do a little wiggly function. So you might have heard about polynomial regression, for example, but they're linear in the parameters. And then logistic regression, which is, as it turns out, a classification method, just to confuse you. 

But these are considered relatively simple methods. And then we go up to something called large neural networks or neural networks in general, and we are going to explain what that is. It's nothing more, nothing less than a function that's composed of simpler, let's say, elements that we will learn about as we go along. 

So there's nothing magic here and no wit stuff in your brain or anything. It's essentially mathematics, right? And then we will learn about methods in between and let's say one by one, so to speak. So these are sort of the methods or models that we're going to look at. Then we're going to look at principles for controlling complexity of our models. So if I fix this model over here, how can I now actually control the complexity of it or to some extent learn it? And there are techniques called regularization that we are going to look at here, essentially putting a penalty on the complexity of the model. 

There are other methods in the same sort of shangles, namely ensemble methods, where we combine relatively simple models together and simply by doing a majority or having a group of models make a prediction about things that comes with certain disciple properties. All right, so just a few more advances and I've already alluded to what's been going on in the last three years. And I'm afraid we're not going to actually look at those models in this course, but this gives you a, let's say, foundation for actually understanding these models. And one interesting comment I want to make here is that, what has happened here is that these guys from DeepSeq, they have actually to some extent, well, they disrupted the field a little bit, I would say, in the sense that they have actually published their work in the sense that the model is now free for everyone to see what it is and that code. That's a big deal. Now, an aside here would say, we have been, I wouldn't say we've been going around and fearing that we are way behind in academia compared to these companies, but at least it's nice to see that actually, it's nothing more than nothing less than a great bunch, oh, let's say, quite a lot of an engineering effort that goes into building these systems. It's not some deep insights that we haven't got, let's say, in a university or academic setting. So for us, that's sort of nice to know at least. Anyway, let's go back a little bit. 

Again, a history lesson here. So there's much stuff going on below here, but I just want to point to a few things here that I've listed. So a few things sort of stand out at least to me, much more than this, but one thing was, sort of in 15 here, it says close world image recognition. So there's a semi-famous dataset called ImageNet that came out there. So emphasis on dataset that came out. That sort of triggered the fact that people would now be able to train and compare different models and actually show, let's say, beating human performance on the same dataset. 

Remember this dataset part. There's also another thing that's interesting here, namely there are a number of games that goes in here where these machine learning methods, simply by observing what happens if I make a move, can I learn from that, can I learn a policy, or can I learn a strategy, simply from experience to play that game better than humans. So there's quite a lot of games going on here, and I think games are interesting in the sense that it's something we find fun, or at least some of you, some of us, do sort of computer games in that sense that they require, even if you know the rules, they require some notion of intelligence to actually solve it and do it well. And when our computer can do that equally as well as us, then maybe, well, okay, that's at least interesting. It's sort of a milestone. For example, if they can solve the game of go, much more complicated than chess, that's probably a milestone where we should start thinking, okay, what can our computers do compared to ourselves? Now, maybe going back a little bit to the image case, there are other exams up here which are potentially more useful than playing Atari, for example, which is a very old video games, probably you can't remember it, but there are some more useful exams up here in the medical domain where you take x-rays or other, let's say, medical scans of humans, and you try to predict, let's say, do they have a disease or not? 

And there are some interesting, what's it called, claims to benefits here where essentially, some of these big companies went out and said, okay, now we don't need radio, all this is just animal because our AI system can do it better. That didn't quite happen, right? And some people, very famous Nobel Prize laureate, were a little bit too ambitious at that time, so it's hyped, but let's keep our feet on the ground, okay? So anyway, all of these things here, build essentially, let me just check here, yes, all of these things build on neural networks, and we're going to look into these, but the underlying principle is the machine learning principle that you learn from experience, right? 

So you can probably list a thousand more impressive, maybe not more impressive, but at least equally interesting cases that I've done here. Right, so one could maybe ask, why is this happening now? And remember what I said about the dataset was the sort of key driver for some of these advantages. So that is a very big deal of what's been going on, that we have better curated, easily available datasets that are public, and the same goes to some extent for what's listed here is the social aspect, that companies, they actually open source their code, and the algorithms here. 

For everyone else to try out, that simply accelerates progress, as it turns out. Stuff are happening in the world now where people are trying to, let's say put terraces and stuff on each other, and that's probably not a good idea, and that means hiding things behind, let's say custom boundaries and stuff like that, whereas the, let's say the similar case here would be that every one of the big companies start not publishing, not attending conferences. Okay, so datasets and openness of, let's say code and ideas is a big deal, and then of course, with big datasets and big models, you need big computers and the GPU sort of genre, but type of processor has been a driver as well for this, and you probably all know NVIDIA, or at least some of you would know NVIDIA. Then you can ask, has nothing really happened in terms of, let's say, algorithmic ideas or insights, or let's say scientific progress in the algorithmic side, and sure stuff have happened, even though neural networks are basically from, well actually the 40s, the first sort of very simple neural network, 1940s. 

So definitely stuff have happened, especially in terms of these very large models, so there are definitely insights there, but maybe if you look really carefully, some of the ideas actually came early, not in the last five years at least. Of course, then there's money, and, oh sorry, there's money and having good AI, apparently. So of course, big companies are gonna throw more money into their products and into hiring very good people, that's also to your benefit if you wanna work in this area. On the other hand, it also sort of benefits us in the sense that research grants and stuff, they also now try to incorporate AI, machine learning in many of these contexts. So for us as researchers at a university, this is also, let's say, a benefit that this has now become, let's say, so popular. 

All right, but again, I hope this just tells you, it's not the last three years this has happened. I started getting into this 20 years ago, that's when I took my first course similar to this, and there we were 30 people, max, right? So now you are 500 and something, so definitely it's a hot topic, and it's good that you wanna know about it. 

Okay, so how is this course, let's say, what's the philosophy here? And I've already alluded to this picture here, and that is this holistic view of machine learning and data mining. You need to worry about your data, you need to know what it is, you need to know the domain it comes from, or you're working in, first for you to do sensible stuff in the machine learning business. Okay, so I already said, we need to think about the data, we'll do that later today, hopefully. We need to sometimes do data preparation, we'll talk about that today as well, so I won't go into it. 

We need to do modeling, and that's of course why we need, let's say, 10 weeks or something to talk about modeling and specific methods, and then combine with the evaluation part where we need specific ways of evaluating if our models are good, interpretable, for example, and visualize how they're performing. I would say if you think about other machine learning course you can find out in the world, there are a few distinct aspects here. First of all, this holistic view is not really sort of emphasized in many machine learning courses, at this level at least. Then there are a few other things, namely about the, let's say, the focus on evaluation and statistical comparison on machine learning methods, that's also something that we are, to some extent, very proud of, that we actually force you to think about, because it's all good and nice that if I give you a particular model or a bit of code and you can run it on your own data, but you have no idea how to evaluate it, that's not good, it's not good when you go out into, let's say, medical companies and claim you can do machine learning, but you have no idea how to evaluate it correctly. So that's an emphasis in this course. So who should then take this course? And of course, you should, all of you, I just wanna maybe highlight a few things, and I've said that already. 

We do assume something, we do assume that you have mathematics, namely linear algebra in particular, and then multivariate calculus, and that includes how to differentiate and integrate these functions. Some of you, and this is maybe for information for everyone, some of your people or colleagues in this room, they are on the second semester at university. Do they know all of this? Do they have experience in this? So in order to support those a little bit, we have actually arranged for special rooms and a little bit of extra help to make sure that this multivariate calculus stuff that you are just learning about, that we actually get you on the right track, right? So I would refer to these students as kid students, and it's not because they're kids, it's just because it's called constant elegance and data. So that's why, and that's also why I want to include this slide just to make everyone aware that there are some, actually we have students from second semester up to PhD level, so you can imagine the difficulty and discussions in deciding the level for every single lecture, anywho. Aside from mathematics, we also sort of expected, not sort of, we expected you know statistics or probability. What do I mean by this? I don't expect that you can name every single test, statistical test in the world. 

I would much rather have you be able to explain to me what is the random variable. Now I might sound a little bit old and grumpy here. My expectation would be that 10% of you would be able to do that on the spot now, but we'll help you get there and remind you what this is. In our experience, this is something where we all need to just remind ourselves what things are. So random variables, expectations, all of these rules here, we will remind you of those, and then of course we'll use everything you probably learned in some basic statistics course. 

Don't worry, we'll do that together. Now there's an element of programming in the course as well. It's not a programming course. 

Please keep that in mind. We're not going to teach you how to implement numerical algorithms from scratch. We'll show you code on how to use this and how to implement models, and you'll see examples of that. And you will need to tear that code apart and learn what is the code doing. But it's not a programming course in that sense, neither in Python, MATLAB, or R. There will be some challenges here and there where you actually get to implement stuff if you want, but the reason why it's not a programming course is because of the diversity again of students coming into this course at the moment. Some of you would have just learned about Python and how what is a variable, what's a for loop, what's the list comprehension in Python, for example, and others would have 10 years of experience, not 10 years, five years of experience in programming. So we're trying to sort of meet a little bit in the middle and give you some code that you can look at and learn from, but we'll not ask you to implement it from scratch, all of it at least. Right, so if you're in doubt about all of this, then maybe think about some of the stuff I said here and if you're still in doubt, then yeah, well, come ask us and see do you actually have the required prerequisites. 

It's perfectly fine with me if you don't. You just need to be prepared to then spend more than the time I'm going to talk about in a minute, actually catching up. Right, so learning objectives is a thing at all universities at the moment, so this is where we essentially outline what you're supposed to learn from the course. I'm not going to read this because then you're definitely going to fall asleep, but I will mention that you need to look sort of at the type of learning objectives we have here. So we have something called knowledge comprehension, knowledge comprehension, comprehension and application. All of these things is about you knowing something, maybe being able to explain what it is. It's not about you actually applying it or analyzing it, evaluating, generating new stuff from what you know. 

So these are very low in what we call the Bloom's taxonomy here, and the way we're going to test that, test that which is probably of interest to you, is mainly in the written exam and I'll come back to that exam later. Then there's something called application here, that's why you have to do something. Of course we can test the aspects of that in a written exam, but it is mainly the projects and the weekly exercises that will to some extent prepare you, while the week exercises will prepare you for the projects and the projects will count towards the final grade and are mandatory for passing the course. So just be aware of this when you actually think about what am I supposed to learn here and how am I being evaluated on it. Right, so what do we do to actually support your learning here? So people made it to the lecture, we have a basically full auditorium here, 300 people roughly, we have 100 next door listening to a crappy laptop playing the audio, and we have people on Zoom. So there's a lot of people involved here, but what I want to say is that the lectures are essentially high level presentations of the concepts and the relations. I will never in one single lecture show you any code. 

Well, that's not my intention at least. These are high level lectures that can connect the dots to some extent, and you need to come well prepared for those. They are roughly two hours every week, that's tough, but it's university, so that's just the way, something you have to deal with and be able to concentrate. We'll try to put in breaks obviously and quizzes and all of that, but you won't learn anything or get anything from me rambling if you don't read the material. Okay, then we have the homework problems, they are listed as I said on the webpage. Here you get a chance to actually practice or consolidate what you're supposed to know during the lectures and by studying yourself. And by the way, there are also previous exam questions. 

They're also on did you learn at the moment? So this is a way for you to test that you know what you're supposed to know at this point. Okay. Exercises, I've already mentioned that and ramble a little bit about that. This is where we give you, it's basically a worksheet where we want you to go through the exercise and the questions we've asked you there. They are at least for people on the second semester relatively open-ended. That means it's up to you to figure, not to figure out what you need to learn from it because we have thought a little bit about it, right? But they are not step-by-step baby steps of what you should now do to understand this topic. They are slightly high-level questions where for example, the question could be run a script, look at five different figures, or not five figures, but a couple of figures work out what can you actually get from those figures? What happens if I change the parameter? 

How will that affect my performance on this particular data set, for example? So again, relatively high-level exercises that doesn't prevent you from actually going down breaking apart the code and rewriting it if you want to. But that's not the expectation. 

All right. The experience here from the exercises, the weekly exercises, serve two purposes, at least in my mind. First, they prepare you for doing the projects. You will take that code, or whatever you're being prepared with, or, sorry, presented with. 

You'll refactor that and put it together so that it can run on your own data set for the reports. But you can imagine sitting a very tough exam, I would say, I think relatively, at DTU. And being able to solve that efficiently, simply by having a good intuition about what a method does, how would that method do on this particular data set? That is important, so it also gives you intuition. 

And let's say stamina against, let's say, various tricky questions, we might come up with. So what I'm saying here is, do the exercises. Exploit the fact that we have very good TA's to help you. And if you don't know what you're supposed to get from and question in the exercise, ask the TA. Ask me, ask someone on Piazza. Question? 

Good question. So I'll repeat for people all over the world. So the question is whether or not you are supposed to do the problem, or homework problems, listed on a webpage before showing up at the lecture, or after, or during, or whatever it is. I think that's entirely up to you. But if you read a book chapter, I think it's a good way to actually see, can I now solve this problem? If you can't solve it, or it's too easy or something like that, okay, maybe go to find some harder problems in the old exam sets. If you can't solve it, then there's, of course, an opportunity in the TA session, or exercise session, to actually ask questions about it, and also use a little bit of time on it there. Our recommendation would be that you don't spend the full, full, or, sorry, you don't spend the full two hours in the exercise session, actually trying to work on homework problems. We want you to look at the code and the algorithms and that stuff, so I hope that makes sense. So try to do, I guess my, I guess my conclusion here is, try to do them at home at least. Does that answer your question? 

Okay, thanks. Okay, now onto some, we'll have a break. When did we start, a quarter past or something? Okay, it won't be too long, maybe 10 minutes, then we'll have a break of 10 minutes, and then we'll continue. Okay, so I think this, you might want to wake up now, because the assessment in the course has two parts. There's a four hour written multiple choice exam with negative marking. No aids, no computer, no, nothing, right? No book, but you're allowed to bring two sheets of A4 paper with handwritten notes. Okay, you didn't fall off your chair, that's good. Now, it just means that you need to take notes throughout the course. 

Any questions about this statement? That's a question over there. No. 

No. Post that question on Piazza, so the question here is, if you write on your iPad your notes, can you then bring that to the exam, print them out and bring them to the exam? The current policy is no, but I'd be willing to reconsider if a lot of people want me to allow that. 

So put it on Piazza, the question, then I'll answer there, and I'll repeat next time what I have come to a conclusion about. Why am I saying this? Because there's extreme value in you sitting down using your hand to actually write stuff. I know you think you're doing the same on your iPad, it's not the same, in my view. 

But I'll reconsider, and then if a lot of people support that idea, then I'll allow that, okay? So the question is, how do we make sure that we actually take the correct notes, basically, every week? So, okay, I don't have A4 papers here, but I think it's fairly, is it the content or the actual format that you're writing? I'll write off the notes. I mean, this is two pieces, two sheets of paper. You can write on both sides. Yeah. 

So I'll not answer that question here in Plano, but I can talk to you afterwards, because that was a glitch, and it was not our fault. But I'll not go into that discussion, right? So the intention of taking notes and bringing notes to the exam is not that you need the notes in the exam to solve the problem. It's the process of writing the notes. Generally, we would try to put whatever complicated formula you need, if you need one, but you wouldn't need a very complicated formula, because it's a no-aids exam. 

We would expect you to have that intuition to be able to solve the problem. Okay, any other questions about this? Yes. Two sheets. You can write whatever you want on both sides. And I will make a suggestion, probably not today, but next time, what should you put in those notes? What are the most important equations? What are the most important concepts? So I will help you in that sense, but it's your job to then consolidate that. And I know we didn't do that last semester. 

That was the first time we tried it, or George just didn't. So, I mean, I'll try to help you a little bit along those lines. So I'll try to help you a little bit. This worked extremely well, I would say, when we look at the grades and the performance on the exam, compared to you being allowed to bring a computer and notes and stuff and not really preparing, hoping to do your best anyway. This forces you to take this seriously, and it saves our time in reexamining people who don't prepare for the exam. Okay? 

That might, it might sound like a very tough format, but it's actually good for you. That would be my statement here, right? And it's not going to change. So maybe either format of the note, whether you can write stuff on your iPad and print that, I'll probably be willing to change that. But the general format will not change. 

Okay. Okay, so aside from an exam for hours, and it's tough, it's not necessarily, I mean, we expect someone to get a 12 year track, it's been four hours. It's not like other certain exams that you do, where you can leave after one hour, if you just prepared reasonably well, we expect you to spend that time for us to be able to evaluate correctly. 

Right. We'll get back to all of this and discuss this in much more detail, so don't worry about it too much now, but if you don't like this format and have this course as an elective, then maybe consider if you wanna go through this. We think it's a very good idea, and it worked really well. That's the conclusion here. 

Okay. Aside from the exam, you also need to submit two mandatory reports. They must be approved to pass the course. That means if you don't get the two reports approved, well, you can in principle sit the exam, but you're not going to pass the course. 

So it will be a waste of your time. Make sure you buy, let's say a week or two before the exam, you have two reports that are approved. We will be very explicit about whether or not they're approved or not approved. You need to submit the first one in a month or so. If you, when the feedback arrives, get another approved, you'll get one week to revise that report and resubmit it with the aim to then pass the course. 

You get one chance extra. You can't improve your evaluation at a point, but you'll get a chance to pass the course. Then the final, there's another exam here. Oh, not exam, report, sorry. And then we have the final grade that's an overall assessment. That means you will not get a grade on the reports, you'll get an evaluation that more or less clearly, or well, clearly states whether or not this is good or bad or whatever it is and suggestions for things to improve if needed. And the reports are weighted by 10% roughly overall in collaboration with an external examiner. 

So 90% for the exam, 10% for the reports in total. Questions about this so far? I don't know, maybe we should check soon. Okay, so I see some debate on, assume about this iPad thing that I mistakenly said I would reconsider, but I will reconsider it and I'll probably allow it, but you cannot bring your iPad to the exam, right? 

That's obvious. No electronic devices except according to the study administration and non-programmable calculator. We are trying to work out what that means. I don't know, no one knows. 

We are trying to work out what a non-programmable calculator means. All right, we've seen this two hours for lectures, two hours for the exercises, okay? And if you're online only, then you can use the Teams channel to actually interact with the TAs. I think otherwise I've said all of this already. Recordings will come up at some point. I'm not saying when and, well, where is, obviously it's Panopso, but it might be, in three days time, it might be in a week. 

They will be there at some point, okay? Then maybe just my expectations for you because maybe this is not clear. And we do expect that you actually spend a nominal time on this course. That's nine hours per week. That is a lot. And you always say that you spend more than that on this course. 

And we always keep doubting you a little bit. But just be aware that this is a long time on average, right? I mean, nine hours per week, that's a really long time, I think. So all of this should essentially be boiled down to two hours of lecturing, two hours of exercises, and then five hours of your own study. 

This means about five hours ish, right? So this is where you read, this is where you do the homework, or attempt the homeworks, problems. This is whatever quiz you might not have actually completed. It's where you try out all the exams. It's where you also do the project work, ideally. 

We know that it's convenient to actually sit with your colleagues here, group members, to do that in this sort of already scheduled two hour exercise session. And then the exam on top of that. The purpose of this slide is just to make you think about what nine hours per week actually means for you. It's not about thinking about stuff while you're sleeping. It's actually working actively on studying, okay? Then we highly recommend group work, both during the lectures here, and I was happy to see that you actually discussed stuff when I put up the quiz. Then of course during exercises, please work on the exercises in groups. The projects must be submitted in groups of three. 

That's the target. The TA can approve groups of two. I can approve groups of one. So if you wanna work alone, and think you have a good reason for that, must be really good, then you email me, or this generic email we have. So definitely work together in groups and collaborate. Of course, the report must be the three of you, ideally, that submit that, and you must indicate who has contributed what to the report. All right, online help, I've already said that. 

Piazza, use that. It will be monitored by TAs and your fellow colleagues. Think about how you post question. It's not a good idea to say, can I explain a Gaussian mixture model? The end. Then I'll point you to a chapter in the book, and say, go read that, and come back, and explain to me what it is you don't understand. Be specific about what it is you don't understand. And also, if you actually, let's say, put up an exam question, put up the original text, put up the model solution that was in that exam set, and then it makes it much easier for everyone to help you. 

Right, almost the break in two minutes. I just want to say that this course is a prerequisite for a lot of other courses on, let's say, machine learning and machine learning related topics. These ones are taught by us in what's called the cognitive system sections at DTU Compute. We consider ourselves the core machine learning, teaching research at DTU. Then, but there are many others, let's say, application-specific, domain-specific, and applied machine learning courses in specific domains, where this is also very useful to attend this course. So I'm involved in some, what's called, molecular dynamics in machine learning, for example, is like one example where it would be very beneficial if you take a course like this. So have a look at all of these courses. They basically, in terms of complexity, go that way, maybe with a twist here, and then end up here. So if you want to specialize in machine learning, you go this way and you end with this currently. 

This one is tough, right? If you want to specialize in machine learning, it's not for people who want a broad knowledge of it. Have a look at these courses, if you're interested. All right, then a pretest, purpose is not to evaluate you. We never look at the individual results, but it is to know what you know, essentially. You can't do it all in this small break here, but do it at home afterwards or something like that, and we'll collect it and see where we are at, basically. So a small break, I'm on time, except for the 15 minutes we actually wasted due to IT. So I will, whoa, that's red. Okay, so I will, I will continue to a quarter past three, just so you know. So we'll take a break now of, let's see, let's say to eight minutes until 20 minutes past. 

Okay, a few people asked me about a couple of things. So I mentioned these second semester students. They are only the ones who are on the artificial intelligence and data study line on the bachelor. These are the ones I mean when we're on the second semester, only if you're on that study line, they're called kids in Danish. They have special rooms, and I'll come back to what their rooms are. Everyone else, you will be kicked out of there if you're not a kid student. 

That doesn't mean you won't get enough help from the TAs, it's just that the learning style of people on their bachelor's is different from the ones on the master's. So that's what we're trying to accommodate here. I hope everyone can have a certain appreciation of that. I was supposed to say one more thing, which I forgot. So ask if you're doubt about something. All right, so what I'll do now is like a two to four of machine learning in 20 minutes. So I will speak even quicker than I did before, hopefully, so we can get through everything I wanted. Yes, and we will try to actually lower the brightness on these slides because otherwise you need sunscreen to be in here. 

Okay, so I already alluded to this. So the main two areas of machine learning, we are going to be concerned about here, so-called supervised learning and unsupervised learning. There is a third one called reinforcement learning, and there was a course, if you remember, on that. So just to get that out of the way, reinforcement learning is where you use machine learning to learn a policy. If you know you need to make a decision today, knowing that you need to make one tomorrow as well. So you need to do planning. That's a topic we're not going to touch on. 

Sub-wise learning. I've already said that, so you can imagine you have an image. That would be our x here, somehow represented as a vector. It maps to a label that you have observed, so you have observed examples of x and y, in this case. So of the input, something you measure about the world. Image in here, it could be humidity. 

I go around the room, at certain x locations, I measure the humidity for example. That would be my value level. That would be for the regression case. But if you stick to the classification case, and the images we talked about like cats and dogs. Then we would somehow vectorize our image in this course at least. We would, through some model, some abstract mathematical model that depends on the parameters theta that we need to learn. And we also need to learn the complexity of the model. We will be able to, for any x, predict the y. 

There will be an example of this. Similarly for classification, I mean, let's say I go around the room, measure humidity at various places, not all the places, but I will be able to predict it. Let's imagine I only measure it over there, so that would be my x values and corresponding y values. 

If I go over here and plug in the x values associated with that quadrant or whatever the half, I would be able to give you a prediction of what the humidity would be over here. So the key point here is that we have this function that maps from x to y. We can only learn that because we have observed x and y together. So you go out into the world and observe these, or you look into a dataset, you look into a database. 

The answer, the wise one, that's where we don't have the label. You might have some labels, but not necessarily a label that would allow us to map from some input. You can think about having a collection of images. So you have tons of images of animals, but we don't have the label associated with each individual image. 

So we can't learn a model that maps to a label or category y, but we can still do interesting things. And we are going to see how we can do clusterings, outline and homily detection or outline detection, and association rule discovery. And I'll show you small examples of each of them. Classification, I've mentioned that a bunch of times here. So we have a data object, and that's something where you measure something about the world. You can think about your, that object has associated, let's say, measurements of the world x values, and it has also a label, meaning that it belongs to a class or category. Now we need that model. 

That was this abstract mathematical function of x given theta. Then what we want now is to act in machine learning, unlike other fields. What we want now is to be able to assign a label to a previously unseen image. This is slightly different from the main objectives in statistics, at least. So here in machine learning, we want to make prediction about unseen stuff. 

What could that be? Well, I mentioned the images of, so this would be a bunch of one, two, three, four, five images of a zero handwritten on proper paper, not an iPad. And you have a tons of these examples in your training set, and you also know that everything in this column belongs to category 27, I could call it, but why not call it zero, just for the sake of it? I want a map from images looking like this to a category called zero. 

And I want to be able to classify on these examples over here. It's easy for you, right? You can eyeball this and see this must be at two. As it turns out, it's not so easy for a computer, and you need to learn a function that maps from this input to the category two, in this case. And probably you want a map from that image to the category four. 

Okay, so how can we do that? So I said we need a vector, and generally that's not the case, but in this course we want to vectorize everything because the methods we're going to work on are mainly designed for vectors. A lot of stuff in the world can be vectorized, images, even graphs, and a lot of stuff is already vectors in the sense that they come in a tabular form, as we'll see. But in this case, what I'll do is I'll take my image of a zero, I'll just scan it row-wise, let's say, and then I'll just concatenate it to a vector, put it into a long vector. 

So what I have here is actually a long vector. And all the values here, the brightness, or not the brightness, but let's say that the grain is here, would indicate an numerical value. So zero, let's assume that could be one. 

The next value could be 0.5, and then there's a one, potentially, right? As it turns out, in this case, you have 784 numbers, but it's a big vector. And now we need to think about vectors in a vector space, and that's going to be an ongoing theme. But we can do that for all of our, let's say, images here, and now we have a vector for each of them. If you have a vector, you've probably heard about a norm. 

What is the difference between two vectors? Okay, now we can start to compare images, for example. So if you actually take that big 784-dimensional vector and somehow use our tricks from linear algebra and mathematics, project it down to two dimensions in a clever way that we'll learn about in week three, you might see something like this. Well, I've taken all the images and I've projected it down to 2D. 

So what this axis means, we'll come back to that in a later lecture. But what we see is some grouping here. Already, we haven't even trained a supervised model yet. We just projected our images. 

And let's see what happens. I've also, and this is important, I've color-coded the examples by the class to which they belong. So the zeros here are over here. The one seems to be distinct in a sense. It's probably, in most cases, just a straight line. 

So easily distinct, easily to differentiate from the others. Okay, everything else seems to be a mess in this 2D world. But what if I use 3D, 4D, or even the 700 original dimensions that I had available. So there's something here where you need to get your head around the fact that we went from an image into a vector. That vector lives in a 784 dimensional space, vector space, to put it into proper mathematical terms. That vector space, we can use everything we know from linear algebra. We can rotate, we can project, and so on. 

So we can apply a linear map to it. All of this is the stuff we'll go through next in two weeks. All right, but anyway, if you trust me, we can project it into 2D. And here, I might actually just, if you could draw a line somehow, here. I could actually classify these examples of ones from everything else. So if you, in this 2D space, can define a line here, then I can test for a new image that comes in. So let me draw the line. 

Maybe something like this. It's not perfect. It's never going to be perfect in this course, in that there's noisy images, there's overlap between concepts and so on. But if I now bring you a new image, I'll make a big dot here. That means, let's put it up here. That means you take a new image of a handwritten digit. 

If I know the mathematical function for this line, and we'll get to that, then you can say it's on one side or the other side. You can use a computer to check that. And that is essentially a simple classification rule. Of course, now we're doing it in the 2D space, not the 784 dimensional space. 

But anyway, it's doable. Okay, so what we'll do here is just very quickly have you think about this problem for a bit. So this is, again, maybe a slightly more proper quiz question and an exam question that you might see in this course. 

Remember, you can't bring your book to this, so in principle you should have learned how to do this without any aids, except a basic calculator. So if we just read through it while I keep talking a little bit here, so there's one thing you need to know, and that is that the max here of something, let's say two numbers, let's say minus 10 and zero would be a good thing here, that's zero. So it's simply just the maximum value of these two things that goes in this argument here and here. 

So we observe something in 2D, so that's your input. The class label would be the color blue or red. Now you need to work out which one of these conditions is true, given those decision rules here. Of course, in real life you would learn these rules, but here they're given, and now you need to check which one of these is the correct one. So I'll give you a few minutes for this, but I'll probably interrupt you like a few minutes in and then go through it. So just think briefly about how you would do this. In the interest of time, you would normally have like seven, eight minutes to work out a problem like that on average. This one is an easy one, so I would say maybe two, three minutes or something in an exam. 

You should be able to identify what do I need to do here and how to rule out things. Of course, this is just an example, so don't take it too serious if you didn't get to the right answer here. Now there's two ways of solving this. 

There's a very rigorous time-consuming way of doing it, time-consuming for me, not necessarily in real life, but there is a trick here that I would maybe sort of give you already, and that is one way to solve problems like this is to simply say, okay, I'm going to pick a clever point now, and of course you experience, you'll be able to actually figure out what a clever point is. So what I'm going to do here is say that my x and y I'm going to consider is zero, zero. So if we look into our sort of, let's say, problem here, then we'll see if I go out in the world and measure something that's zero, zero, where would I expect to be? Well, zero, zero, I would be here, so let me draw that. 

It's right here, so I would expect a coordinate of that to give me a blue, let's say, output. So if I now plot that into my two sort of equations here, if I plug in x, that's zero, what's the maximum of zero on minus one? That's zero. I plug in zero here again, so I'll get the maximum of zero and minus one. That's zero again, so this would be zero. Okay, what if I plug in, so remember x is zero, right? 

And I do the same for set two. I'll plug in zero and I need to take the maximum of zero and minus one. That's zero. 

Okay, so in this case, set one and set two will be zero for that particular test point that I chose. Okay, but we know that it should actually be blue in this case. That should be the output. So let's check which of these matches that. So let's start with one that doesn't match D, for example. So here it says that if set one is zero and set two is one, classifiers blue. 

Oh, that's not correct, right? Because for that test point where we know it should be blue, we actually got zero and zero. Okay, so let's, the correct one would be A, where set one, set two, zero, classifiers blue, otherwise it's red. So this is one way of actually getting, let's say in this case, getting to a correct answer. It's not always a failsafe way of solving these problems because one of these could simultaneously be true, but they're not in this case. Of course, you need a little bit of experience looking at these problems to be able to make that inference straight away. Okay, so try to solve this, ask your TA if you're in doubt. 

All right, and again, the solution will be in the uploaded presentation version of the slides. So regression, we talked about classification. We could also think about regression. 

So I again go out and measure something in the world and now why is my target, it's not an input like on the previous slide. We will assume that we can just switch between notations and retrieve it from the context. So here I have X, let's imagine that example from before I go out, I go somewhere in the room, let's say up there. Along the way, I measure certain humidities along the way. Those are the blue points. 

I consider that my data set. From these blue points, I am now interested in learning a function such that when I go up a place where I haven't measured before, I can actually make that prediction of the humidity, my Y label. Okay, so the goal here would then be to first of all figure out what kind of function do I need for this green line here. Here it's a sinusoid, so it's relatively simple, but is it a good match? 

Well, it's a sensible match from just eyeballing it here, but of course we need to be a little bit more rigorous about this. And that's what we will talk about. How to actually figure first determine what kind of function do we want here and how do we fit it to our data. And the way we evaluate if it's a good fit would be that we keep out some of these blue points, do not actually use them to determine the parameters in our model, but then we can use those points that we've held out to actually test the performance to figure out the generalization error. 

Much more about that later. But generally, the key point here is we measure something about the world at a certain location. We can fit a model such that we can make predictions for unseen places here and predictions about a continuous variable, not a category. So what could be examples here? So you could think about predicting sales amount based on how much you use on, let's say advertising or the PR department, or on your stuff and administration in one on one. 

And we could think about wind velocity similar to my other, let's say humidity example here as a function of other, let's say parameters like temperature, humidity, air pressure and so on. Or it could be if you're into buying stocks and those kind of things, then you could try to predict the stock market in the future, for example, using these location things. And most of you would have seen functions like this where we have still the blue points, the training set. We have straight lines. 

You would have seen that in high school. You would have seen that probably in production to statistics as linear regression, except here we have an explicit goal to be able to do well outside of these blue points. You've seen something in 2D and this is where the multivariate mathematics come in. We now have a function of two variables, X1 and X2, for example. 

We now want to predict Y. And you might have seen functions that looks like this or parameter rise like that. That's a hyperplane in 2D. 

Now, we're going to certainly see this. This is a simple thing of the functions we're going to look at. This is something where you have polynomial terms in your function here. So we have a vector containing the weights or parameters associated with that. We'd eat sort of element in my X vector. And what can I do here? 

Specifically here, I can do a polynomial in 2D like a ball basically. That's what I can do here. So we can build on this. You can put third-order terms, say, eight-order terms if you want. So we need to choose what's the complexity of our model also for the regression problem. And now sort of a similar sort of small thing here about functions. Again, well, you observe some red points. We have fitted a black line here, which is one of these four functions. So this is just to make you think a little bit about what's going on here in terms when we say function. So let's see. Again, I'll give you a few seconds to just read through this and think about how you would do it. Maybe try the same trick I used before. 

Choose a very clever point. Maybe X equals 1 and see what happens. Oh, sorry, zero and see what happens. So just again, feel free to discuss, but I will interrupt you in a few minutes. I'll grab some water. That's the time we have available too. Does anyone have a suggestion here? 

So the answer is C, and that's correct. Do you want me to explain why that is? Or do you want to come up here? No, no, no, no, no, no, no, no. It's the correct answer. 

And of course, I gave you the hint here. Try to plug in zero and see what happens. So we plug in zero, X equals zero, and then we noticed a few things. There's sort of a trend here. Okay, so maybe we can already see that it shouldn't have a negative trend like it would get here. So a line like that would be something along this. And then basically we ruled out two options now, and this could be a strategy for solving these kind of problems. Now we simply need to work out is it this one or this one? 

And now we need to remember your sort of cosine, sine rules, sine of zero is zero. Yes, so that's the one. Okay. Again, here I gave you the functions. 

We want to learn these functions. Okay. Then how are we doing on time? 

Okay-ish, given the end time of a quarter past. So if we now don't have a label, so you go out and you observe a lot of images. You take a lot of images of the world. 

What can we do with that? You don't have a label. You don't know if it's a cat or dog. 

You don't know if it's zero, one, two, three, four, or five, six, seven, eight, nine. Okay. So just imagine images of animals. What you're given here is some, let's say, vector representation of that. So in 2D, we've done the same we did before. Then we see something. So when you plot it in a scatter plot, which we'll hear about if you haven't heard about that before. Okay, we plot, we can eyeball. In 2D, we can eyeball what's going on. It seems like there are two clusters, or we'll call them clusters, three clusters, sorry, here. But you could also see something else, namely that it looks like that the distance between the elements in these clusters are closer to each other than there are to any elements in the other so-called clusters. 

But all of this actually comes down to something very important. How do you measure stuff in a vector space? One way to measure distance is what I sort of did here. It's simply the straight line distance. 

You've heard about that one before. Euclidean are the two norm, LP2. Okay, so that's one way. I could also measure distance in another way using another norm, and I can use one that's called the Manhattan distance. So I could also measure distance between this point, this point, simply going up here first and then this way. That's another way to measure distance between data objects, and we are going to look into that next time. 

Just be aware that, of course, the properties of whatever algorithm you then use to cluster things and group things afterwards will depend on what similarity measures we use. But the important stuff here, regardless, is that, okay, let's assume we just use straight line distance, get away, there we go. Then we eyeball stuff at least using the Euclidean distance. We can apply some algorithms. We will look at K-means and Gaussian mixture models for, let's say, finding what we will call clusters, and we see we get a yellow, blue, and whatever it is, reddish, brown, or whatever it is. 

Okay, but how did we actually learn about these colors? Because that would indicate that we have a label for these clusters, but they were not part of actually finding them. That means grouping two points together. They often arise from a post hoc analysis where you then go out and ask an expert, okay, this thing here with pointy years and a tail and whatever it is, what is that? 

That is a dog or whatever it is. So the important stuff here is that we will definitely look at clustering methods and we will look at similarity measures and different algorithms associated with that. Now, I'll give you an example of clustering that also part of the exercise today, and maybe if you run out of time for the exercise due to IT problems or whatever, then maybe skip that one and the TAs will tell you about that. But I'll give you the gist here and then you can go look at it in more detail. So imagine that you are a user of a library that actually has like real books like I use for the doorstop. So you go in there and you say to them, I have this book, I want to find something similar to that book in the library, but it has millions of books. So one approach we could do here, we insist on this vector notion of how to represent things in the world, we could simply look at the frequencies of words in those documents and maybe only focus on the most frequent ones. So we simply go into all the books in the library, convert a vocabulary and say how many words of this particular type is in that book. We do that for all the books. So now we have a bunch of different vectors, one for each book. And if you then come with your book and say, I want something similar to this, we can simply look at the distance between whatever was in the library, that means the vectors and what you come with. 

And we pick the closest ones or the set of the closest ones. Okay, so once we have these vectors, what we can do here is actually define the similarity measure based on them and that's what I already said. I assume we can compute distances between vectors that represent the books. In turn, these are the most frequent words and how many times they occur in each book. So we can, we have, we can perform clustering and that is to some extent what we also call topic modeling. If we actually do that clustering and say, okay, these thousand books seem to be about the same thing. 

For example, politics. Just based on the frequency of the words in those books. Okay, so as I said, we can use that to actually search for things or we can use it to actually associate with a particular new book a topic. Maybe that would be relevant for categorizing things and ordering and archiving stuff. 

All right, so that's sort of just one example and we'll see how we can actually go from a document to a, let's say, a vector like this in the third part of the lecture, which we will hopefully get to. All right, then one more thing we can do in this unsupervised setting. Remember, we don't have any labels. It's actually to figure out if something is odd, if something abnormal or an outlier. So again, I'll just give you two seconds, I mean literally two seconds, and think about, so these are measurements, two measurements about the world, temperature, humidity in this room, at various places, and then you ask of you, is something weird going on? So remember, each data point is associated with two numbers, right? 

What is the temperature, what's the humidity? So if you look at this, okay, something seems to be going on, this one seems to be slightly odd, right? The distance from this one to the nearest point is, this big, this big, okay. So I would probably say this seems like an outlier, but why am I not saying that this one should be an outlier, or a not one? There's a specific reason for that, and that is that we seem to, with our eyes and brain, we seem to associate this point with this group of points here. So we sort of think there's a linear trend here, and that this point here should actually just naturally belong to this one, because this one must extend down here. So again, if we just look at pure similarity, maybe this one will get flagged, and this one will get flagged as an odd one out, or not an odd one out, sorry, but an outlier that you may want to actually have a look at. 

And we will use what's called density models, and also these sort of more intuitive distance-based models for doing that. You can also say, what's actually going on with this stuff over here? Is that perhaps even the normal behavior? Okay, so now we actually have something that seems like two groups, at least. So which one is the normal, which one is the abnormal, are they all normal, so to speak? So the problem with this is that it's actually a pretty well ill-defined, sorry, problem, and you often need domain knowledge from whatever domain you're working in to identify whether or not the data point you think is an outlier, is actually an outlier. So don't just remove stuff because you think, ah, the distance is a little bit high, so I'll cut them off at that point. 

So we'll look into this, and in particular, the, let's say, maybe mathematically slightly more difficult thing, namely, mixture models for doing density estimation here. Okay, so again, you can imagine where this is relevant. For example, if you're a big Danish company building wind turbines, if suddenly you have a normal behavior that's here, and you have a group of points that sort of day by day captures this, and then suddenly you see a point over here when you measure vibration, for example. Then maybe you should flag this one over here as an outlier, and something needs to be done to that wind turbine, simply based on a notion of what is normal. We don't have a class by saying it's abnormal, it's just because it's far away from everything else. 

Okay, then the final thing about the unsupervised stuff, that is what's called association rule discovery, and here you should think, I'll actually do this by example, because I think it's much easier. Some people say this is true, I'm not sure. It's an anecdote from the US. So imagine you're a big American supermarket, you keep track of what people put in their basket as they go through the supermarket, and also sort of in the end figure out on your receipt what have you actually bought. So we record what's called records here, so a record is basically an observation that one person, person one, bought bread, coke and milk together, and of course the stuff you drink, and then you see a lot of other, let's say, observations together here. So based on these co-occurrences of, let's say, milk and beer, you're interested in extracting rules that are, let's say, frequent, and likely to occur, because those rules might be indicative of how you should place stuff in the supermarket. 

So in this case, in this example, which I'm 90% sure is made up, then we extract these rules. So it turns out that if you buy milk, that means if you go through the shop, you put milk in your basket, then you're also likely to actually buy a soda. Okay, what's slightly more concerning is that if you also buy diaper and milk together, then you're also more likely to buy beer. 

You can, there could be many reasons why that's true, but I'm not sure it is, but regardless, maybe if you're a supermarket, you may want to break this association by placing stuff in different places in the supermarket. So these rules, we extract by co-occurrences and we'll use probability to actually do this efficiently, or tricks from probability to do it. It's a relatively simple algorithm, but that's week 12. Okay, so we can do stuff like that as well. Now, just before the sort of break here, then I will go through, just summarize maybe what we've done here in the last 20 minutes, and that is essentially say, we have a training set, we have a so-called test set or an unseen data set, we want to train a function, that is my F here, in maps from the real numbers in some dimension here, 700, to a category. Simply that means we are able to predict something now, so we're able to predict, give a probability if this is a 0, 1, 2, 3, 4, 5, 6, 7, and 8, and then we pick the one with the highest probability, and that's hopefully two. 

That means someone has done it right, at least in my view. Okay, but the point here is that we will call this function a model, and this is terminology. So, we take our training data, we have our model, which is an abstract mathematical function with some parameters that we need to learn from our training set. That gives us an ability to actually squeeze through our function, once trained, a new image, and get a prediction. Okay, the key point in machine learning is that we actually want to measure or estimate what's the generalization error. So, that is how often do we actually make a mistake on this unseen data? Can we quantify, can we evaluate that? 

Because that is the number people are interested in, if you are going to sell them a machine learning model, you claim can predict something, or new unseen data. So, we are going to talk about that. So, let me just think for two seconds here. I will take or do the last, this is really annoying, the red stuff, isn't it? Anyway, I will do the last section in five minutes. So, let's just do a very short break. 

You can stay here, you can leave whatever you want. But that will take 20 minutes. So, we are almost on time. But hopefully next time the IT people will fix whatever is wrong in here. 

So, be back at three o'clock. Okay, so in general, I have already emphasized this, that it is important to think about the data and basically what is the domain you are working in. So, in general, you know the term garbage in, garbage out, and that's exactly true for machine learning methods. It's even more difficult here because I can give you pure noise, and the data will be pure noise, and I can come up with a model that fits this noise perfectly on the training data. And the problem being it's pure noise, so there's basically no meaningful signal in that, but you can get perfect score on the training set. 

You'll do extremely poorly on whatever test set you might have. But it just emphasizes the point here that we can build really flexible functions that can fit any data if you want to, but we don't want to. And we need to think about what methods is the appropriate one, what method is the appropriate one for that particular data set, and that depends on the attribute types you have in that data set, and of course the data set type. Okay, so what is data, and generally the way we think about this is that it's a collection of data objects, that's sort of general term, and their attributes. So, in this case, we have seven different observations here. We have certain attributes, we have three attributes here, and like the exams I've gave before, an attribute could also be, let's say, going around measuring what's the position in X space in here, and then another attribute could be the humidity. 

And then from the X coordinates in here, or XY coordinates in here, I can predict the humidity. But generally we call them attributes, features, or variables, or characteristics, and just be aware of these many names for the same thing, essentially. Okay, generally in terms of our attributes, we think about two distinct types, namely if it's discrete, that means something you can count. 

It could be zip codes, it could be, yeah, counting how many people are in here, or it could be like we talked about before, sets of documents in a, sorry, sets of words in a collection of documents, or in a book, for example. And if you think about the computer, then you would often represent this with an integer value, like one, two, three, four, five, for example. Whereas in the continuous setting, that would typically be stored as a floating point variable, and even though that has, let's say, finite resolution, when you think deeply about it, we still consider this a continuous variable or attribute, because that's what it naturally represents. Okay, so real numbers and examples could include, if you measure temperature using a continuous, let's say a thermometer, you measure temperature, and you measure whatever height in here using some measurement device. That could be continuous. 

Of course, for discrete, that would be generally discrete, or it could be binary, zero, one, for example. But it could also be like eye colors, for example, blue or brown or green, whatever. Okay, so just be aware of that. That's one thing you need to distinguish when you're looking at data. Then there's a, let's say, subdivision here, or another additional, sorry, I should say, way of categorizing your data and your attributes here. 

So each variable. We talk about nominal-ordinal interval ratio, and this is a hierarchy. So if you're a ratio, you also interval, if you're interval, you also ordinal, if you're ordinal, you're also nominal. The key point here is that your attribute, it may make sense to actually compare them, are they equal or not? So you can think about ID numbers, or as I talked about before, eye color. Two people have the same eye color, brown, for example, so you can do an equal sort of operation in terms of mathematics. 

Then there's a slightly, let's say, types with more mathematical structure to it, if you want to talk about it that way, namely, ordinal. Here it makes sense to actually compare things. So you can, let's say, rate the taste of potato chips on a scale from zero to one to ten. And here it makes sense to say that a potato chip with a score of nine is better than one with a score of one, for example. 

So you can rank stuff. Also grades in the Danish 7-point scale, for example, twelve tends to be better than ten, but actually the distance between them doesn't necessarily make a lot of sense. And that becomes clear here, if you think about heights, for example, so short, medium, tall, it doesn't say anything about what's actually the distance between short and medium. 

They're just two categories. Yeah, they're too late to height, but it's not that the distance between short and medium is the same as between medium and tall necessarily. Okay, or how broad these bands are. Okay, so it means we can't actually subtract short from tall and get a meaningful number. But if we are now interval, if we have an interval attribute, you can actually measure distance between attributes or values. For example, calendar dates, it makes sense to talk about a difference of two days between, yeah, today and then Thursday, yes. And it makes sense to talk about distance in, let's say, temperature in Celsius. So if you have 10 degrees and four degrees, this definitely makes sense to talk about a difference of six degrees. 

That was not possible up here. If it's only ordinal, you can only say, well, it was a high temperature and a low temperature, but you could not subtract them. Okay, then there's another fine distinction here called ratio. This is where zero, the value of zero has a very distinct meaning, and it means the absence of what is being measured. 

So think about weight. If I measure an object, this is almost zero, but it's not zero. If it had been zero, there would be none of this object. Okay, so it has a physical meaning, the value of zero. And if you now think a little bit about one of the tricky ones here, is that we have temperature in, let's say, Celsius here and one in Kelvin. Why is the, let's say, an attribute where we measure stuff in Celsius, why is that not in ratio? And it has to do with the meaning of zero. So zero in Celsius is simply the point where ice melts. 

Okay, would someone, let's say, another way to distinguish these two would be, say, if someone from another civilization, like aliens or whatever, come to Earth, would they actually put the same zero on the same scale? Probably not. They would maybe, maybe not, but it's not a definite. But they would definitely say that in Kelvin, where zero means the absolute zero, there's no movement, there's no energy in the system, that would be zero. Someone else from outside would agree with that notion of zero and what it means. Okay, so this can be tricky, the distinction between these two, so just be careful here. But one way to distinguish them would be this, ask yourself, does zero have a physical meeting? And let's say that it means the absence of what is being measured. 

And would someone else, for example, aliens, would they agree on that if they were to rebuild civilization after some climate crisis or whatever it is, right? But ask yourself this question if in doubt. Now, normally in an exam, because we sometimes also get a little bit confused here, so normally we would make sure it's an obvious thing. 

So sometimes you need to debate a little bit here and argue from which viewpoint you actually make this distinction. Okay, this one, I'll leave that for the TA session and you can have the TAs actually go over this with you. So there are some tricky ones in here and if you, the TA can answer it, then come see me, I'll be walking around. There are also a few correct answers here. Okay, but just to practice this a little bit, here's another quiz and again a few minutes for this. So I hope you can read this because I decided to put in this thing here. Is it okay to read maybe further up? 

Anyway, this is what we're going to do. So here you have a problem where you're given, again a typical exam question, you're given a number of attributes, you're given something about what these attributes means and then based on this sort of, let's say, way of categorizing stuff from before, first figure out are the discrete continuous? I don't think you actually do need to worry about that here. And then figure out are the nominal, ordinal, interval, or ratio attributes. So there's a very tricky thing to this question. 

I don't know if anyone has noticed. Logic wise, this is a really tricky question. So from this categorization, it's actually correct according to this, but it's not the answer you're supposed to give us. It's negated. 

Have a think about this for a few minutes and the way I would do it if I were you, simply go through here and say, it is named discrete and nominal and then say a tick mark, if that's the case, you do the same for everything else here. And then afterwards, you figure out which one should you actually answer here because we've negated the question. This is really, really evil whoever did this exam question. And I will try not to do this. 

I will try not to do this. Okay, so go through this and read this a little bit and see if you can feel free to discuss. I'll give you a little bit more time this time. So discuss with each other. Let's get on with it. So let's do them one by one and see if we can actually categorize them. 

So I'll start from D. Just that's the easy one here. If you're ratio, you'll also be interval. Remember, I said this is a hierarchy. If your ratio you also, oh, sorry, then you're inherently also interval. Just means that the way you should characterize it is ratio because it adds the more structure to it. 

It's the more precise definition. So I'm not going to answer the question yet. I'm just going to say if the statement is true. Okay, so don't be confused. This is not what you're supposed to answer here, but I'm just putting a tick mark saying that statement is actually true. 

Then I'll, I think I'll jump to A now. So name is discrete and nominal. So where's name is down here. It's just the name of some serial brand. Can we subtract two names for two different serial brands? Make sense? 

Probably not. So it's discrete and nominal. Okay. Is that also what it says? Yes. 

Great. That is also not what you're supposed to answer in this question. Then we take pro about prod, fat and salt. So some elements that they put in the serial and we ask ourselves, are they continuous and ratio? So how they measured so pro what was it? Fat and salt. 

So they're here. They all have to do with the weight of that particular element. So if there's zero weight, there's nothing of it. So at least that's sort of a tick that mark that the absence of it means something. And it's something that's measured on a scale that's continuous. 

So it's continuous and ratio. Would anyone disagree? That's really not nothing to disagree. 

We have some not, I'm not trying to put you into a trap. But that's also correct. So of course, if you trust yourself now, you know what the wrong answer is, but let's just go. 

What the wrong answer is that you should actually answer here. It's very confusing. So if you look at type and bit, if you just first look at type, it's sort of just another word for category. So maybe we can already sort of see what's going on here. 

So let's see. Is type discrete? If you look at the type of the serial, well, actually it's the way it's served. It's discrete. Certainly there are two options. It's even binary. Okay. But is it also ordinal? 

No, it's not. Why not? Because zero is basically it's arbitrary. You could flip them. I mean, it doesn't make sense for the task you're doing here. So and it doesn't mean that just because it's zero and sort of cold is better than served hot in any sense. So there's really no physical meaning here. 

So this is actually not true. And this is the answer you're supposed to give because we're looking for the incorrect statement. Okay, I'll try to avoid these in an exam. 

Good. But practice these, especially on these ones that I showed you before, practice these and go discuss them. Sometimes it depends a little bit on your viewpoint what you're supposed to do here. Let me quickly just read types of data we have. We have record tabular one that's basically stuff that's already in a table. 

And we've seen a couple of them. We have relational data where you're looking at relations between objects. We have ordered data that could be time series. That's a sequential nature to it. Okay. Tabular or record data. 

Generally very simple. We've seen it with the market basket analysis. If you buy bread soda and milk, we can have an attribute cotton bread. If it's one, you bought bread. 

If it's zero, you didn't buy bread in that sense. So here we have an observation here. It's essentially a vector. It's already a table. That's tabular data. That's what we primarily focused on. We can turn a lot of the other things into a table if you want to answer an example. 

Okay. We can also think about graphs or relational data. So we know that Alex and Ben are maybe friends on Facebook. We can represent that also as a matrix. But the key thing here is that these columns does not necessarily, or they shouldn't be interpreted as attributes as such. But this is just a different type of data. 

There are specialized methods for working on graphs, graph neural networks, which is really interesting, for example. But you can also extract features from this and represent it as a vector if you really want to. But in this course, we prefer not to look at these also in the projects. 

I will discourage using those. Sequential data, time series, and other type of data that you might want to look at in some context. Again, the method we were looking at here, not really suited for that. But if you know already how to extract, let's say, a standard vector from this time series, then you can do that. So here, actually, the observation is the full time series, for example. It could be that, or it could be that we simply know there's this temporal ordering between one observation to the other, like it's shown over here. 

Just be careful and talk to your TA if you want to work on data like that. Okay. Be aware of quality. 

I said garbage in, garbage out. That's definitely true. Make sure your data is suited for the particular purpose you're interested in. And now I'm referring to the projects because we see sometimes people get lost a little bit. So there could be many problems here. 

It could be noise outliers or missing values in your data set. Be aware of that. We'll, in the exercise, ask you to look in for that and you should do it in the reports. Noise could be essentially unwanted, perpetrations to your data. So if you're working with EG and measuring time series, just the fact that you're mostly interesting in monitoring people who are live means that there's a heartbeat. That's probably noise in terms of the brain activity you're actually trying to look at. Okay. You can filter that out using various methods. And if you see a noisy, let's say, attribute or a noisy example, maybe think about removing them for the purpose of your project, but you need a very good argument before doing it. Yeah. 

Outliers. We talked about that already. If you see something that's like an extreme value and you think it, or you can argue also in your report that this is actually an outline, I have removed it because it makes perfect sense to remove it. 

By all means do that. Don't just remove outliers. But it could be a measurement error where just once in a thousand years, it gives you a very weird value. Maybe that's not useful for training a good model. So in that sense, it might be meaningful to actually remove it. 

Just be aware of all of this. Missing values, it's sort of maybe obvious, but it's something people often miss. And the problem here is that if you're trying to do machine learning on this, then you do all sorts of hacky things. 

It's okay to do things, but you just need to be aware and argue why you're doing these things altogether. So here's an example where we actually observe missing values in some of these attributes. Why could that be? It could be, well, maybe some people are not very keen on revealing their age. 

Who knows? In this case, it's a male. Maybe society would say it's more often a female. 

Who knows? But maybe also people are not necessarily keen on revealing that gender in this data. Maybe because they don't feel like they fit into one of these two categories, or because, well, I don't think it's any of your business. 

The name, of course, here again, an example where someone may not want to give you their name. So just in your data, you are actually looking at, just be aware of all of these things. And they're pretty intuitive. Just make sure you have good quality data and you know what you're actually looking at. So some of this stuff you can remove, but you can sort of see, if I remove everything here that has to do with missing age, then I'm left with only half the data set. 

What to do? There are ways where you can actually impute values here. You can simply take the mean. That's one way of doing it. And it's not necessarily a very good way always to do that, but that would be one option to do that. The same for gender and name. Okay, so just be very clear about what you're doing here. 

Then have a look at this one for just two seconds. Someone on a piece of paper, not an iPad, were asked to write down some things on a piece of paper. Basically, how many children do you have? Okay, their response was this. Now a poor research assistant were asked to actually take this, put it into a vector that someone could actually use for something sensible in a machine learning context. What could possibly go wrong, you could ask yourself, but if you look at this, there are definitely things going wrong. 

Let me point to a few of them. I don't know what minus two is, but that's a really cynical thing here, right? Well, try to think about it what it might be. 

It's really sad, right? But maybe if someone indicates minus two because something bad happened, or it could just be because I don't know if this is a seven or two. And then now you already have noise and you have corrupted data and all of that. I don't know what this is and why it comes to zero. It should probably be indicated with a missing value. Again, just be very aware what's actually in your data. So think through this and try to do the matching and see if you can actually spot any other mistakes. 

There are certainly some. Okay, then what can we actually do here to deal with our data and subsample and all of that? And typically we talk about four different things here. So sampling, we can take a representative subset. We'll do that when we talk about splitting data into trainset, for example. We can select some of the attributes slash features. If we want to only look at the importance of certain features, for example. We can do feature extraction. So that means from the existing features we can make combinations of them or we can transform them. Sometimes it's good if you have a very skewed distribution, take a log transform of them to get a slightly more relevant distribution. At least relevant in the sense it's easier to model. 

And we can even predict data. I've already shown you that. So maybe if you have 700 dimensions, that's very difficult to view, at least in my mind. We can view stuff in 2D, maybe 3D. There you can see if there are any patterns. Okay, we can think about this feature processing. We can eliminate, suppress, or attenuate certain aspects. So one of these things could be that we remove a linear trend and perhaps try to do a modeling there. It could also remove the background in the images, for example, to focus on the most important things. Or filter, like I talked about the EEG example, we can put a filter on it to remove certain aspects. One thing I'll come back to next time, so I'm not going to spend too much here, but it's actually to standardize your data. Such that we know that they all live on the same scale. 

What can we do? Well, if you have stuff in a matrix like this, we can simply take the mean, subtract it from all the elements in that column and divide by the standard deviation. Now all the attributes here columns, they live on the same scale. Why is that important? I'll show you a very explicit example next time, because you can imagine if you have things living on a scale from one to a million and another living from zero to one, if you try to compute distances between those things in that vector space, one feature will dominate that measurement. So we need to think about this, and we'll think much more about it next time as well. I alluded to something where, maybe I didn't, but the thing is to be always, to some extent, interested in working with vectors, numerical vectors. So if I give you a data set like this, well, some of them are numerical values, they're even well-aged. 

If that's probably continuous and ratio, if you have an age of zero or you're not born yet, or at least if you define it like that, we have height, continuous ratio, continuous ratio, then you have something else, namely a nominal discrete attribute here. How can we present that in a numerical vector? What we can do is do a so-called k, one-hour k encoding. So in this case, I look at the fact that I have three distinct values. I can simply create a new attribute, and this is an example of doing exactly that. So I'll call this one, this column, the third one, Sweden. I'll call the next one, Norway, this one, Denmark, and I'll simply indicate in that column for that attribute was the Denmark-Norway suite. That's one way to turn something that's tricky, at least on the first glance, into something that's numerical. 

We can put this in a vector, we can compute distances between this vector and this vector. Good. What will I do? Okay, let's continue. 

We're already stuck in the mod here. I'll give you an example of a feature transformation where it relates back to what I talked about, these books and the frequency of words. How can we actually take something, three sentences here I should have put in the italic, but we have three sentences about what a bag of words representation is. 

Okay, so what I'm interested in now is taking this sentence and representing it as a vector. And all I'm going to do here is put all the sentences in a bag and shake it a little bit about, that means I lose the order. And then I'm also going to remove the various forms of the words. And then I'm going to count how many instances of each word that occur in each of the sentences. That is essentially what we're going to do here in a nutshell. So what I'm going to do here is ignore frequent words, only focus on what actually gives information. 

I'm going to count how many of these words are actually in each of the three sentences. I'm going to put them into a table. So now we have tabular data or slash record data. That's something I can put in a matrix. 

I can compute distances between things if I have stuff as a vector. Okay, so I have all the possible words I'm considering here. So now I transpose the matrix, so to speak, because now I actually have the attribute here, is how many times did that word occur in sentence one, two and three. Okay, so I'm simply counting in sentence one, I have bag of words, the text written once, and so on and so forth. And then I'm also just going to remove basically the... reduce them to the basic form of the word to reduce redundancies and different sort of forms of the word. So plural, single, for example. 

I don't care about that. But what I have now is actually a matrix with some elements in it, and I can compute the distance between this sentence and that one. And here are sentences, but they could be books like I alluded to before. You'll do this in the exercise. 

Can I do the same with... almost there, bear with me. Can we do the same with image representation? And I've already shown you this. We can take the image, here handwritten zero, it would appear. I can scan it row by row. I can flatten that matrix, I get out of that at the table, I can flatten it into a vector. So now I have a one by seven hundred and eighty four dimensional vector with the pixel values in there. Well, if I have multiple ones of these images, I can take one image over here, so there was a three. That's the first vector in that matrix, so on, so forth. Okay, so now I have a matrix with all of my data in it. Yeah? 

Easy peasy. What is the key thing you need to see from this? I took document sentences, I put them into a vector. I took an image, put them into a vector. I claimed that we can do similar stuff with other data sets, potentially. But the important thing here is that we are very keen on working in vector spaces because exactly we have that one data point, this one, lives in a... in this case, three-dimensional data set, where the coordinates are exactly the elements in that vector. 

Let's say, I don't know, 0.3 comma 0.5 comma 1 or whatever it is. Okay, so I have a vector with these three elements. It lives in that three-dimensional vector space. 

I know from Linealiber how to rotate this, I know how to project this point, maybe down to this subspace. And all of this, we'll talk about much more in the coming weeks. Okay, but really try to think about this vector perspective. Good. Three more slides. Yeah? 

You do the exercises, I've been telling you, do those. You need to install Python. For those of you wanting to work with Python, that's the hardest part of the course. And if you have no experience with Python, choose Python, choose Python, choose Python, it's the one way to go. 

TAs will help you learn Python, even if you come from Matlab or R. Yeah? We have certain rooms. They are on the web page, so I'll not go into that. Just remember what I said about these AI and data students. 

I think there was some over here. Constantly against the data students, you have your own rooms for a reason. And I've justified why they have their own rooms. Good. The rest of you have many excellent TAs, PC students, master students, so on. 

It's on the web page. Then we're in a big auditorium, the biggest on DTU. It's not big enough, unfortunately. So we have people in here as well. We have people on Zoom, of course. But we have a lot of different rooms in this building. And what we are not going to use is this area here, but we have this room, sorry, and I think a few of these side rooms and these two. And then we have two rooms over here. So please, if you don't know where to go, go over here. There's like room for 90 people over here. So make the way here, go up here, and then it's in here somewhere. Those are the room 40 and 42. 

But spread out, I would say, generally. Okay, you can also stay in here. Okay? Final slide. 

Shhh. Final slide. Follow the instructions today, and the TAs will have something more to say about how to do the exercises. Start forming the groups. As I said, target is three. TAs can allow two. 

I can allow one. If you can't find a group, then there should be a sort of spreadsheet on teams where you can enter your name. There you can try to coordinate that way. Put it on Piazza if you're looking for a group member. 

Okay? Then start looking for a data set as well and discuss this with your TAs so you don't get into this problem of choosing a data set. There's too many missing values. 

Actually, it's a graph, and we don't know how to do graphs yet. Okay? So that's it. Well, a bit over time, but anyway, just to be clear, we might go a bit over time because we're restructuring things a little bit. 

But of course, this today was mainly due to other people. I would claim. Go do the exercises. See you next week. Bye. 
Speaker 1: All right. Thank you. Can you hear me in the back in this room? Okay. And I think some people can also hear me. Yeah. Maybe still. Good. All right. Are you ready for the red sun here with the front page, which seems to light up this room like, I don't know what. But anyway, my name is still beyond. I'll be giving this lecture in week two of introduction to machine learning. We're going to talk about summary statistics, similarity, which is sort of the main thing. You should really get out of this and some entration about visualization. And as you see in this screen, there's some feedback groups you are supposed to just reflect a little bit on, especially, let's say, the lecture, the style, the tempo and whatever, even the YV systems. 

And also the sort of the exercises. And please do write something about that. And I might have a few words to say about that later on. Sounds like the microphone is not on. Is it actually on up there? No. Is it on now? Now? 

Now? Yeah. Okay. 

That's a bit too loud for my taste. Okay. It seems to be on now. Yeah. Otherwise, start waving up there. Yeah. 

Okay. So feedback groups. Give some feedback about the lecture, the exercises and so on. Especially since we have actually restructured the first four weeks of this course. 

So everything is running a little bit just in time in terms of getting slides ready and everything. And we might mess up a few occasions. So bear with us on that. But think about that and then put in some feedback. Good. 

Your name will hopefully appear up here in one of the coming weeks, if not already. You're supposed to read something about similarity measures, about visualization. You're supposed to do some homework problems. And if you haven't done them, then consider doing them during the exercise session. 

And at least look them over during the exercise session and ask questions if you have any immediate sort of concerns or problems solving them. Good. As I said, week two, I'll probably be with you until week six where George's will take over. 

So I'll see you next week as well. But generally, we are still in this introduction block thingy where we are trying to recap everything you need to know and then do it in the context of machine learning such that you are ready to actually do some, let's say, real machine learning. I will hint at your first algorithm today. 

It's a very simple one. So hopefully you can sort of see what that's about in terms of based on, let's say, similarity to similarity. Next week, that's typically the hardest sort of lecture to get, let's say, comfortable with from your perspective, maybe also mine, let's see. But it's about PCA, what's it called, principal component analysis. This is where we actually take and apply, let's say, linear algebra and find some projections that are useful in some sense. And some of the things we're going to talk about today will be helpful to understand that. It used to be this week, now we switched it, just so you know. 

Good. So the learning objectives for today, basically over all, as I said, learn some basic statistics, not learn them, hopefully, hopefully you have learned them. That's a prerequisite for the course, but I'll sort of say how can we then use that stuff, which maybe didn't make a lot of sense or wasn't motivated. How can we use that to understand some data? And then we're going to look quite a bit into, well, not quite a bit, but some into, let's say, measures of similarity and dissimilarity. How do we measure distance? Usually you have a ruler to do that, but there are other ways, and we're going to see that as well. Well, not other ways, but variations of that. 

And then we're going to understand, from an iteration perspective, what kind of plots might be relevant to actually understand our data, first of all, before we start doing machine learning, but also to disseminate our results. Good. So I have a few practicalities. So I'll try to include the screen slides for practicalities and announcements based on what I sort of receive in terms of questions and critical things. 

And then the blue stuff is sort of what we are supposed to go over in terms of content. And today, I do not expect this to be a very long lecture, so maybe an hour and a half or something, but let's see how my expectations fold out. And then, so I might have time to actually say a few, a little bit about the exercises. And this is maybe maybe obvious to some people who know how to program and use VS Code in Python. So it's mainly on Python, but just three different views of how you could actually interact with the code that we give you. So just a few hints from my perspective, if time permits, I should say. 

And then, of course, the exercises afterwards. There are a few of the TAs who are ill today, but I think we can manage in terms of there should be enough people around. All right. 

So just a few practicalities and no uprising about point two, please. So we have fixed the AV streaming issues. Someone made a blog and some code here, so that's why we couldn't stream the audio to the other room that I'm listening at the moment. That's fixed. 

Good. Then maybe more relevant for you than the exam. We discussed last week something about, could I actually write notes on my iPad and then print that and bring that to the exam? We have consulted. 

That means, oh, everyone on this course who is a course responsible, and we've agreed not to do that for various reasons. I can give you some of the reasons. One is that it's very easy to share them, so what's the point? You can just copy the file and everyone prints the same stuff. That's not the point of this exercise. Second of all, there's a little bit of unfairness in terms of not everyone having a touchscreen that they can actually write on and then print it. 

And third option is you can actually make this, you could bring like a zoom lens to the exam and you could have the entire book basically written on a piece of paper. Those are the main reasons. It's basically not for debate, but of course, I'm willing to take critique about this and suggestions for other things afterwards. 

But come see me afterwards if you still have concerns about this. Then I have had a very long email thread with our local department and the study administration and the guys who actually write the rules at DTU because in the DTU rules says, what kind of calculator can you bring to the exam? It turns out it says, it should have said quotation mark, a non-programmable calculator. I have no idea to define that, but in the next 10 weeks I'll figure out what that means and then I'll try to help you on that. 

But the problem is DTU needs to rewrite their rules in my opinion. Okay, for now just assume it's sort of a very basic calculator. Nothing about plotting graphs or anything. It's just basically making sure you add stuff together and maybe square them and take the square root and stuff like that. So very basic calculator. That's what we expect for the exam. 

If you need something else from your calculator at the exam, you're doing something wrong. So we'll talk much more about this as we go along. But anyway, just note it's on my to-do list to work this out. Then I had a few questions about the actual exam dates and just remember that this course because it's so big, it has its own exam date, a special one, so it's not in the usual time slot for what is this, E4 or something, like Tuesday afternoon. So this is the exam date as determined by the study administration. If they somehow decide to change that, I guess they could do that, but it rarely happens. 

It has happened once in last three years. So assume this is the exam date. Yeah, good. If there are any questions about all of this, put it on Piazza. Come see me afterwards after the lecture, basically. 

So back to where we sort of ended last time. We talked about data and those were hopefully relatively intuitively things about how to collect data, what to be aware of and things you might want to do to your data after you have collected it, namely, for example, transformation of attributes. And then of course also figuring out what type of data is it actually. Is it discrete continuous or maybe even binary up here and what type of, let's say, variable or attribute are we actually looking at because if you're trying to do regression on a classification problem and the classes arbitrarily assigned 3, 1, 2 or something like that, that's not going to end well. 

So it is quite important that you actually work out these, let's say, taxonomy here and this hierarchy we have defined here. So we sort of ended saying, okay, it'd be relevant to actually look at this data and well, if we know what's in it, maybe we can do something useful. So today we're going to do something that's hopefully useful in the coming weeks, namely measure distances between data points. And before that, we need somehow to represent our data and maybe get an idea about maybe summarize these potentially very big data sets somehow and that's what we're going to talk about. 

So just again, a relatively basic slide that will hopefully build to something useful in a bit. So just remember that we think of a data point as a vector. I'll come into what we mean by vector and vector spaces in a few slides. But generally it's a set of real numbers and we typically represent them sort of as a column vector in this course. Just be aware of how people actually are the column or row vectors and stuff like that. And an example here is this data set over here which you saw last time, you tried to load the iris data set. 

It's the most tortured data set you can ever imagine. It's from at least the last century, but by Fisher, a famous substitution. So what he did, I guess maybe he didn't do it, but someone, a botanist, went out into the world, found iris flowers and they measured basically certain geometric properties of the leaves of that iris flower. Okay, I guess that's something you can do and the interesting task here is then to actually classify the type of iris flower and I'll come back to that there are three types. Okay, but just basically based on these four attributes, how do we actually consider those? 

Right? One example could be for one particular iris flower, we put those numbers into a vector, in this case, four by one and that's it for that particular example. Now we move on to the next flower, I presume there are lots of iris flowers in the world and then we collect a whole data set and the way we sort of collect them is in a matrix and here's the notation for the matrix. So it's an in by in matrix basically where each of these vectors up here are rows. 

I remember that. Columns basically, well we have a vector, it doesn't really matter if it's column or row, you just need to remember what we have and then we put them in as rows in this matrix. So that means we now have a big matrix here where we have at least a sort of one, two, three, four, five and then some bunch in here and a last one here, so the nth one here, inth, in. Okay, so hopefully all of this makes sense, but if you actually look at this, this can be sort of well slightly confusing, is the first instance and how do we actually find any patterns in that and how can we sort of quickly get an overview of what's actually in that matrix. 

That's sort of one thing. Now it turns out that actually doing matrix operations on this other of this data matrix is a good thing. Oh sorry, not a good thing, it's a beneficial thing whether it's good, it's a different matter, it's a matter of preference, but at least we can do something sensible if you actually sort of look in the toolbox and look in matrix operations. So some of you would have had this like maybe yesterday or the week before and some of you, it would have been years since you've seen what we can do with matrices. 

So today I'm going to just remind you and I didn't ask you to actually read about this, but I'm just going to remind you what can we actually do, what is a matrix, what can we do to it and some basic stuff. That'll become much more, let's say operational or we put into operation next week where we'll use it for something. So the basics today and then next week we're going to talk about why this might be useful. 

I'll show you how it can be useful. So common notation that you will see all over and you'll even see some more from me because I tend to write a matrix like this, so just be aware and I've picked sometimes like this. Okay, so it's, we have as we saw before, we've got in rows, in columns and we assume in this course only real matrices, that means the values in here are real numbers. For those of you who have seen matrices with complex numbers, we are not going to deal with that because it turns out stuff you want to measure in the real physical world tend to be real. 

So like temperature, it's a real number. Okay, it doesn't mean that complex numbers can be useful, but in this course we're not going to deal with that, especially not for representing the data itself and of course the vector notation we've seen that. So a very big thing we want to do to this vector essentially or to the data matrix is to essentially scale, rotate, warp these vectors somehow and a very useful aberration which we'll see next time for doing that is to apply a matrix onto that data matrix. So here you should think about the data matrix as B and then we apply some other matrix that can do something to this data matrix here and that's essentially a matrix multiplication. You can think about rotating stuff, you can think about scaling it and stretching it. 

Okay, just think about why that might be useful. Okay, so a matrix multiplication, I hope you've seen that before, we can take an L by M matrix and multiply it by an M by N and we get an L by N. Okay, so an example down here with some numbers in it, so just to show you how we do this, here we've got a 3 by 4 and a 4 by 5 matrix. We take sort of one row multiplied by each column to get this particular element down here. So row 4 and column, sorry row 3 and column 4 and we multiply them element wise together. 

So 1 times a, 2 times b, 3 times c and so on. Basic stuff and if you can't remember this stuff, go home and read it and try it out. Yep, so now we can do that for each element in this resulting vector and yeah, we get some real numbers here as it turns out because we've got real numbers in here and real numbers in there. And yeah, we can do a small example down here and maybe try to do that yourself. What else can we do to a matrix? We can transpose it so we can flip columns and rows. So this row here becomes this column, let's say this column here becomes this row for example. That'll become useful again next week as it turns out and we can take also, if we have a sum of matrices, then we can take the transpose that's simply the sum of the two transpose. Yep, and we can also transpose the product or we have to do a transpose them individually and flip the order. 

So these small tricks are something you've, I'm sure you've seen it before, just remembered. Something that'll become a little bit relevant today is namely, or not namely, but it is what we call an identity matrix. So it's an in-by-in matrix where all the diagonal elements are one. That we'll see how that plays in a little bit later and then we, well, if you have an identity matrix multiplies or applies it to a matrix A, then nothing happens. 

All right, so you have seen that before. So for example here, nothing happens. We take this identity matrix onto this two-by-two matrix, nothing happens. 

You get the same amount. Now it's useful actually in defining what the inverse of a matrix is. So the definition we're going to use here is that A times A inverse, so the matrix inverse, it's not just taking the, let's say the inverse of each individual number, is simply the, well, it's simply the identity matrix. So that applies for a square matrix here. All right, so remember, we have a matrix, could be a data matrix, it could be something that does something to the data matrix. We can do certain basic operations on that to potentially achieve something useful. 

So that was just a little bit of an aside. The main point here as we move on is that this matrix here, how do we summarize that somehow? How do we summarize what's in our dataset? And that's what I'm going to talk about now in terms of summary statistics. And I suppose you have all seen this before at some point in your life and some of you basically last semester. 

So we'll just go through it relatively quickly. So formally we've given two samples. So what is a sample in this case, in the flower case? So we've got one, two, up to n numbers here. So basically we've made n observations of one of these attributes. So the sample here, one sample from that population is now located as a column in that matrix. 

I could of course go out. I'm not observing the whole population of Iris flowers. So I could go out and get a new sample and the numbers would change slightly, right? But remember, I'm observing it, let's say simultaneously or concurrently as I'm also observing the other four. So here I've got, let's say, four specific samples of four specific attributes. Now, if I look at these numbers, yeah, what could we do? Well, one thing we could do is see what's the average, let's say, see per length here. 

So I do the average. That's intuitive. It's simple. So I could write that and I'm going to write that as mu raised to sort of indicating one to indicate it's the first attribute. 

I'll use this notation in the slides here. And then I'm going to put a head on it because it's an empirical estimate. And the empirical estimate is simply taking all the individual numbers, this one plus this one plus this one blah, blah, blah, divided by the number of numbers I have. That's the empirical mean. 

All right. What might we else want to actually look at? Well, now you have the mean of a bunch of numbers. How about looking at how much is the variance around that mean? That means how much variation is there in this attribute? Remember, it's column wise. 

Everything is column wise. Okay, we simply do the empirical variance. So that is subtracting the mean we've just estimated from all the individual numbers. So if we do this one up here, I need to take that one scalar number, subtract it from everything in this column, square it and then sum it all together and then divide by this one over n minus one. So don't be too bothered about this n minus one. 

There's a very good reason for it. The short story is that essentially we're interested in not estimating the variance of an of an individual sample here, but the variance of the population. So if I sample, let's say, if I go out and sample all of this again, I'll get a different sample. Well, obviously I'll get a different sample, different set of numbers. 

And what I'm interested in is that the average of the empirical variance across these samples is the true population variance. Okay, that's why we need to make a small correction here. If you can't remember why we're doing this, at some point, I might be able to show you a small proof once we've covered expectations and stuff like that. But anyway, just accept that this is the case. 

And if you're worried about whether it says one over n minus one or one over n, yeah, maybe you have bigger problems in your life. But anyway, it's essentially convention that this is what we do. And there's a statistical reason for doing it. 

Okay, just buy that argument for now. But anyway, we can estimate that for all our, so I forgot to write down you had for all the other ones here. So mu what would be four here and hat again. So I've got all the means for all my attributes. So that's a, you can think about that as a one time four matrix or victor in this case. But I can also do the same for the variance. 

So I have now have an s hat, and I have that for all of them. So I'm not going to write that. Okay, so now I have the mean on average. What is that? What's the average of that property of Irish flowers? I also have how much spread is there around that mean? Oh, variance is there. Now, just jumping down here, we also know there's something called standard deviation. That's simply just a square root of the variance here. 

Just remember if you're working with variance or standard deviation, I mean, go ahead. Ah, sorry. Sorry. Yeah, so I have confused you a little bit here. Sorry. So here, basically, the way things are set up is that x correspond to one of these, y correspond to another one. So I use this other notation down here. 

Sorry for that. It's just to be able to do it across more than two at the same time. Is this clear? Okay. Yeah. 

I want to make sure it's clear. Right. So we have actually talked about why yet. So maybe I should have waited for that explanation until I've actually explained it. Anyway, if I want to actually look at the, let's say dependency between two of my variables. 

So how does the sepal length versus sepal width? How do they actually covariate? Or what's the covariance between them? 

So if I change one, what happens to the other one on average? I can also compute that. And that is why we need the x and y notation is used here. So in my notation, you would use mu, let's see, you would use mu hat and then whatever k column you're choosing. 

And then you use xi k column. Okay. These are details. 

So don't get bogged down with this. The point is we choose an attribute x, sample from that, and an attribute y in this case, basically two vectors. Now I'm looking at them. I'm subtracting the mean from them individually. Okay. And now I'm multiplying them together. 

That's given me an estimate of how, what is the empirical covariance here? Yeah. So I'll come back to what that means in a little bit. So what can we do here? Well, we've got our data matrix. 

We have the mean, the standard deviation, and we can even figure out what's the relation between two of these columns here, two of the attributes. All right. There's one more thing you can do. And maybe we need to draw a little bit here. So we can also look at quantiles and percentiles. These are not quartiles. 

Yeah. So quantiles means something specific and it's basically just about splitting your, let's say, your range up into different numbers or into different fractions. So if I look at now, at now I just have a single of my columns and I actually try to plot that on a line here. So let's imagine it's width. So that line would represent from, let's say minus infinity width or probably zero. That's why you need to know that it's a ratio attribute. So from zero up to something. 

Okay. I can plot all of my data points on that line. Let's make them green today. So we might have some data points here and a bunch in here and maybe a few out here. And what did we do on the previous slide? Well, we actually took, columnized, we took the mean. So the mean might be around, I don't know, here. 

So that's what I call mu hat. And then we had an estimate of the variance around that. So how should I draw that? Maybe something like this. 

This is sort of an estimate of the spread around that mean. All right. What else could we do? Well, we could ask if I want to split this sort of range or scale. 

Give me a second. If I want to split that into, let's say two, what would that point that just exactly splits the number, the points into two parts? That means that there's equal probability in being in one of the, let's say, what do we call that, ranges and 50% in the other range. I could also split it into 10 different parts and ensure they have equal probability, all of them. What I get here is what's called the quartiles. And if I split it into 100, the small elements and actually report those numbers where we split, then I get percentiles. 

You have probably all seen this. So we can have the median. That's the 50th quantile. And of course, again, we can split it into 100. That way we get the percentiles. 

All right. You have seen this before, but it might be a good idea to also report such numbers when you're actually trying to estimate your, or sorry, when you're trying to summarize your data matrix or your data set. So it might be useful to know that, up to this point here, let's say this point here, that's actually 25% of the data that lies there. Then from there on up to this point here, there's another 25% of the data. From here up to here, there's another 25% of the data and so on. So that means we're reporting sort of the place where it actually splits this, let's say, distribution. All right. I'll point back to this slide when we get to the visualization part. Okay. 

Then something maybe new. So I just want to do a small survey here. How many have seen or heard of or understand what a covariance matrix is? Just a show of hands. Okay. So what is that? 

A third or something to that effect? Bear with me. The reason why we're doing these introduction lectures is to make sure everyone sort of understands what we're doing here. So remember covariance. It sort of explains, if I change one of the attributes, it could be the length. How much does the other one change on average? So I'm looking for a relationship here. 

Is there relationship or not? I want to quantify that. That's what we're doing with this one again. So just to change the notation to confusion now, because now it's in my notation and not some generic stuff. So here we have a case attribute. We're looking at the case attribute. Let's say it gets one. I want to see how does it co-vary with the mth attribute? 

So let's say it's the last one here. Is there a linear relationship between them? I can quantify that using the covariance between these two attributes. Of course, I can do that for all the possible pairwise sort of comparisons I can do here. So I take this one versus this one, this one versus this one, this one versus this one, so on and so forth. 

Okay. A good way to represent that is actually in a matrix. So now we're back to the matrix stuff. 

And matrices are important. So what I can do is I can say what's actually the covariance between c-pill length and c-pill length, so itself. So between k equals one and m equals one here in my notation. That is the variance. So you'll see here in the diagonal, you'll see the variance, 0.7 as it turns out. 

Okay. So in the other, what about this number here? That's the covariance between c-pill width and c-pill width. That's the variance if you look at the equations. Okay. So that means that everything in the diagonal basically corresponds to the variance of that attribute. Yep. Good. What about the off-diagonal elements? 

Well, here, let's pick an interesting one if there are any. So in this element down here, so element 3, 1, I'm looking at the covariance between the third attribute or the sample of that and the first attribute, so this one. Okay. How do they co-vary? Again, if I change one, how much do the other one change on average? Does it go up or down? 

Or does it not do anything on average? Is there not a relationship at all? Okay. As it turns out, that's actually some relationship it seems. It seems at least it's positive. That's what I can say. 

And it's above zero. Yeah. Well, that sort of implies one implies the other. So what I want to say is that it's greater than zero. That's what I mean here. 

Okay. So what I've done here is do these pair-wise sort of, let's say, computations of the covariance between all the attributes. That is a hugely interesting thing to look at, so it highly encourages you to do that for your report. Why is it interesting? Because if I actually had another fifth attribute here, let's say the class or some value that I'm interested in predicting, and I also do this, then I add sort of a row and a column here, then I can see how much, in a linear sense, would my, let's say, C-pull length correlate with that attribute I'm trying to predict. So perhaps plot this as a, let's say, a heat map or something like that, and I guess we'll show a few in the last part of the lecture. Okay. 

Are there any questions about this? Because this will also become extremely important when we talk about the bivariate and multivariate norm distribution in two weeks. Have a look at this and try it out in Python. 

I think that all MATLAB or whatever it is, actually try to do it and see what you get out and do it by hand, maybe even for one of the, one of the numbers in this matrix. It's important. All right. You have probably heard about, okay, so there's a timely question on Zoom saying, why am I talking about covariance and not correlation. Voila. 

Okay. So covariance is, well, that's one way of computing a sort of a linear relationship. But what you have probably heard about in some courses and presumably also, let's say, only for two, two different attributes, X and Y here, so two vectors essentially, you probably heard about correlation. And correlation is nothing more than a standard or sorry, normalized version of the covariance. So to compute correlation, we need the covariance. And then we simply take into account what is actually the standard deviation of each of these two attributes. So let's have a look here. 

So I'll pick an sort of interesting one. So here I've got X1 and now, okay, with the notation up here, I would choose, oh, sorry, this would be X and this would be Y in this case with the notation up here. Yeah. I guess we need to fix that notation. 

Okay. So here we've got one attribute X, another one Y, are they correlated? So remember, I have an sort of a column in my data matrix that corresponds to X and another one that corresponds to Y. 

Now I'm asking, are these two measurements of width, let's say length and width, are they actually related somehow? Well, I can plot it like a scatter plot and we'll see a lot of scatter plots today. Okay. And I can see what happens. 

So what do eyeball, what actually happens here? Well, there seems to be a sort of small trend that when I increase X by one, let's do that. Let's assume that this is going from this point here, zero to one. The other one seems to go up at least on average. So if I were to draw a line here, then on average, it would actually go up. 

Okay. There would be a positive slope to this line. So in this case, if we are looking at the covariance, it's not normalized to anything. It could be minus, it could be negative as well, but it's not limited to minus one to one like correlation. So correlation is often useful if you want something that's normalized. 

It turns out that when we are talking about multivariate normal distributions, we work with a covariance matrix and not a correlation, but you can always convert back and forth. Okay. So in this case, we had a slight sort of a small, let's say weak relationship between X and Y. Here we've got a perfect relationship. 

So actually if you gave me the sepal width, I already knew that the length, didn't have to actually go out and measure it because there's a one-to-one relationship. The world is never like this unless you made a mistake. So always expect something like this. If you're trying to measure two, not independent, but not identical things, you would be surprised if you see perfect correlation. So correlation can also be negative. 

If, let's say, if a change in X turns out to always result in a decline in the other attribute, well, then we have a negative correlation. Yeah. Also negative covariance. 

All right. I hope all of this, I guess maybe some of you heard something new today about the covariance matrix. You don't know the thing about a correlation matrix. You can just normalize that one as well. 

Usually we talk about the covariance matrix in this course. Okay. So I hope most of you, if you haven't heard about this, then make sure you have for the coming weeks. Now onto something slightly more maybe interesting and perhaps slightly also seems, okay, you summarized your attributes and stuff and those were on the attribute level. So now what we're going to talk about is how do we actually measure different distance between this example and this example? Okay. Well, that should be easy, right? You know how to measure stuff. You could take the difference between these two and maybe take the absolute difference of the difference of each individual element, square them and add them all up and then we got something that's the straight line distance. 

Take the square root as well. Okay. So how does that work in general here? The problem is that there are many ways of measuring distance between things, also between data objects. So you can think about this example down here. 

So I've given you shapes, but how would you measure distance between this one and this one? Is it because of the size of the one versus the other? Is it because of the color? So here size and color would correspond to the attributes, right? So which one would you weigh more and more in this computation? 

Or this should be weighted the same? Okay. But of course, also here we have something categorical namely the type of shape. So it becomes a little bit complicated. But generally similarity and dissimilarity are at the heart of, I would say, well, a huge sponsor of the machine and the method we're going to see. Either they enter into a distribution or they actually enter into the loss function we're trying to optimize. So understanding these and at least have a certain awareness about the fact that we can have different similarity measures and dissimilarity measures is going to be important. 

Right? So if you look at a couple of examples, for example, like building on what we did last time, there we had sort of figuring out what's the topic of a book, for example, or document. So here we implicitly actually did this distance measuring thing because I said, okay, we can sort of count how many occurrences of specific words do we see in one book versus the other. Maybe we can sort of compare that and measure distance. And then if I want to find some book that's close to what's in the library, I measure distance to everything that's in the library and maybe the one that's closest is the best one for me. Or the most similar one at least. 

Okay. So that's sort of an information retrieval view on things. And that happens all the time in whatever internet thing you're using that people use distances and similarity for ranking stuff. There's also another thing that we talked about last time, namely, outlier detection. And this is where we are interested in finding observations that are odd compared to basically the normal behavior. Again, you can intuitively think about, okay, I just want the one that's furthest away from everything else. That might be an outlier. 

It might be an abnormal observation. Okay. So how are we going to build on, well, well, the next slide is going to be a little bit technical, but I'll try to just motivate it from the high level. 

Okay. So the question here is, given a vector, like, let me jump back a little bit, given sort of two vectors, how do we actually make sure we can measure distance? What are the properties we need to be able to do that? And if we have those properties, what could we then sort of expect, so to speak, or what properties arise? Okay. So you have heard about vector spaces before, I'm sure. 

So I'll just highlight the important things that we sort of assume here, basically. So I didn't have time to actually do a pretty picture here. So I'll try to come up with one on the spot. Let's see how that goes. That's always a good idea. So let's put something in place here. 

Not red. Let's do this one. So I'll try to draw something out here. Maybe that's one. Maybe that's something like this. So what we saw before is that we have this vector. So I'll write a vector somewhere on the screen. So x equals, in this case, it's a two-dimensional space. So I'm only looking at, let's say, seepal length and width again. And maybe it's, I don't want five because that becomes clumsy. 

So I want specifically, let's say, 2.0 and I want one here. Okay. That is essentially two measurements of coming from my buttonists about the length and width of that particular flower. 

Okay. Now, is that just two coordinates? Or is it a point? Is it a vector? 

What is it? And I think all of these sort of views are helpful in some cases. The thing is, if we need to do it formal, then the way we were going to see it is that actually what we have here is a vector. 

And I also alluded to that many times last time. So if I, first, I just want to draw it as a point, basically. So now I want the red one, I think. So now we have, let's say, so 2, 1, 2, that was the first coordinate, and then one of the other one, right? 

Easy peasy. So I could draw sort of a dot here. So now it's just a point. But as you've probably seen in your linear algebra and maybe real analysis, the way you're taught by mathematicians, and I'm not a mathematician, I use math for doing interesting things, but they would typically draw an error from the, or not error, error from the origin to that point. But essentially, your observation, the way we thought about, okay, we're just observing some two coordinates of something or two measurements, now we're thinking about them as a point, and we are also thinking about them as a vector. 

So now we're going to require something of this space that we are sort of in here. And that is essentially that we can scale that vector. So I can make one that's eight times longer than my x vector here. 

It might, I'm going to dash it here, so I don't know, maybe half 50% longer. That's another vector, and it's still in the same vector space. Okay, what I can do, I can also add, I can also add vectors. So that means if I have another vector, let's say it's here, whoops, here. 

That's another data point. We can call it y if we want. Then I can actually add it to the other one. Okay, and one, when I add them, then I actually get another vector that's also in the same vector space. 

These are properties and technicalities, but they are a little bit important. Okay, then, so I can add that the way I add vectors is to take each coordinate, while I add them coordinate-wise, and then I get a new vector essentially. I can also do linear combinations, any linear combinations of any vector in that vector space with some scalar attached to it, is also in that vector space. 

We say it's closed on the linear combinations. Okay, that's all good and fine. Am I going to ask about this in the exam? No, but what it does allow us is to actually, let's say, define a norm. So if we equip this vector space with a norm, now we are now, well, now we allow to actually measure distances of vectors. 

And remember, a vector represents a data point, an observation, which in turn are made up of two coordinates, or four, if you want the full, let's say, geometric view of that flower. Okay, but so that means I can actually define also what's the neighborhood around a data point. This is important, again, because now I can sort of start to compare and see is there anything close to me that might be interesting. So I'm just going to draw that, sort of to indicate here. 

Now we can actually draw, let's say, let's say neighborhoods around a an observation. But anyway, the important thing here is that norm allows us to measure the length of a vector. Also, in a bit, allows us to measure the length of the difference between two vectors, or two observations. Then just for technicality, we also need an inner product, because we're going to talk about what's the cosine distance between two observations. So what would that correspond to here? Well, we would have an angle between them, and this is essentially what we are trying to measure using what's called the cosine distance. 

What's the pros and cons of that? I'll come back to this, but obviously measuring the angle between two vectors or two data points. So let me rephrase that. Remember, we have two data points here. I'm measuring distance between them in two different ways. 

One could be the straight line distance, simply taking the distance here. That's one way of doing it. But the other way I just alluded to is actually taking the angle between them. And that means something, which I'll come back to and draw an example. 

All right, so there's some technicality associated with this. All we're interested in the fact that we now have a norm and we have an inner product. So what is the norm? So your Euclidean norm or distance is essentially taking the individual elements of that vector, squaring them, summing them together, and that's essentially it. Okay, so remember this notation, the two bars, or vertical bars, and then the two down here to indicate it's Euclidean norm. 

I'll show a general expression in a bit. We can do the same for matrices, which I'll come back to next week. So basically measuring what's the size of the matrix. Essentially, it's the same thing, square the individual elements in the matrix, and sum it all up. That's called the Fabrinius norm. Okay, what can we use this for in machine learning? 

A lot as it turns out. So if I just pick something that's a little bit more visible here. Give me a second. So we are interested in the difference between two data points. We want to compare, let's say two flowers are they similar in terms of their geometric properties, and we can quantify that using the distance. So if I have a measurement of one flower as x and another one as y, so let me write that out. So I have x, oh that's a bad call on now. 

Not a good idea. So if I have x, I have four values, 5.1, 0.2, 1.1, and one something like this. That's for one flower. I measured four values, right? 

For another one, I measured four other values. I don't know. Just making this up. Okay, I want to compare how similar are they. I want to quantify that. Of course, it typically needs to be in relation to something else, but I just want to say what's the distance between them. 

Now there are many ways of measuring distance, as I've already alluded to, and the one you know is the Euclidean one. So like what we saw on the previous slide, what we can do is take the difference. We are allowed to do that now, right? We can scale matrices. 

So if we put a to minus one and sum them together, we're allowed to do that. So if we do this, we get another vector, and I'm allowed to measure distance in this p norm. So if we put two here, so p equals two, you get the Euclidean norm, and that's the straight line distance that you're all probably comfortable with, the one you always use when you're looking at things. So how far away is the clock up there, for example? I'll probably use the straight line distance. Okay, so let's look at this one. 

What am I doing here? I'm taking the sort of coordinate wise difference between the two, so between 5.1 and 3.1, squaring them, summing them all together, taking square root. That is exactly the norm of the difference between the two, in the Euclidean norm. Now if we actually want to look at that sort of visually, that's what's been done on the right, and the Euclidean one is this one, so that's p equals two. What's been done here is that if you imagine x being in the origin, so zero comes zero. Of course, now we're in 2D, not 4D. So if I do that here, that's x visualized in this 2D vector space. And now I just simply compare to every other point and call that y, and then I say, if the distance is less than one, you become red. If you're more than one or higher, greater than one or equal to one, you become blue. So it's a way of visualizing the properties. It's not surprising, if you look around you, then you would say there's a, you know, let's say this, if you say that you want a distance of 10 to something, it would be a circle, because it's the straight line distance. Okay, so that's the one we all, no one loves, I guess. 

You'll learn to hate it in a bit, but oh well, not really. But all I'm saying is that there's a reason why we have an expression up here where p is a general thing. So instead of putting p equals to two, we can put it, set it to one, for example. And then you get what's called the L1 norm or the L1 distance. So what does that do in terms of the same graphical sort of representation here? So it's not the same, right? So if you take if you take a point here, just here, that one has a distance greater than one now, whereas down here it was less than one. 

So they have different properties, whether or not those are good and bad properties, that's often an empirical endeavor, but as it turns out, this has certain properties I'll compact. But you can also do something else and set p to infinity and then you get the so-called max norm. This is where we simply take the maximum of all the coordinates, coordinate, and advise the differences. So what does that do for us? Well, then we get this square thing here. That means again, everything that's read here, that means in this box, have a distance, a let's say infinity norm distance of one or less than one. 

Okay, you'll get to actually do this and I'll give you some time to actually compute these things in a bit. So why are we interested in this? As I said, we're interested in measuring distance to things like maybe for finding out lies or maybe finding something that's similar. So we need to be able to do this and you'll get to play around with this a lot today, also in the exercises. Good. 

Are there any questions about this so far? Because now you actually have to do it. So if you can't remember what these things are, then look it up in the book. 

I didn't write it on the slide. So brand new exercise for you here. So try this out and I'll give you 15 minutes, including the break. So we'll do the break now. And that means be back All right. Okay. 

So these quizzes are actually not in DG Learn, so you can't put anything in there unless you hack the system. So just a show of hands. Or does anyone want to yell out what they think are Huh? How many think so the response down here is C. So how many things it's C? How many things it's B? Okay, good, because it's C. Okay, the reason why I actually put this up like this, and they're a little bit tricky somehow is to actually emphasize a few ideas here about these distances. 

So maybe let's do the first one. What's the distance between a point and itself? Using this. Oops, this norm up here. 

It's zero. So this somewhat convoluted statement D down here. If you take the whatever P and it needs to hold for all the three P values we've indicated here, then the distance between 04 and all the other points. Then all the other points here 123456 for all of them needs to be bigger than point one. If the distance between 04 and itself that's zero. 

So we can always already rule this one out. Okay, whoops, let me do that. So correct one C. Okay, what about a and that that's easy for very particular reason? 

Because D one between this test point, the blue one and 03, which is, let's see, it's this one here, this black one. So let's actually write that. That was 03 and 06. Where was that? 

It was this one. They have the same sort of second coordinate, so to speak. That means that it doesn't contribute. 

So all we have to do is now consider the distance between the blue one and the black one and the blue one and the red one along this axis. That means P doesn't matter. It's going to be the same. Remember this? Yeah. So of course, this distance here is actually smaller than this distance. So this is false as well. Now we asked about the infinity norm here. So remember that's how the choose the biggest. 

You take the difference between coordinates and choose the biggest one. So if we look at 02, that's this one up here, 02 and what was it? Five. Where's that? 

Down here, 05. So we have to consider the coordinate wise distances. So if we take along x1 first between this point and this one, the difference is one. 

The second coordinate, 345. So it's not one. This is actually another norm. 

So this is also wrong. So of course, unless I've made a mistake, then the option C is correct. So if you just try to write it out, so what do we get? 

We need to compute something here. And it says this norm business. The p at the bottom refers to the Euclidean one. The p at the top just says it should be the square distance. It should be the squares of what we have up here, essentially. So if we plug this in, then you get, because o star, that's 0, 0, you essentially just get this one, so let me write that. So we get just the norm of minus, come on, there we go, of 0.2 and 0.3. Now, do you need a calculator to do this? 

I don't think so, but let's see what happens. So if I could, I'll write it up. So I'm not going to take this square root, because I have the squared already, so I'm going to ignore that. I'm just going to sum the squared individual differences, and since we have already computed those, we have them here, so we're going to sum the squared of these two. 

That was a bit convoluted, sorry. Squared equals what? And of course, now you might need a calculator in the exam to do this. 

You could also just go back and remember maybe another way of doing this. So what is minus 0.2? That's 2 over 10 squared plus 3 over 10 squared. That gives you, now you square the numerator and denominator, so that gives you 4 over 100, and of course the minus is not there, and then you get 0.2, 2, was it 3? Where is it 3? Sorry, that's supposed to be a 3. 

Someone needs to teach me how to write. So we now get 9 over 100. That's 13 over 100, that's 0.13. Yeah? 

If you can do this in an exam without using a calculator, it'll become much easier, and we try to be nice, so we don't have to use the calculator a lot. Good. Any questions about this? Otherwise, I'll of course upload the annotated slides, and you can have a look at it there, and that's a relatively detailed explanation. So those were the LP norms, or LP distances, or Minovsky distances. I also mentioned the cosine distance. So what happens, actually, if we look at this, if I look at this one, and actually remember, I can scale a vector like I want to, would that actually change something? And yes, it would. 

But sometimes we're not actually interested in scaling, or sorry, getting a different distance between two things just because we scale one of them. So back to our book example. So let's just say that I go into, like the library, we've been talking about a couple of times, I look into the books and I say, is this particular word there or not? And then I indicate that in a binary vector. Okay, something like this, and then I have one, two, three, four, five. 

I only have six possible words, and my vocabulary is six. It's not great, maybe a baby, or something. Well, not even a baby, right? But something like that. 

I don't have a big vocabulary, that's the end of the story. Now, so there was this document, and now I'm looking at it, and I'm trying to compute the distance to another document. Some other document, something like this. 

Okay. So now I want to compute the document. I can do that using the LP norms, of course, but as it turns out, using the cosine distance is a good idea because if you think about these as sort of word occurrences in documents, and then you plot them in a vector space, so this is, of course, imaginary, right? That I've took one document here, another one here. 

And if I'm looking for the angle between them, that's defined up here. And now imagine that I take one of the documents and just duplicates it. That means I'm now scaling the number of counts I have in my vector. So now it says two, zero, two, zero, two. That means I'm scaling the y vector over here. Does that actually change the angle between them? 

No. And that's a nice property which doesn't hold for every single metric or every single distance, a similarity measure you can think of. So just be aware of these things that, okay, are your distance measures actually invariant to scaling. 

Now come back to this. But anyway, the cosine distance, you can use that as both the, let's say, for binary vectors, but also for continuous vectors, of course, that's where we normally think about it. And in the continuous case, you would have a two norm here. Okay. So if I now maybe preempt the next slide a little bit and say, okay, what if I'm interested in simply counting the number of times both words occurred in the document, in the two documents. I'm going to call this F11. So now I need to count how many times is there one sort of occurrence of 11. And so this was a zero one. I can draw that as well. I can put that on my list as well, zero one. And then I, of course, have zero zero and I have, what's the last one, one zero. I can count the number of times these occurrences occur, right? So occurrences occur. 

These words occur in both documents. So if one one, do we have any of these? So there's one there. So the third word occurred in both of them. So I count one and the last word as well. So I count two. So that would be two. And then of course I can count all the other ones if I wanted to. 

But the one I'm really interested in, let's actually do that. So zero zero, I have one. So one zero, I have this one. That's the only one, right? And one zero. I have this one, this one. That was it. Okay. 

This is just for completeness and my OCD that we need all on one slide. What I'm really wanting to sort of maybe tell you here is that I can do this counting of F11 using the inner product, namely what's written up here. So if I take these two vectors and simply try to do what's here, let me do that. Then you get zero one. 

I'm transposing X now. So zero one, one, zero, one. And then the way I learned to do this is sort of vector matrix process to write Y on top here. 

So zero, one, zero, one, zero, one. And then how do we do the inner product? Multiply the millimand vice. So zero times one. I need to add to that one times zero. One times one. So now the sum is now at one. Zero times zero. 

That doesn't contribute. One times one. That's two. 

So if one, one is excited, can be computed using this inner product. So there's one up here who asked about it a couple of minutes ago. That's why I'm doing it now. Remember this. So this would give you a two. You have to do this in a bit. Okay. Cosine invariant to scaling. 

Very good. Mesh is distance. You could go from one to minus one. So if they are equal, it will go for, so it's essentially similarity now. 

I remember that I should have said that clearly. So we're measuring how similar are they. And if the two vectors are identical, there's no angle between them and the cosine will be one. 

If they're totally opposite, the two vectors, they're pointing different direction, the distance would be minus one. Yeah. Make sure you try to do a few scribbles to figure that out. 

So now until another. So remember cosine could be used for both the continuous vectors and something where we have counts. And it's essentially just because we are interpreting these as living in a continuous vector space. Something that's different in nature is something that's specifically made for binary vectors. 

So again, up here you have X and Y. So the numbers here are examples are different. But you have all of these things that I just wrote on the previous slide. That means you have these ones as well. You're counting how many of these occurrences do you have? 

So how many times did one word occur in X and also in Y? Yes. And then we define a couple of metrics here. And the reason why they're all on this slide is that they're predominantly defined and used in these binary situations or at least counting. 

So, and typically they arise from information retrieval if you're interested. So there's something called a simple matching coefficient. So this one is taking the number of times a word did not occur in the two documents. 

So it's zero in both of them, that entry. And plus the number of times that a word did occur in the two. And then summing by the size of the vocabulary now. All right, but what is the size of the vocabulary? Okay, it's simply the sum of these guys. 

I need to be a bit more careful here, I think. So this covers all everything. So it basically counts the number of words in your vocabulary. Okay, so is this a good thing or a bad thing? Well, at least it's symmetric sort of in these zero zeros and one ones. But the problem here, maybe if you think back at the text document, is that if you're comparing two books and one of them had a vocabulary of six and not using every other word in the Danish English language, and you have another book which essentially uses the whole vocabulary, then you're going to get, first of all, you're going to get a lot of these zero ones. But if you now simply make the book longer, then there's simply going to be many more of these zero zero occurrences because these words simply don't occur together in the same two documents. 

So that means that the measure here is very sensitive to this thing here, which is maybe not that relevant. We're not interested in things that do not occur together. So the same word, do not together in the two documents. We are more interested in the other way around that these two words actually did occur in the two documents. That's a much more interesting sort of co-occurrence. So it's sensitive to that thing. 

That's one thing to notice. So how can we fix that? Well, the so-called Jakard coefficient fixes that by simply subtracting this thing down here from the total number. 

And that, to some extent, fixes that problem. Okay, so again, slightly different properties and you get to play around with that in the exercises. There's sort of an extended version of it where you can also use it for continuous vectors. 

Okay, so different properties for these guys. And of course, the cosine can also be used in both the binary and the continuous setting. So now you try to do this. And this, let me just say, it's a slightly odd exercise in the sense that you don't actually have to compute the cosine one to work out which one is the correct. 

So at least through the simple matching coefficient, end at the cart. And I'll give you five minutes to do that. Thank you. Okay. 

Boom is the sound on. Okay, so this is about the amount of time I would expect you to use or spend on an exercise like this in the exam. So it's not sort of the average eight minutes or something. 

It's less that way you have more time for the, have slightly more challenging. So make sure that this is, it comes natural and you do this by practicing and doing the exercises and doing the, yeah, also the programming bit. So let me try to do it. So we have the, we need to sort of look at this. And of course, there's a little bit of strategy involved here because maybe you could actually start out testing this one. 

It's certainly, or maybe this one because they're definitely different, right? So here there are only two different options. So here you have one half, three thirds and one third. So maybe the strategy would be to maybe test this one that will give you the most information. Okay, just a little bit of strategy for doing multiple choice exams. 

So let's do the SMC. So it's F naught naught over F 11. So how many, so again now we have the data matrix here. 

So how many times do zero, is the attribute zero for both, let's say vectors. That seems to be one. And then what about F 11 there and there? So we have one plus two. That's three, three over five. Hopefully that gives you the correct answer. 

By the way, the correct answer is A. So what about your cards here? We need to then do almost the same. So we take the deck card. We need the F 11. That was two. And then we have the five. That's the length of the vocabulary or M as we also call it. 

Of course, here it's called K. That's great. Then we take five minus one and that's two over four. That's one over. 

Well, that's a half. Hopefully that's also correct. Yes. 

Good. And then the tricky one. And basically you don't need to do more now because no one other option sort of a sort of is available if you trust yourself in computing this correctly. Let's do the cosine one. I've already sort of shown you how to do it. What do we need? 

Well, we need this thing here. What was that? It was F 11 and there was two. So if just look at the cosine X or Y, I'll have a comment about this notation a bit. The cosine X or Y equals F 11 over basically this norm. 

So let's try to do that one. So we know what if one is that was two and you can try to do the inner product like I did on the example before that. So now we need the norm and it's specific norm. 

It's the two norm of X. What do we need to do? Well, we need to take the square root of something because I haven't squared it this time. 

So we need to take the sum over all the element wise differences and square them. So let's try to do that. I'll do it manually here. So we have written up the vectors over here. So the difference between one and zero or sorry, we need to just it's not the difference here. 

Forget what I said. It's just the length of vector X. So all I have to do is square the and some in the individual elements. So one squared from X here, square this one, square this one. So all in all, that gives me one squared plus one squared plus one squared plus zero squared plus zero squared. That's three squared. And then I need to take the square root. Then I get three. So the square root of three. Did you follow this? 

It's okay to say no. I'll do it in detail for the other one. We need again to measure the length of why there's no difference involved here. The length of why using this P equals two. Okay, I need to sum all the individual elements squared. So I take zero squared plus one squared plus one squared plus zero squared plus one squared. Should I write it up zero squared plus one squared plus one squared plus zero squared plus one squared. That is square root of three. 

And if you plug that in, you will get this, which is two over three. Okay. Yeah. Good. Moving on. 

Unless there are any questions. Sorry. I did. Square them. 

Oh, sorry. The question is whether I should square them. So I squared things here, right? 

The individual elements squared. One squared is one. One squared is one. Zero squared is zero. 

One squared is zero. Yeah. And then this stuff. That is three. Square root of three times square root of three is three. 

Yes. There should be a square root. It's here. 

So it's the square root of three. Okay. Let's talk about afterwards. 

Then all right. Now you learned how to do measure distances between vectors in a continue or in a vector space. And we also have measures for binary vectors. 

Okay. So I already mentioned scale invariance. Sometimes you want your distance measured to be scale invariant. 

Sometimes you don't, but it might be a good property. Like for the cosine, we can scale one of them without a matter or changing the distance between two things. Sometimes you also want to be translation invariant. Sometimes you want to be translation, each variant. But generally what we talk about here is an example given here. So you think about invariance as we think about it here. So the distance between two handwritten digits. I mean, oh, we can take a digit and put it into a vector. 

So there's nothing magic going on here. But imagine that I can compute this distance. Now someone actually writes a six in a slightly different way and you want to compare them. Or you want to compare that the distance between that six and another two in this case. So someone maybe write it in the left hand corner scales a little bit. 

And someone writes a two slightly as Q and maybe slightly smaller again. Do you want these two distances to be the same? If you want that to be the case, then you are looking for invariance. Again, some distance measures have that. But in some cases you also want to try to change in the same way. 

That means a current. Just keep this in mind. So a very important point I want to make is standardization. So we talked about this last time that you want, regardless of what scale your observations live on, your attributes individually, you want, let's say, them to actually sort of be on a, let's say, unitless scale. One way to do that is subtract the mean that we computed, remember, in the summary statistics. 

And you want to divide by the standard deviation. And I'll show you a very clear sort of picture of why that is. But you can imagine something like here. The old word just here is that you can imagine counting the number of children you have, one to five. So it lives on a scale from zero to five, maybe a little bit more, but zero to five here. Age, maybe zero to 120 or something, not 100. Then you have annual income from zero to 50,000. 

Okay. It lives on a scale that's, I don't know, a factor or the hundred greater than the other ones. So that'll mean if you compute distances, the Euclidean one, in that case, it means that actually a relatively small change in this one will have a massive impact on the distance. Whereas a small change in this one, let's say of 10 years, will not actually influence the distance so much. So I'll show you this using some of the similar plots that we've looked at so far today. So I have plotted last night a data set here in a 2D vector space. I've created some data points. And what you should notice here is that, okay, where's the mean? It's probably in here. What's the standard deviation around the mean if you look at the first attribute, or X1? 

Think about age here, for example. It's sort of roughly here. That's the spread along X1. If I think about the spread along X2, it's massive compared to X1. So now I'm interested in, and that's also indicated in the covariance matrix that we talked about. So the variance along X1 is up here. It's one. Long X2 is 16. 

I mean, the standard deviation is four. Okay. So what am I going to do now? I'm going to actually put in some points here to measure things. 

Okay? So what I want to measure is the distance from this point to this point. And I want to measure the distance from this point to this point. 

I want to see what happens when I scale things. Okay? So just ignore here. 

I see that's a bog in my code. But the points actually changed. Just imagine that from the same distribution, just imagine that the same basically. That's not the important point here. 

The important point is that this is a much different scale than the X1 scale. Okay. So what happens here? Well, I'm going to compute the Euclidean distance between, as I said, this one and this one. And here I've actually squared it just to confuse everyone, but it makes it easier to look at. 

And then I took the Euclidean distance between here, basically. Okay? So perhaps remember this number up here, this one here. X3, X4. So what happens if I now actually change X4? So what did I do? I changed it from down here to up here. 

Okay? So I've basically reduced it, as it turns out, because it's going to see with a relatively large, let's say, percentage wise. That's a huge difference from here to here. 

So what happened? Then the distance becomes large. Oh, sorry, much, much, much smaller, right? 

This distance here. But what if I then actually do the same relative change on the X1 axis? So relatively, it's the same change. So here it went from 1 to 0.1, I believe. Okay, what happened? 

Nothing. So that means that this change here, so if you move about on this, along this axis here, has very little change in terms of the Euclidean distance. And that is, in part, due to the scale issue here. So simply because the scale of this variable is measured, let's say, in dollars, like, wage before, and then we have age down here, then this actually the same relative change does not affect the distance in the same way. This is key. 

If you understand this, it's key to understanding distances and why it matters to rescale things. Okay, what can we do about this? Well, I've already told you, right? 

You simply remove the mean from the edge, meets, let's say, point in that attribute, divided by the standard deviation. And now you get data here. So what do we see? Well, we have a variance of one along here and standard deviation of one. 

Standard deviation and variance of one along the other one. So here we've got now a distance of eight between X3 and X4. Okay, what if I do the same thing I did before? So relatively, I'm changing X04 up here. So I'm changing the observation here, right? 

What happened? Well, it's 5.2. What if I do the same relative change along X1? Ah, voila. So now the same relative change along the two axises give the same change of the distance. 

Sometimes you want that. If you don't know that, let's say, imagine measuring stuff in grams and milligrams. Why would you want to compute the distance in that space and not actually do it in the same, let's say, scale, like gram? But of course, if you know that this attribute here, it could be income or something. If you know that that's really, really important, then you might want to actually, let's say, rescale it somehow, keep the original scale. So it comes down to you knowing about what are the attributes you're working with. And that is why it's so important that we actually look at this in the first couple of weeks here. Okay, so aside from standardization, what you now know is important and it has influence, at least on the Euclidean distance. 

What else can we do here? Well, if you think about how can we combine different notions of similarities? Remember the shapes example where we had triangles of different size and color? So if you can measure difference in color and difference in shape independently or similarity, then you can sort of combine that. And it's the same here in this example where we want to combine, let's say, distance in terms of age. 

You can compute Euclidean distance here, right? And maybe the similarity in terms of education. So do you have primary, high school, and tertiary, so university level? 

That's a binary variable. So we can simply compute a similarity metric based on the education, a distance measured based on age, and we then take the inverse to get a dissimilarity into a similarity. And then you can combine different notions of similarity based on your observations. You can also weigh them differently and that sort of eludes to what I said on the previous slide. Okay, so all of these things are in the slides, I hope. Then I just want to make one comment here and then I'm going to jump directly to the visualization, which I think is much more intuitive. 

Now, I've alluded to the fact that you need to standardize stuff. Could we perhaps not come up with a metric that can do this automatically based on this covariance matrix that we talked about? So here we have a covariance matrix and that means actually there's a linear relationship between x1 and x2 because this element here is 2. That means if this one goes up, the other one is likely to go up on average. 

Not likely, it will go up on average. So what I'm interested in here is now measuring a distance. So the Euclidean one can be written in terms of matrix notation like this. Yeah, so that's a missing thing here. So if we then actually consider what do we put in here? Remember the identity matrix applied to something doesn't change it. 

But if we put in this, in this case, empirical covariance matrix into this expression, what we can see here, and I'll just do it empirically and then you can sort of think about why that might be, is that the difference distance here, right, you can think about in terms of Euclidean distance between all of these, that's the same. It's 32. Sorry, between these ones is 32 and between these ones is 16. 

16, 16, 16, 16, 32, 32. In Euclidean measurement. In this Mahan-Lobis distance, where we actually take into account the variance and covariance, we can account for the shape of the data here or the linear relationship. So if you look at the Mahan-Lobis distance going from x3 to x4, where do I have that? 

x3, x4 is 320. It's the squared Mahan-Lobis distance. What about the distance here? So from observation 1 to 2, down here, Mahan-Lobis 82. Voila, I have somehow captured the fact that distance along this one should not be, let's say, as high as it should be the other way around. 

So you can think about this as moving above the data manifold here. Oh, x2, sorry. This one, yeah, so it's even lower. Thanks. Yeah, so this distance here, it's this one up here. Sorry, I should have nice catch. 

Have a look at this and think about what is it that's going on here. But essentially, if you're on the data manifold, you can think about walking on a mountain. If you're walking along the ridge of the mountain, that's not too tough. But every time you have to walk across it, it becomes downhill or uphill, it becomes expensive. So going from here, across the mountain, so to speak, over here, that's a long way. It's uphill and downhill. Okay, this is just an example of why matrix notation is helpful. 

Also, this will enter into the bivariate and multi-variate normal distribution. So have a look at this distance. Then I'll skip this in the interest of time. All I want to say here is that we think we can imagine stuff in 2D, 3D. 

I think we can. You can imagine a cube in 2D, so that's just a square. And in 3D, it's a cube. 

What happens in 4D with that cube? Now we have to imagine vector spaces of dimensions that are 4D, 5D, and even very high D. Like some of your data sets will have hundreds or even thousands of dimensions. What happened to distances in that space? 

If you use the Euclidean one, something very interesting happened. That means that everything is basically equally distance in that high-dimensional space. Okay, what does that mean? It means try to imagine living in that high-dimensional space. And every city in that space is, I don't know, always 4 kilometers apart. So you don't have any cities that are close to you if you are city living in a space. And you don't have anything that's further away than 4 kilometers. So everything is 4 kilometers or whatever it is away from each other. So you don't have anything that's really close. And this is really a paradox about high-dimensional spaces. 

So try to Google that a little bit. But think about what happens in high-dimensional spaces. And some of the distances do break down here. The Euclidean one turns out not to be super great. 

Some of the other ones are slightly less sensitive to that. I'll give you no time for a break. And then I'll hurry on. This was supposed to be... Do you need a break? No, no. 

Let's move on. So I'll do a tour de force of visualization, the quickest I can possibly do without collapsing. Okay, because I think this is much more intuitive than some of the other stuff. Visualization, important. An image is worth a thousand words in the sense that a huge part of the brain is actually, let's say, associated with visual cognition and visual recognition and so on. 

Probably because that's prerequisites, actually surviving during evolution. So why do you need to know a little bit about visualization? It's because in this course and in your projects and everything else you go on to do in your life, being able to communicate graphs and results and your data efficiently and correctly, I should say, is really, really important. So think about when you're actually making plots. 

Think about actually doing this as sort of a... when you write technical reports. You're trying to tell a story. You're trying to answer some question. And of course you're doing that either because you have some data or you need to collect some data to actually do it. So figure out what is it that you actually want the reader. That means whoever's looking at your plots for the projects, it'll be a TA or me. Are you actually communicating what you want to do here? 

Is the story clear? Are you showing it to, let's say, the best possible way? So there's a lot of choices here about choosing stuff, but it comes down to common sense and basic notion of, let's say, clarity of presentation. So think about in text, here you want to communicate an idea. 

Then you say a plot. Also, once you communicate an idea or a result, you want to be correct. That means elementary rules of good plotting. Always put access labels on it. Don't over-proud your plots with stuff that's irrelevant, what you want to do. And of course ensure that everything is readable. Don't really, really, really, you piss off the TAs if you put in too small caption labels like I do all the time, but I'm not being assessed in the same way as you are. 

Maybe I should be. But anyway, and then avoid long complicated paragraphs. That means don't have plots that are over-complicated, have tons of overlays and ten different curves trying to show the same thing, isn't it? Obviously, obviously, don't do this. Don't make or stuff, put it in your graph and to lie about your results. 

Okay, so what can we do? Well, first of all, you need to choose what kind of plot or graph you actually want to use for whatever you want to, what story you want to tell. It could be about choosing the type of plot, scatter, bar, and so on. How will you actually display it? Will you flip the figure 90 degrees and so on? 

Will you actually include all of the data points here, all the results you have, or do you want to, let's say, do a sensible subsampling of your data set? Okay, this is going to go fast. I'm going to do it again. 

So you tell me which color has the highest area. I'm going to do it again, like 200 milliseconds. Wrong. It's not green. It's not blue. I'll show you again. Can you tell now, even? 

It's red by a margin. So a better plot here is to do something like this. In 200 milliseconds, you will be able to decipher which area is the biggest. Of course, this is made up, but it sort of illustrates the point that if you choose wisely and not do like all the newspapers do and put in bar, pipe plots into everything, maybe you can communicate your idea better. What would be an even better idea here? How about just writing in text three numerical values? 

You're spending a lot of time showing this graph for nothing. You can just put in 0.3, 0.35, or whatever it is, and 0.37. Just put it in the text. You don't even need to show a plot here unless there is some really clear purpose. Again, think about when you're actually doing plots. Think about selecting a subset of attributes. If that's the only thing that's needed to communicate your idea or your main finding, think about choosing only a subset of your data. Because, well, you can overcrowd it if you have a million data points. Then these scatter plots are not going to be informative in any way. 

We're just going quickly going through a bunch of different ways you could do this. Basic plots, you all know this, and they're all wrong because there are no access labels, no titles, of course. Just a line plot, you can think about plotting a vector like this. You could do that even like this, and maybe perhaps even better like this. The value of the vectors, like element one in vector x would have that value. That will actually become relevant when you talk about PCA next week to do something like this. 

But just think about it for your purpose, which one are you choosing? We have already talked about the Iris dataset, the flower one, so we have three attributes, and importantly, we also have a class. We could use to illustrate things. We talked about summary statistics about looking at an individual attribute at the time, computing mean, standard deviations, and quantiles, or even percentiles. So can we actually visualize something that will give us the same effect? 

And of course the answer is yes. Here I took a bunch of numbers, so think about this as a column in your data matrix. So here I've plotted the histogram, and you've seen that before. But of course, on such a plot, you could also indicate the basic summary statistics that we talked about. You could put the mean somewhere and indicate that we're aligned if it serves the purpose. You could even indicate the, let's say, the spread somehow. So basic histogram determined by, of course, the distribution of data, but also what is actually the width of the bins here. So if you choose one that's so small that every single data point gets its own bin, then that's probably not a good idea, then you just get a flat curve where there's exactly data and not anything. Beyond the data. So always do this. It's a very good idea to do individual histograms for your attributes. 

It gives you an idea about the distribution, as I said, about the summary statistics that we computed. Good. And then what if you happen to do this for all the attributes here? Well, we get four different histograms, and perhaps the interesting one is this one down here. So Siebelwidth seems to be bimodal. So think about what the summary statistics would do here. 

You'll probably put the mean somewhere here. Is that representative for this data set? Probably not, because, well, it's actually bimodal. So what about the spread? 

Well, that's always difficult to interpret, but probably not either. Because we may want to talk about actually the spread of this group and this group. So histograms, hugely informative to actually go sort of a step deeper than just reporting summary statistics. 

Then my favorite box plot here, which I hate for, again, I hate a lot of things here apparently, but so I don't hate it. It's just that you need to be really, really careful when using it. Because one definition of what these things up here mean is this one here. And it can depend a little bit on the package you use. So just be a little bit careful before you just take what's on this slide and put in your report. But essentially, it gives you an idea, try to visualize that distribution of your data. So it shows you the median, the red one here, shows you where the 75th percentile is and where the 25th percentile is. So if we look at it roughly down here, then we have the median here, 25th percentile here and 75th percentile up here. That means 75 percent of the data lies here. So it's sort of a summary of the histogram. 

Is it a good summary? Well, sometimes if you have a unimodal distribution, then it's not too bad. Now, there's one more really critical thing about box plots that you should not over interpret. 

You will see these red ones once in a while. What does this mean? Some interpret those as outliers. That's not necessarily true. It could be an indication that there's an outlier. So I tend to call them extreme values because they're sort of way out here and not sort of part of the distribution. 

It's in the tail of the distribution. Should you just remove them just because it's a red dot here? Because the box plot said so? 

No, should definitely not. You need to actually have a look at those data points and see what they represent before you remove them. But anyway, it's another way to summarize your data set or your individual attributes, I should say. All right. If you do these box plots for our iris data set, what do we get? Well, we get this. 

But remember this C-pull length thing here. So is this box plot a good representation of this bimodal distribution where you sort of have two groups of points? No, maybe this plot over here. Where we've actually four the C-pull length, like let's call it four or something. We just permuted around that and actually plotted the data points here, the value of that attribute. 

Maybe that's maybe a more sort of truthful way to illustrate that. Okay. I'll skip this. 

I think the solution is here and I'll upload the slides immediately after lecture. I want to highlight one thing. And the point here is that you basically need to look at this one here. So gender. So here we've shown the box plot for gender and we know it's been standardized. Okay. So we have female. 

That was if you look at the problem here, it's zero and you have male. That's one. Meal. So that's arbitrary, right? Zero one, one zero, completely arbitrary. And now we need to count. Well, we don't actually need to count, but think about how many people do you have here in one of these? So as it turns out, in this case, you have more males and females. So when you standardize it, where does the median end up? 

It ends up here. And that means this here, the way you've got this line here, which shows the median and it's collapsed with the 25th and 75th percentile. Means that there are more men in this case that women women because men have a value of one. Okay. So when we actually do subtract the mean from this binary variable, we end up down here somewhere. 

Okay. So in this case, are the only two male and sorry, only two people in this data set. No, it's just the fact that because they're collapsed, it's binary variables. They're actually in the same spot. All of them. Well, sorry, all the females and all the males are here. 

So box plots that's their bottom line are not always great. What's the other one? Maybe on a highlight here. So the question here is that you can rule out is this gender here? It says there are outliers because of this X. 

Should I just remove them? No, is the answer because then you'll remove all the female people in the study. So let's see. See if there's another one. So the Craig one is B here. So let's see even though distribution on this one and this one may have similar shape that does not imply that two attributes are correlated. Also an important point. These two are considered individually in this plot. So we do a box plot phase of them. And just because they seem to have the same shape, we will say that they are correlated or sorry, we can obviously can't say that they are correlated. 

We need to actually compute covariance of correlation. Yeah, because we subtracted the mean and standardized it. So there's the standardization on top. So of course, if you just subtract it, it could never be. 

Sorry, the question is need to repeat for people in other rooms. If we have a value of sort of minus one ish. Then how can we get that if we just remove the mean here? 

Of course, if you read the text, it says it's standardized. So it means that we subtract the mean. And then we also divide by the standard deviation. That way you can get to a value that's that's closer to minus two. So of course, it depends on the exact numbers exactly where you're not. But just be aware here. Also think about. 

Is it actually a good idea to use a standard deviation on a binary variable? Of course, we did it here. So you can do it. Just be careful. Okay. Relation between between points. 

We've already done this several times last time and today. Take the people with product against people length. But importantly, we also now encoded by their class. Useful way to see if there's any structure in your data. 

And this actually hints at the very first algorithm. So if you combine this with the distance measures we had heard about before. If I put in a point. Here, let's call it a test point like we talked about before. 

And I ask you, based on the Euclidean distance here. Which flower do you think this is? Which type? 

Which class do you think it is? What would you do? Well, I would simply compute the distance to all the points. And when I get tired of doing that, I would say, well, there's no one closer than these ones, at least. And then I would take the closest one and I would say, okay, this test point, I don't know what it is. I would probably assign the yellow class of a genica to this one. I could also use more than two nearest neighbors, as it turns out. 

I could use 20 and then I would have to actually consider this, but I could do a majority vote. So now we've actually learned a little bit about the first classifier in this course, aside from some of the very textbook ones, namely the K and N classifier. We'll look into that much more in the future. 

Now, of course, there's a choice here. I chose to say the Euclidean distance. If I'd chosen the L1 distance, I would have to do down here and to the left to compute that distance. 

Let me make that clear. Okay, if I wanted the L1 norm here, also called the Manhattan distance, between my test point, where did I put it? Here, for example. 

And this one down here, I would have to go this way and then down and sum those two. Okay. So, in doing a classification this way depends on your distance measure. So you have to choose that somehow, and we'll look at methods for choosing that. That means choosing the complexity and hyperparameters in our model. Should you do a 3D plot of a scatter version? 

No, never, I would say. Because, again, it's something where it's easier to say what's wrong than what's correct here. But, in general, it can become very complicated to see what's actually going on here. 

So, in this case, it sort of turns out because it's sort of angled a little bit, and you actually have to choose the view that you're presenting to people. So, generally, 3D scatter plots sounds fancy, but don't do it. Do something like this instead, where you plot all, so you have the four attributes down here, and you have these four attributes here. So, you take them all pairwise and do the pairwise scatter plot. It's also called a scatter plot matrix. 

This shows you some information about what's going on. So, can you separate, for example, the classes if you compare people with, and see the length? Well, it turns out you could put a straight line here if you wanted, and separate them. Simply by, well, you can separate the blue from the other two, right? 

Simply by putting a straight line here. Okay, so this gives us some basic insight into what we can probably expect from our classifier, simply by doing some simple visualization. Matrix plots. I think I mentioned heat maps or something to that effect earlier for doing this for the covariance matrix. 

So, you could definitely do this. So, essentially, you take your matrix and plot it as an image. So, the value of the element in the matrix becomes either, it depends on what color scheme you use, of course, but white or black. So, if you take the raw data, and this is a very, very good idea to do if you can actually do it, simply plot your data as a matrix. So, here you've only got four variables or attributes. And then you plot what are actually the measurements you've made. 

So, what do we see? Well, depending on the color code, it means that this stuff over here, these two variables are presumably higher. So, the length or width or whatever it is here, length and width of the petal, is significantly different for this group of points up here. 

So, these are all the rows, right? Then whatever we have down here, that also gives you insight into whether or not we can discriminate. We can do the same for sort of a similarity matrix, it should say here. 

We're looking at what's the similarity between all the objects. And you get to do that in the exercise today, if you get that far. Okay? So, it turns out that the inter sort of class distance is very low. Oh, sorry, similarity is very high. It's not always saying the same thing. Whereas the similarity between, let's say, flowers in this group here to the other group here is higher at least. 

It's not sort of zero in the sense that the distance is infinite between this group here and this one. But we've seen that various distances can achieve something like this. This one, this has become popular for one specific reason in deep learning for monitoring training runs. 

That's the only place where I've seen it actually been used. So, you can have a look at these. So, essentially, we're taking our vector with values and plotting them and color coding according to class. It gives a little bit of structures in the sense that you can see, if I have a petal width of four, now you follow that one back through the line and see how it relates to the others. So, again, there's a little bit of information to be had here. 

For example, here, they separate. So, again, a different way of plotting it. Okay, and remember, so high-dimensional can be a problem here, but you can have many, many, many attributes here on this line. So, this is one way of doing that, plotting high-dimensional data. 

Then, just remember, there is this sort of accent thing here. Is it easy to see what's important in the graph? Is it clear? Is it consistent? That means you're not just changing colors all over the place on notation like I happened to do today. 

Does it actually communicate the information efficiently? Is it actually necessary to have whatever you want in your... So, it's necessary to include everything you have in your graph. Does it, in a correct way, portray what's going on? 

Remember that. Then, if you don't know this guy, you should buy one of his books. They're really beautifully illustrated books about visualization and graphics and all of that. So, Tufts books. 

So, have a look at those. Here's this general rule, and it's not a rule or anything you'll ever have to remember. Just as the ink used on the important stuff, that ratio should be high. So, you should not, essentially, use a lot of ink on unnecessary stuff. That's essentially what it says. Okay. 

Then, just summary, it's basically just a color bullet point. So, we started out looking at these vector spaces and saying, okay, that's important. We need to at least know about the properties of these vector spaces. We did that because we can represent data points or observations in a vector space. We can now do operations on vector spaces. 

We can add, you can imagine, actually adding two images together, or two flowers together, interpolating between them in a vector space. We can measure distance in the vector space. We can measure angle. And we looked at different, let's say, specific ways of measuring distance. 

These equations, you should remember, right in your notes. And then we looked at visualization generally. And those are just common, I mean, common sense, I would say. But, of course, be aware of that. That's all for today. Go do the exercises. Thank you. 
Speaker 1: We're in the third week of introduction to machine learning and data mining. And again, my name is Björn. And today we are going to talk about linear algebra and in particular, principle component analysis. You can see it either as a very useful way to do exploratory data analysis. So you can also see it as an exercise in linear algebra and actually doing something useful with linear algebra. 

So as usual, give some feedback on content, slides, style, AV exercises, everything you want. And of course, we do expect that you read something. So today it was chapter three and you're supposed to do or at least have a look at the homework problems before showing up. Okay, so next week, this is just a warning. Then the reading will be a little bit more extensive in the sense that we are going to talk about discrete probabilities. 

Maybe from a viewpoint you haven't actually done before and then some new continuous density functions or density functions. Okay, so just make sure you start early reading this stuff to make sure you're sort of on board there. But today it's computational linear algebra and we're in week three. So I'll be here for the next two weeks at least and then George's, my colleague will take over for a few weeks and then I'll come back again. 

All right, good. So the learning objectives generally, let's see what we can do with matrix operations on our data. Just some basic stuff to get you thinking about what is it actually we're doing, what's data, what can we do to sort of manipulate that. 

Using linear algebra, so matrices that we talked about a bit last week. And then we're going to understand and apply principal component analysis. And there will be some, for some of you at least some fairly, let's say, complicated mathematics. And I'll make sure to say when you need to know this stuff and when it's more about, okay, let's see how we do this. Just be aware here that you don't need to know everything necessarily in the slides today, but I'll try to highlight what you do need to know. Okay, so I'm not entirely sure where the break should be because I'll improvise a little bit the sort of order of the slides and actually the fact that we have all of the PCA in one week is new now. So it might go terribly wrong, but let's see where we end up. 

So at some point if I start going way over time, like definitely more than an hour, you start sort of looking a little bit distressed and I'll definitely do a break. Right, so we'll revisit linear algebra. Most of the stuff you would have heard, maybe not, and I guess not in the data context, so that's where the new stuff comes in. And then we're going to look at this principal component analysis, sort of a guy here, PCA for short, and how we do that and what we can use it for. So make sure you actually go away here knowing what can we actually use this PCA for and when I say go away here, I mean also from the exercises in the sense that that's also where you gain some of this intuition. Okay, and then some applications of PCA in various contexts, obviously, that plays into the motivation and applications of this. 

So just a few practicalities announcement, nothing new really. I just want to make a comment about sort of the exercise sessions and how you should maybe prioritize them or how I would prioritize the various things you could do there. This is where you have access to all our 14, 15 TAs in this course. 

They're all very good, either they did extremely well in the course with their PhD students, so they know or should know at least what they talk about and if they don't, they've been asked to contact me and if I don't, I'll try to figure it out. So generally, you have the option of doing these weekly exercises that are mostly sort of programming exercises where the code is given. And of course, you need to now ask yourself, I've been given some code, should I just run it and I just look at it and don't reflect on anything. 

No, that's not the intention. Break it, put it apart, put it together again and maybe do the stuff that's really core to the exercise, try to do that yourself in the exercise session or at home or at some other point. So and the reason, and I think I've said this before, the reason we don't necessarily ask you to do everything from scratch is the fact that it'll take a very long time and it is not a programming course. 

So we do expect that you can actually look at the code and gain something from that and the way I would do it, as I said, I would pick it apart, break it down, break it basically and try to change all the parameters I could potentially in that particular script. Not all, but at least the core ones. So of course, those are the exercises that you're sort of given every week. Then you also have the homework problems that I have sort of indicated. Maybe you should have a look at those before you show up for the TA sessions. And if you have questions, you can easily ask those at the TA questions. I would not prioritize spending a supervised TA session working on the homework problems. 

But again, if that works for you, it's also fine. But also prioritize the actual exercises. Then of course, the other projects that we, when we design the course, you say, okay, this is something you need to do on your own time. Now I do understand that the practicalities and actually, let's say, as a group coming together and that's much, much easier when you're already here and you're sort of in the same time scheme. So we also understand if you want to work on it during that two hour session or even after. But again, I just want to emphasize the importance of actually doing these exercises during that time because they do bring a lot of intuition about the data, the methods, and what the parameters mean in certain methods, for example. 

So please prioritize that. This is from last time, so I haven't actually put anything new here. It's the same apprise. I'm still working on getting an answer from the study administration about what a non-programmable calculator is. Turns out to be quite a philosophical question apparently. 

You wouldn't think so, but it is. I'll get back to you on that one. Right, so actually on to, so let me just recap this stuff. So we're going to do a little bit of linear algebra and then we're going to look at the PCA stuff. 

So you've seen this slide before last time. Basically, we think about data as vectors with numbers. So in this case, we measured four things about a physical object. 

We put that into a vector and when we measure a bunch of flowers, we actually put them as rows into this matrix here. Okay, so nothing new here. The problem is, of course, that we are used to thinking about stuff in 1D, in 2D, in 3D. Stuff breaks down, or at least my brain does when I have to think about 4D. What does it mean to be in a 4D vector space or even higher dimensional vector space? We saw that last time, for example, a 20 dimensional vector space. Everything seems to be, sort of, if you randomly put points into a box in 20D and measure the distance between these random points, everything is just roughly the same distance apart. So weird stuff happens in high dimensional stuff, high dimensions, and this is really interesting. You should go look at that. But for all intended purposes here, we are looking for some practical algorithms we can actually use for something. 

So how do we actually deal with this fact that in this case we've got 40, but of course we've also seen images with 784 dimensional vector spaces. So what can we do here? So normally what we do, and what we did last time, is, okay, let's take two of them and plot them against each other and see if we can find some insights here. 

Okay, here I just picked two in particular, no particular, it's arbitrary, okay. But I could also do 3D. I can just manage, just about manage to do this and plot them in a 3D sort of space. It's a little bit weird, first of all, to figure out what angles should there be yet for me to actually gain something insights. And in this case, what we've plotted here in the sort of XY space here, that's actually what you see over here. So Piedle width versus Piedle length. 

So that's the projection down to this 2D space. Okay, what do we do about this fourth attribute? If we actually wanted to find the, let's say, the relations between all four attributes, sort of in one go, what do we do? So one very key aspect of machine learning and everything in general is the concept of dimensionality reduction. 

So can we actually find a subspace of this 3D space that captures most of the interesting, or let's say, information, which we will define as variance. And we'll do that in some detail. So can I find a line? So now I'm maybe try to draw just a random line here. This is one line. 

It's a subspace of this 3D space. If I project all my data points onto that one, will I gain something useful from that? At least I can plot now stuff on a line. And I can also do this in 4D. 

So you have to make that jump from 3 to 4D now and say, even in 4D, I can draw a line, whatever it looks like. I can project all my data onto it, and maybe I can gain some information by doing that. But in the end, it's about, let's do dimensionality reduction without naively simply just picking two of the attributes. But actually creating new, let's say, axes that are combinations, linear combinations in this case, of the other ones. 

That's the whole name of the game. So, and again, this comes very much down to dimensionality reduction and visualization. So exploring this high dimensional dataset by visualizing it and see if you can find some patterns. For example, here we could see that actually if I put a line here, I can separate the various iris flowers or categories. Okay, so I learned something from doing this in 2D. But again, maybe if I actually included all four, I would also be able to separate the yellow and the red one here. 

That turns out to be the case, to a very high degree. Okay, so now I talked about spaces, vector spaces, and vectors. So the sort of natural language here, and for talking about this, these vectors and spaces and stuff, and manipulating them in particular, is linear algebra. 

So this is where we can basically manipulate the individual vector, project it, rotate it, and I'll show examples of that. And perhaps generally transform our data somehow using the concepts of matrices, which will turn into functions in a bit. But generally, linear algebra, I guess you learned about it in Mat1 and you didn't really know what, why is this useful. 

I'll show you why it's useful, hopefully today. And then also think about this, just as a basic concept, we have already talked about representation of data using vectors and matrices. I've just indicated maybe we could actually do something, for example, rotating stuff, and for example, rotating the plot we have here. So how can you rotate this 3D view using a matrix? 

I'll show it for 2D, so don't be too excited. But then generally dimensionality reduction, visualization, linear algebra is key to that. As we'll see in a few weeks, model formulation, and what I mean by that is that functions that you may normally write up as a times x plus b. We will write those in matrix form, or vector form, not just, but to make them compact and easily manipulable, meaning that we can manipulate these expressions easily. Then also coming, playing into this is the concept of optimization, where if we want to take gradients of an expression like this with respect to, for example, the weights, which we do, then actually also representing gradients as vectors and matrices is also hugely important. So there is some use of linear algebra. So we already talked a little bit about the anatomy of vectors and matrices last time. 

So today I'm going to just continue a little bit on that, not rant, but talk, or direction at least. So that should be an A. So in general, we talked about a vector space last time. So we have an m-dimensional vector space. We are in this course only thinking about real vector spaces. That means we have some coordinates, and they all live in this R-M space here. So in this case, the three-dimension one, we have some coordinates for the point x, or the, let's say, error going from the origin to x. And then we say it's closed under linear combination, meaning that if we add vectors together, it stays in the same space. So, technology. 

All right. What did we gain from that? Well, putting up this sort of technicalities last time allowed us to actually measure distances, and in particular, distances and length of vectors. So we looked at the Euclidean norm, or in more general, we looked at the ELP distances. So in this case, we have this expression here, that's the Euclidean norm. Then today we're also going to, if you really want to, we're also going to look at the Frobenius norm, so that's, to some extent, the size of a matrix. 

And it's exactly the same, almost the same. So in this case, we are squaring that norm here, so we get rid of a, let's say, a square root somewhere, but it's essentially taking the individual elements in the vector, squaring them, summing them all together. Okay. You can write that as this particular trace operator, and that is taking the sum of all the diagonal elements in that resulting vector here. So expressions like this will be a little quite important to today's lecture, so I'll come back to that. Not the trace stuff, that's a little bit of a side note. All right. 

Good. So I promised, or at least I think I promised to show you a little bit about what can we actually do to operate on data points in a vector space. So if you think about, I hope it's big enough that you can see in the back, but here I've got my original space, it's a 2D space, I've got four data points. How can I apply a matrix of some interest at least to that, to those four data points and do something? So if I formulate a matrix like this, so it's a rotation operation, and I apply r2x, what do I get? For example, with this r here, I get this stuff here. 

So now I rotate it in that vector space, I rotate it on my data point. Okay. Why would I want to do that? Well, for now you can just think about if you actually wanted to do that, because you wanted to flip all the visualization of your data points somehow, you could do that using matrix operations. You can do the same for scaling and stretching. So here I've just scaled one axis by one and a half it seems. 

So I'm looking at this matrix here, and then the other one by a half. Okay, so I can squeeze stuff together and I can pull it apart. I can also do that to my data set. 

So remember, all of these red points are essentially data points, their observations in that vector space. Okay, what else can I do? I can do what's called a shear. So that's essentially sort of removing this parallel idea here, so I get this shape down here. If I do it along the x-axis and this one if I do it along the y-axis. So I can pull it sort of in two of the corners and pull it apart. Okay, I can, well, why is this interesting? 

It's interesting in the fact that all I've done here is taking, you can think about what I've done here. It's to apply a function to my x. That function happens to be possible if I apply this matrix here to my x. So it's a linear function. 

It's a linear map. Okay, that's to some extent quite profound, but of course not any particular usage. But what can we possibly do here if you actually put these operations together? So if I apply r to s2d and then to x, I can do something that combines all of these. 

So there are interesting small things you can actually play around with here. And we don't have an exercise for this yet. That'll be next semester, I think, because I think it's important. But maybe try to actually do this in the exercises, in NumPy or in MATLAB or whatever you're working on. It's a fun small exercise to think about what small operations can you do on your data points. There's one operation here I haven't actually talked about, and that is a projection. So I haven't projected anything yet, and that's what we're really going to talk about today. 

Okay, but just remember these rotations, scaling and things that these are essential to what we're doing. So if I want to project something, I need to talk about a subspace. And I've already alluded to this, but you can think about a subspace just generalizing the idea of a line. 

So in this case, we've got the black lines here. These are the canonical, let's say, bases, so x, y, z axis. And then I've shown some particular subspaces in this space. So one of them is this blue line. That's a subspace in our three, three, sorry. So that's a line. 

It's a one-dimensional subspace there. There are lots of these, right? So maybe we need to worry a little bit about which ones we choose. And then we've got another subspace here. That's a plane. In this case, it's aligned with the z and y-axis, the green one. 

What about this red one here? It doesn't seem to be aligned with any of the canonical axes or bases, vectors. But anyway, it's a hyperplane in some space. In this case, it's a 2D plane. But if you now think about this space as 4D, what would a subspace in 4D be? It would be a 3D space, okay? 

Or 2D or 1D space. So try to get a head around it. It's really hard to imagine this geometrically. That's also why we need the language of, let's say, a little bit of mathematics to help us with this. 

Okay. But anyway, so in this case, we say that one of these subspaces is spanned by some vectors. And of course, the canonical 3D space, the one spanned by the x, y, and z-axis here, that's sort of the one we know. And essentially, all the vectors lie in that span. Okay, so that's also what's shown down here. So if we take a subspace of R3, spanned by z and y, then we have all the possible combinations lying in that subspace. Now, some, you've heard about bases before, I hope. So some particular bases, Cs, I guess, are particularly interesting. And that is if they're spanned by linearly independent vectors. 

And we are really going to talk about that today. So what's a linearly independent vector? So if I got multiple vectors, simply means that I can't, so if I pick one of them, I can't come up, I can't combine the other ones together in any way to get the one I just picked out. In other words, if I want to do the zero vector here, the only way I can do it from the n-vectors I have available is that if I put all of these to zero. And again, these are, to some extent, technicalities. But anyway, so if we have a basis, then we have a so-called vector space where it's spanned by these linearly independent vectors. So we can have a number of these. Okay, so again, if you think down here, I think that's maybe the easiest thing to sort of make it concrete. We've got the 3D thing, R3 here. We now span a 2D subspace using V1 and V2. And now, with this point down here, we make another sort of, let's say, statement here. And that is that these two vectors, they need to be orthogonal and they also need to be of unit length. 

That means we have a so-called orthonormal basis. This is going to be really, really important. We are going to automatically find one later today. 

Okay. Now, if we actually collect these vectors, these two vectors in this case that span the subspace we're interested in, we can do that sort of as columns in this vector V here. Now, if the basis is orthonormal, then we remember from last time that, well, actually, that's a, it is orthonormal if we get this stuff here. 

And that means that the inner products are, as we wrote down here, the inner products between two vectors where i and j are not the same. Should be zero. That means they're perpendicular to each other. 90 degrees in this case. 

What is 90 degrees in 40, 20D? Think about that. Anyway, so we now have sort of a little bit of, this is mainly sort of notation. 

You've probably hopefully seen all of this before. Just remember, for now on, we are interested in subspaces with an orthonormal basis. And why are we interested in that? We want to project onto a subspace to find a lower dimensional space where we can potentially see something interesting or maybe even visualize the stuff. 

Okay, so something here that you have hopefully all seen before. So again, we in 2D, we have now a subspace, a one dimensional subspace, and it's this 2D. We have a data point, x. 

We want to project that onto this subspace. So all we have to do is consider the angle between the two. We know that relates to the inner product as written up here. And then we are interested in this, let's say the coordinate or the length of the vector from the origin up to here, where they intersect. 

All right. So this length of the projection is in general this expression here. So it's essentially the inner product between the two vectors divided by the length of v. Okay. But I just told you that the length of v is one. 

It's an orthonormal basis, or at least that's the assumption I'm going to go with now. So that means we have a rare, we have an extremely easy way of projecting from, let's say, from our data point x down to some subspace. Of course, I can pick another x and before I show you four data points, I could be a million data points. I can predict all of them down to this line and then I can look at what actually happens at that line. All right. 

This you should all have seen before, I hope. Okay. So if we just do, let's say a small example again, visual. So we have the subspace of some dimension here in. So in this case, we have the 3D original space and now we have a subspace of size two. How do we actually in general, in vector notation, project down to the subspace? So I want to project from this x down to the 2D plane here. And of course, what comes out of that is that I get two coordinates instead of three coordinates. 

So up here, remember, that's always a very good sanity sort of check. Do I actually have the number of coordinates that I expect? Are the dimensions of my matrices and vectors as I would expect. So up here, I have a vector that's sort of given by three coordinates, x, y and z. Coordinates, whatever they might be. 

They're written here. But they're not lying in any of the trivial planes here. I want to project that down to this subspace here. And I simply do that by an inner product between x and v, like we saw before. 

Now, v is sort of composed of two of my basis vectors, namely v1 and v2. They are orthogonal. They also happen to be orthonormal. So that means the coordinates in b, and we tend to call these b. So that would be the coordinate. 

Let me maybe write that. So this would be a column vector where we have b1 and b2 in the example shown below. So b1 would be the coordinates. There's a lot of b's here. So we have b1 here, and we have maybe, I don't know, b1 here. Or b2, sorry. So these are the coordinates in this new subspace. 

So now I only have two numbers, right? Because I have a subspace spanned by two basis vectors that are independent. So if we do another example here, or actually do the concrete example, and it's not particularly interesting to be honest, but let's do it anyway. So here I've got my basis. So I've got one vector here, one vector here. And I've got my data point with these three coordinates. I do this inner product here. So one times, sorry, x times one, zero times y, zero times z. 

That gives me the first coordinate, and that's exactly x. Okay? That's not surprising. 

It's a sort of particular case here. And if you do the same for the second coordinate, that means b2. I take this vector here, inner product with this one. So I take x times zero, y times zero, z times one. I get z. 

So what has happened here? I've been just removed the dimension, right? So it's sort of a trivial projection in that sense, or not a trivial projection. 

So it's a trivial subspace because it aligns with this canonical basis. What would be a more interesting one? Let's have a look at this one. 

Hopefully you can sort of see it. So in this case, we've got x, y, and z. We've got a point living in this three dimensional space. 

It's the blue one here. And then I've defined a hyperplane or a plane in this 2D space. So that's a plane lying somewhere. It's not aligned with any of the usual basis. Okay? I want to project the blue point down to the red one. 

So that means instead of having the three coordinates now making up my blue point, I want to have two coordinates that are sort of given in the subspace spanned by v1 and v2. Okay? I simply do the same thing. 

Okay? But now I have sort of a space in 2D. That means I can visualize stuff in 2D. 

And that's exactly what's been done here. So here I've got sort of the first, let's say, axis that's given by v1. Here I've got the second one given by v2. And in that space, I can now plot the two coordinates there right here. 

These are the ones lying there. Now, can I now go back to my 3D space? Well, you can. 

But of course you lost some information here a little bit at least. Because you've projected down to 2D. So if I now take my two coordinates here in 2D, I want to go back into 3D. Okay? I need to use my basis and go the other way here. So that means I end up at this point here, not the blue one here. Just remember that. All right. So hopefully it's clear that we are interested in subspaces and projections onto that subspace. 

I haven't told you anything about how we find that subspace. I've just given you some. But of course that's going to be the exercise for the next hour or so. Okay. So I've already, so we are now going to actually put this into, let's say, play. So we're going to talk about principle component analysis. 

And that's just a name. I'll explain what we mean by principle components in a bit. But generally we've seen data tends to be high dimensional. You measure more than one or two or three things about the world. And you put this into a matrix and we have many, many dimensions. Sometimes hundreds, sometimes thousands, sometimes a million and so on. 

Okay. The whole name of the game is we want to find a lower dimensional representation of the high dimensional data, either for analysis or if you in doubt, think about visualization that imagine you could not do a 2D plot. Could you find a subspace here that would explain a lot of the information you have available? So if I draw a line here, then maybe the data is, it's a good idea to project onto this, this plane here. So in this case, it turns out that X1 and X2 are all, they're very linearly dependent. 

Not totally. It's not like if I know X1, I also know X2, but it's close. So maybe you can imagine this as being two sensors, X1 and X2. They measure exactly the same thing about the, let's say, the temperature in here. Just hadn't, there's a little bit of noise on these two, let's say, sensors. So when we plot them against each other, they're not really sort of on a straight line, but in principle, they should be. 

So what line would we actually draw here to do something? So sorry, so in principle, they live in a 2D space, you measure two things about the world. It happens to be the same physical thing, but in reality, they live on the same one-dimensional subspace because they're measuring the same thing. So that means instead of representing the temperature measurement by two numbers, we could represent it by one. And we could do that by drawing a line here and then projecting my points onto that. So let's try to do that. 

We'll probably do that a few times. So this black line is supposed to be a straight line. So if we look at the points projected onto that, I would get something like this. 

Okay, so I can do that. It's not a really interesting application here, but if you think back to the iris dataset, and oops, sorry, I'll come back to this one. So if you think about the iris dataset, then that might look a little bit more like we have here. 

So in this case, we've got x, y, and z. We've got a tons of data points in here. But if you look at it like we did last time as well, which direction is the data more interesting if you just think about it intuitively? It's probably along the direction where there's most variance. 

At least that will be sort of an assumption here. Okay, so what about the next most interesting direction in this 3D space? Okay, so aside from this one, if I now pick an orthogonal direction or perpendicular direction to that one, then maybe this direction would be the more interesting one. 

So I'm looking for a 2D subspace. So this direction here, there's a lot of variance going on, or sorry, there's a lot of stuff going on. Along this one, not so much. I could of course also have chosen another direction and projected onto that. But this idea of choosing that direction along which there's more variance is the underlying assumption of what's called principal component analysis. Now again, if I do the same thing I did before, we have the data in 3D, so we have three coordinates. I somehow define a 2D subspace and I plot, I project onto that and plot my data points in that 2D subspace. 

I get something like this. Okay, well, have we lost any information? Maybe not, just by eyeballing it, but of course that's difficult to see, so we're going to actually measure that. But anyway, we can do this. If you have data in 3D, you can pick a subspace in 2D and plot your data points in that space, just to make the visualization easier or maybe elicit some latent structure in your data, for example. 

Okay, let's continue this game a little bit. And when I say high dimensional here, you need to jump to the conclusion, I'm not talking about 2D, this is for illustrating the point here. So generally, you need to make a jump from this 2D examples I give you, or 3D, 240, 20D and beyond that. 

But, so here's a sort of small exercise for you. If it's not a CRISPR-Z, but if you look at these data points, and these data points have the same coordinates in all three of these panels, you have to believe me here, I know it looks like they don't, but they do. So this point here has a minus 2, minus 4, minus 2, minus 4. So all the points are at the same coordinates. What I've done now is put in three different directions, and projected my data onto that. 

Forget about the colors for now, just think about along which one of these directions is actually the more or the better one to project your data onto. Keep in mind, I talked a little bit about variability and variance. So if you think about what does that mean here? Well, variance is variation spread around the mean after I've projected my data. So if I put my mean in here, let me find something that's a little bit brighter. So if I put, that's roughly the mean of these data points here, along that one dimensional axis. If I look at the spread here, that means the spread along here, versus the spread along here, versus the spread along here. 

It's a little bit difficult to see this one and this one, but this one is definitely smaller than the other two. So it matters which direction you choose if you want to maximize the variance after you have projected your data. And the name of the game, and I'm going to keep boring you and repeating this, the name of the game is to retain as much variance as possible, at least along this first direction that we choose here. So how do we do that? And the general strategy has already been laid out here. We find the direction in whatever m-dimensional space we have, that captures the most variance after we've projected the data. 

Okay? So we need to measure variance, and we did that last time. We measured variance, empirical variance. After we've projected the data, and we've already seen how we can project the data easily using in a product, and then we simply need to find, and I'll say simply, it's not going to be so simple. But anyway, we need to find the direction V. You can think about fiddling around with that V. You can try all possible options until you find the one that maximizes the variance after you've predicted all the data. Of course, that's not how we do machine learning on numerical optimization, so I'll have a small trick for you. We can do this automatically. 

Okay? But intuitively, we want to find the direction here and down here in that 3D space, that after I've predicted all my data onto it, forget about the colors here, after I've projected all the data onto it, the variance is the greatest. And from looking at this one, this might be the better option here. 

So we know how to measure variance. How about we actually try to do that? So I know this is a little bit of a nasty slide here, but bear with me, and you don't have to remember all of this, but if you do, I think it's a good exercise in actually manipulating these things, and it's not as complicated as it looked like. 

Okay? So I've just repeated basically what we've done a couple of times now over here. So I'm in a 2D space, I have a line, and I'll project onto it. All my points are simply project onto it. If I assume that I have a unit length basis here, so V1, what's called V, yeah, we call it V on the slide, and after I've projected it onto this 1D line here, that's what I've done down here, then I compute the mean of all the data points. We did that last time. I haven't shown the variance, but I mean, you know how to do that as well. And then we just need to maybe put on the slide how we actually project. 

Oops, that was not a great caller. Probably need to just remind ourselves how we actually project onto the line. So essentially, we write it here as xt times V. Okay, so if you take the data point, that could be this one here. I've projected onto V, so in this case, V would be a unit vector starting here and going one up here of length one. That defines the whole direction on which I'm projecting. So once I've projected, I get some coordinates there. 

Yeah, okay, let's start and see where we end up. You know how to write up empirical variance of numbers lying on a line. Oh, yes, okay, it's whatever coordinate you have, so that would be the B. Okay, I have all my Bs here for B1 to N. I subtract the mean of all of these data points, so that's what's indicated by M, and I square it. 

That's empirical variance. And remember, just keep in your bag of head, this is what I want to actually maximize in the end. All right, and then I divide by this stuff here. 

That gives me a non-biased estimate. Okay, what can I do now? Well, I've already said that B, B, I, B, I, that's simply the inner product between X and XI and V, so I plug that in. That's all I've done here. Now I need to look into my Victor sort of cookbook or whatever you call it, and I pull this V out here to the right. 

So now I have an expression here. What does this tell me? It tells me that, okay, I need to take the data points and subtract the mean of those data points. 

Okay, well, I can do that up front, so to speak. So I can define a new variable called X tilde here. This is simply where I've removed the mean from that data point. 

It's clear what it means to remove the mean from this data point. Yeah? Okay, come ask me if you don't know what it actually implies. Okay, but anyway, that's all I've done here. So I took this part here, put it into a new variable, where simply the old variable where I removed the mean. Okay, and then I still need to multiply by the and still need to do the inner product and take the square. 

Nothing has changed there. Now I expand this expression here and move it around a little bit. So I exploit the fact that I can, if I have the, I can move around the order of vectors if I have the transpose. So what I can get here is this expression down here, instead of this one up here. Okay, I still need to sum over I. But now if you really sit down and concentrate hard and keep your indices sort of in line, then what you can do here is notice that the V here doesn't depend on I. So we can pull this sum inside and only deal with these two, this inner product here. That turns out to be, that we can write all of this. So we can write the sum as simply a matrix product. 

So matrix product of the transpose data matrix centered, that means I've removed the mean, times the centered data matrix. But now I have an interesting expression here that we will come back to a little bit. So just to summarize, what we've done here is simply just measure. You gave me a line V, this line here. I've just measured, once I project all my data points onto that line, what's the variance? And I have a sort of a hidden agenda that I want to find, or actually I want you to give me the line that maximizes this variance. 

But now at least I can quantify the variance. And I've done nothing more than use basic rules of linear algebra, manipulating order of transposes, or transpose, and of course this thing down here at the end. So just a few basic rules. Just a side note, if you can't remember all of this, that's a very good note actually from DTU called the Matrix Cookbook. I would encourage you to actually go into that, not only go into that, but keep that in a bag of your mind. Every time you need to do matrix vector manipulations, that's a very good resource. All right, now I've got sidetracked a little bit here. 

We have the variance, we can measure it once we project our data points. That's it. All right, now here's the slide where you don't need to remember everything. So this is a little bit beyond what most of you would have seen by now, and even by the end of DTU. 

So I'm just going to give the gist of what's going on here, and then come see me if you want to know all the details. This I think you should all be able to do. This I don't think you should all be able to do, at least sort of from scratch. But keep in mind, what I want to do is maximize the variance. That means I want to maximize this expression with regards to V. So I want to fiddle around V until I maximize this expression as much as possible. Okay, that's a so-called optimization problem, and some of you would have seen this, some of you would not, but we'll see a lot of these in the coming weeks. 

So we are a little bit early here. But this is just the expression capturing the variance after we projected the data set in x tilde down to V. Okay, and I want to maximize the argument here with respect to V. I also want to make sure that I actually have an orthonormal basis, because that's nice and handy, and it's sort of the way PCA is defined. Okay, so we also want to subject to something we want to find as V. Now, that's a little bit of a nasty optimization problem. You probably know how to find the optimal of the function. 

You take the gradient and put it to zero, for example. You've probably seen that in high school or wherever. And then maybe sometimes you can actually solve that analytically. But what I have here is a so-called unconstrained, so you should say, yeah, I have a constrained optimization problem, I should say. And we're going to use what's called the Lagrangian to actually help us a little bit here, and that's a trick. 

You don't need to know this. It's just a way of making the optimization problem analytical as it'll turn out. So what I do is that I add a little bit of sort of a cost here, if you will, says that if we are not, let's say one, that means that the length of V is not one, then we are somehow penalized. And then the whole hand-wavy thing here is that, conditioned on this being zero, then what we have to do now is actually maximize this thing here. So now I have sort of a new expression here, so I had the old one, that's the variance. And then I have this term here that measures sort of what's the mistake I made. 

What's the mistake I make in terms of the length of V? Now, hand-waving a little bit, you know how to optimize function. I said that you take the derivative of your function and it created to zero, and again, as you said, maybe we get lucky somehow. 

I'm not going to show you how to take the gradient of this expression here, because some of you would not have had multi, what's called a multivariate calculus yet. So it's a little bit beyond the point, but look in the matrix cookbook if you actually want to see this. It's very nice to know this stuff. So anyway, just believe me that I've done it correctly, at least I hope here. So we need to do that in terms of both V and also this new weird lambda thing. Okay, so this lambda thing is sort of a scalar that I put on to this problem here. Now, in the end, if I manipulate this, you can see if I take this stuff, leave it on the left-hand side, take this stuff on the right-hand side, get rid of the two, I get an expression like this. 

That was for this part over here. But I also need to worry a little bit about this part over here, and what this is telling me is nothing more than actually V needs to be one, that the length of V needs to be one. So I haven't learned a lot there, but we'll come back to how we find this one. But you've seen an eigenvalue problem before, and never wondered and never really questioned what should actually use this for. Now you can use it for actually solving this, because what we can do here, you know how to plug stuff into NumPy or Matlab or R, and get the eigenvalues and eigenvectors of a matrix. 

The matrix you need to put in is this one here, and I'll come back to this one. Okay, and then you multiply it by some V, and all you get is a scalar times V. So this is a really special vector, this one. So remember that, that's the eigenvector here of that matrix. So an eigenvector is, as I said, it's a really, really special vector, because when you apply this matrix to it that's defined by the data, all that happens is that I scale it. I scale it by this amount here, by this eigenvalue. 

So if we, for example, I think I'll actually wait a little bit with this until the next slide probably. But anyway, you know how to get to solve this problem. You know how to find V if you have this matrix, and you do. 

Because remember, this matrix is nothing more than the data where we remove the mean. So I think, just keep that in mind for now, one more slide. Okay, I said something about this, it's actually the eigenvalue, this lambda thing here. Okay, so this stuff up here didn't help us, or sorry, the gradient of that stuff didn't help us a lot. But if we actually look at our expression for the variance, it was this expression here. And we now know by solving for V, we know what this thing here should be. So if we plug that in, then we get basically that the stuff corresponding to this part and this expression over here gets replaced by lambda V. So all I have is this. 

And if I know that V is one, then I get that the variance along B is simply the eigenvalue. Job done. You have to believe me that this is job done, I guess. 

So don't worry too much about the argumentation facts we saw coming to this conclusion. But the important thing is that you remember, or at least appreciate, that what we have here is an eigenvalue problem. If we can solve that using Ike or whatever it's called in NumPy and Matlab. And then we can actually find one direction that maximizes the variance. What happens to all the other directions? Because we said, for example, in 3D, that there would be, let's say, sub-spaces of two dimensions that are interesting. So what happens here is we need to repeat this, essentially, for all the other ones, but also having this constraint that the next V, the next direction we are interested in, is actually perpendicular to the one we have just found. 

But we have an eigenvalue problem, we can solve that. So now back to this thing. What is this thing here, actually? 

Some of you might have seen this before. But that was the reason why I included something about the covariance matrix last week. Because it is, essentially, or not essentially, it is exactly the empirical covariance matrix. So what we had last time was this expression here, to find out how do two variables or two attributes, how do they co-vary? So all we have to do is sum over all our data points, take the data point minus the mean for that attribute, times by the other attribute minus the mean for that one. 

Then we get a matrix here. So up here it says, how does attribute 1 co-vary with attribute 1? And that turns out to just be the variance. 

So what about this one? That would be the, how does, in this case, attribute m, how does that change if we increase attribute 1, for example? On average, that's what we are measuring here. So we get a covariance matrix. And I think if some of you would remember that, I called it sigma last time, there's a very good reason why I'm not doing that, which you'll see on the next slide. I just called it s hat here. 

You would also just call it co-variance hat if you wanted to. Just keep in mind that this matrix here captures the something, something, the linear, captures the linear relationship between all the variables. Now, yeah, this can conveniently be actually computed using this expression here. 

And maybe I'll leave that as an exercise for you to show that you can go from this one to this one. But I've actually used it once today, because if this mean here is zero, that means you remove the mean from the data matrix, just get a product, X i, and so X i l and X i k. And try to write this out, actually try to write out these matrices. So this is a way to compute the covariance matrix easily. You don't have to worry about implementing sums and whatnot in your favorite programming language. You just do the matrix product between the transpose data matrix and the data matrix, centered, important, centered. 

Otherwise you don't measure variance and covariance. So are we done now? Because I told you, you can find V. We just have to solve that eigenvalue problem, and all is good. So it turns out, and you can do that, I mean you can plug this matrix, and you should do this if we haven't done it for you in the exercise, you should actually test this, that I'm not lying to you. 

So create this matrix, plug it into this numpy function, and get the eigenvalues and eigenvectors. See what happens. But it works in your case, I'm sure. But generally there's a better strategy for finding these eigenvectors and eigenvalues. 

Instead of using these sort of standard functions that you have, there's a particular decomposition we can use, and that's called a singular value decomposition. So just out of curiosity, how many have actually heard about this? So for those of you who haven't tried to look around, it's basically 10%, I think. So this is not something we expect you to know about. So I'll go through this, and I think let's do a break now, and then I'll go through it afterwards. So let's meet at two o'clock. Have a seat, guys and girls. 

All right, let's move on. So you can actually get to do this yourself. Okay, so there was just a question about this whole, what does it mean to center and remove the mean of a data matrix? So let me just insert a small slide here that where we tried to do this. So remember, we have our data matrix. Here we have our, let's say, data points as rows. Here in some, in here, two-dimensional space. So we have two values per row. Okay, so what I said we wanted is this thing here, tilde x. 

Okay, how do we get that one? I said we simply need to remove the mean from each individual data point. So if we look at this from a high level, and I have plotted a data set down here in 2Ds, I've got x1 and x2, canonical basis. All the blue points are data points. And I want to find the mean of that data set. So that means, yeah, that means I want to take the mean along x1 and the mean along x2 independently, of course. So the mean along x1 would probably be around here, along x2 around here. So I would plot the mean of that data set somewhere here, for example. And that's essentially also what I did on the slide I showed you. Okay, so if I take that red point and remove it from all the blue points, that means subtracting it from the blue points, how would that new data set look like? If I plot, now I need to, of course, be a little bit careful, so it's not going to be perfect here, and maybe we should do it automatically. But it means that this thing here essentially moves up here. 

Okay, so if I'm just to have very withdraw a little bit from data points that is centered, rough, it should be exactly the same coordinates. It just moved up or trends, that's not great. Okay, this is a little bit hard, right? But hopefully you get the point. So now the mean is actually 0, 0. Does this make sense? I don't know who asked, who was that? 

Somewhere. I hope this makes sense. I really hope this makes sense. 

Yes? Okay, great, this makes sense. Then I just want to make another small point, and this will be something to think about now. Remember when we did, we found that v thing here. I said we could find that v somehow, solving an eigenvalue problem. 

I could find a direction in 2D, along which the variance is the greatest one I've projected onto it. Okay, what is that v? So this thing here, I called, once I called the ex and ey, that's the unit vector here. So that would be 1,0, you've probably seen that before, and 0,1. That's sort of the basis for the canonical basis. So if I take now a vector here that I want to find, it's somewhere, what would it be? 

Well, I don't know. I can draw it up here. So I draw a vector here, it has length 1. But if I actually put this, it's going to be a combination of ex plus e y with some weight to it. Just arbitrarily weight here. But these two coordinates sort of indicates that this vector is actually a combination of the two. 

So if I measure height here and weight here, and I project onto that, it means the scale I'm on is sort of a linear combination of the two attributes I measure. And that's the key here. I'm not just removing one of them and only keeping the coordinate along the x-axis. Think about what this v means, and we're going to spend a long time actually looking into this. Now, jumping back, we had an eigenvalue problem. 

I said maybe it's not the greatest idea to solve it the way I told you. So let's not just naively create this matrix and put it into an eigenvalue solver and Python or MATLAB or whatever. What we tend to do, because it's numerically more robust, is to use another type of decomposition. So I think we need a slide about the eigen decomposition for next year or next semester, but for now, you've probably heard about it, but for now we are stuck with what's called the singular value decomposition. 

And this is a way you can do a lot of things. It can also help us solve the eigenvalue problem. So just accept that this is a decomposition and any matrix can be decomposed using this method. 

So it doesn't have to be symmetric, for example, like for the eigen decomposition. So here I've got a data matrix. I've now run my SVD algorithm. We don't have to understand exactly how that works. It gives me a U matrix that's in by in. 

It's orthonormal, meaning that the columns are, if you take the inner product, it's zero and they have link one. We're not going to worry too much about U because it's not necessary. So just accept it's there. Then we get another special matrix here and it has this form over here. So it's a diagonal form in the upper part. So the first diagonal here, that's going to be what we call a singular value. It's not variance directly now. 

Be careful here. It's called the singular value. If you're talking about this in the context of SVD, so your singular value one, two, three, blah, blah, blah, up to XM, up to the dimensionality of my data. So one more thing to notice is that everything below here is zero. It's going to be a little bit important later, but it means that we only have values in that weird diagonal up here and everything down here is zero. 

But then we have a very, very important matrix here. It's an M by M, so it has the dimensionality of your data, four attributes. So it's four by four. 

And in the V matrix, not the transposed one, so be careful what you get out of your SVD function you call. We have as columns. We have as columns. We have as columns. 

Remember this. We have these V matrices, oh sorry vectors, these eigenvectors, or the principal component that define the principal component direction. So the terminology here is hugely important also when you solve exam questions. So make sure you read into this and get this. 

So we have M of these. This V here is sort of that's the first one is the one that defines the direction along which the variance is greatest. As it turns out, the next one with the highest, second highest, the singular value is a scalar, this one. There we have the direction perpendicular to that one that has the second most variance, of course, once the data is projected, so on so forth. But we have perfectly decomposed it if we keep all of these vectors and all of these, let's say, values in the sigma matrix. 

And now you see why I don't want to call the covariance matrix sigma here. So you need to be careful here. All right. 

I said something about these things here. I said it indirectly, I think, so I'll just emphasize it. I said the U is an orthonormal basis, or it's an orthonormal matrix. So is V. And of course, if you have eigenvectors of a symmetric matrix, then we get orthogonal vectors here. 

Okay. And then this one, of course, it's not symmetric. Oh sorry, it's not orthonormal anyway because it has this weird structure here. So the important point here is you have a data matrix, not the covariance matrix per se. We can do that. 

Maybe you should try to do it and see what happens. But this is the data matrix, typically centered, which we'll see in the next slide. Out you get a V matrix that contains the eigenvectors that define the principal component directions. They have length one. If the inner product between them is zero, that means they're independent, they're orthogonal. And we get this sigma matrix here which sort of defines something that we'll look at in a minute. They are ordered, so this one should be the highest, single value, second highest, so on and so forth. So to some extent, this is just the thing we have a way to decompose a matrix. 

It's called SVD, has certain properties. Now, an orthonormal basis, remember from last time, if you take that vector, sorry, that matrix, transpose it, multiply it to itself, essentially, on transpose, then you get the identity. And this is the consequence of these vectors being independent, so when you take the inner product between them, then you get zero. And you only get one. 

If you take the inner product with one vector, let's say the first one, with itself, you get one. All right. We really know about the sort of anatomy of these matrices because it's going to be important. So in your code, look into it, look at the numbers, try to pick them out and see what are they, are they actually one, do they have a length of one? It's the only way you're going to learn this. Now, another slightly nasty slide, but this is to connect what we have in terms of the eigenvalue problem and the SVD problem. And so essentially what I'm doing now is saying, okay, I'm going to take my centered data matrix. That's the one where I move the mean. 

That's going to be important. I'm going to decompose it using SVD, so I have all the properties from before, orthonormal, orthonormal, and this weird diagonal structure up here. Okay. 

That also means the VT now, now transpose it looks like this. Okay. All right. So you remember that we had this eigenvalue problem where on the left hand side we had exactly this here. 

So of course, of course, now we have all the, we have all the m eigenvectors we can get from this matrix here. So I'm going to pick the ith one. So you can think about the first one if you want. Okay. So I'm just going to plug in a specific V here and then I'm going to see what happens. 

So I know that I can decompose these matrices as this matrix as this. So I'm just going to plug this stuff in for both of these. That's all I've done here. Nothing more plugging in stuff. Okay. Now I'm going to exploit what I know about how to do transpose of a matrix. 

So this is why you need to know this stuff. If you want to be able to do this fast, it turns out that I can sort of, you know that the product of A and B transport the whole thing is B transpose A transpose. Okay. So if I consider this one A and these two B and then take the transpose of this one, move it to the other side and then you get transpose of U here. So that's what's happened here. Only thing I changed are these three things here. 

Now I'm going to look at what UG us. It's an orthonormal basis. So I get what? Huh? Is it five or is it something else? I get the identity. Someone said it. Yeah, I get the identity here. 

Identity matrix does nothing when applied to whatever is here. So I remove it. Okay. So now I'm back down here. Okay. Now I need to think a little bit. So here there are very strategies here. 

So what happens here? So if I take V of T, remember I have it up here and I take VI. So I apply V of T to VI. So I have a vector here if you want. 

Then it's basically going to pick out that one element, the ith element here. So I take, let's actually do it. I think. Because it's so I have an VI. Let's say it's actually V2 just for the fun of it. So I have V2 here. It's a, it's a, oops, that was not very careful. So I have V2 as a column vector. 

Okay. And I'm going to apply to a matrix vector product here between this one and this vector here. Remember that V2 is orthogonal to all the other ones, except it's of course identical to itself. So it means when I take V1 times this vector, I'm going to get zero. So I get zero up here. V2, in a product with V2, I get, what do I get? 

V1, yes. And I get zero down here. And I'm going to write that as sort of a unit vector E here. So that's all that happened here. I just actually carefully looked at the matrix and looked at the properties of it. So now I get E. Now it gets a little bit tricky here. 

Well, nothing too complicated. But here I'm going to take this matrix. I'm going to flip it. So I'm going to transpose it. I'm going to multiply it with this stuff here. 

So what I get here for this thing here is an M by M matrix where I, in the diagonal, have these guys, the singular value squared. Okay. But then I pick out this E, EI thing here. So actually, when I then multiply this EI onto that M by M, I only get this sigma value here squared. So the sigma value corresponding to, in our example down here, the second one squared times that VI. 

So V2, for example. What have we learned other than manipulating matrices and vectors as one? Then we've learned that, and useful, by the way, I should say, then we've learned that if we match it to our eigenvector problem, if we look at what's over here, we had that this was, this thing here, we call it S hat y. But that's the matrix we are going to take our eigenvectors and eigenvalues of. 

Then this special vector here, the eigenvector, it's simply a scalar, this part here, times the same vector. So we have identified that lambda is equal to this part here. And lambda from before was the variance along the direction. So now we know how the singular values relate to the variance. 

That was the only purpose of this exercise, is to figure out how does this value here, the stuff we get from the SVD, how does that relate to the variance lambda? Make sure you try that out again. You can try it out in Metallab or Python and stuff. And see that if you actually do the eigen decomposition, or you find the eigenvalues and eigenvectors, sorry, of the covariance matrix, and then you look at the eigenvalues that you know that corresponds to the variance, then you check, you do the SVD on that data matrix. And you check that if you actually take the singular value squared and divide by n minus one, you get the same value. 

So it's just a relation between doing a basic eigenvector, eigenvalue solution towards doing the SVD here. All right. And then I would suggest that just to get some intuition about what happens here, this is an aside, so there's no note on that, even in the book, I think. But I said you can decompose any matrix here. So I took the data matrix. It's not symmetric or square or anything. But what if I actually took this guy here and put it in here? 

What would happen? I think, come back and tell me that next week. You would also get the eigenvalues and eigenvectors. So anyway, it's all connected. So try it out and make sure you get the intuition about this. 

Actually, I think I want to make a point of that while we're on it. Because if you think about an example here, and now I'm playing the game that I have actually removed the mean from the data set. So if I play the game here and plot some data points, which I might want to do in an exam or whatever it is. Okay, so I've got x1 and I've got x2. 

Which direction, that means which line is the one I would find if I do this correctly. I'm sure you can figure that out. Think about it just for two seconds. Think about a line on which you want to predict. 

And then you see if the line I'm going to draw now. Was that the correct one? No. Good. 

It's going to be this one, roughly. So this is the sort of line, right? And it's that direction is defined by the V eigenvector or principal component direction. That goes from zero up here with a length of one. So it's going to have a length of one that vector. It's going to be in that V matrix you get from the SVD. Or if you do the eigenvalue sort of root, then it's going to be an eigenvector. It's still an eigenvector. 

Okay, so what was my point here? Yes, if I now do the empirical covariance of this one, I'm just going to eyeball it and see sort of roughly what it is. Assuming I have roughly sort of one here and one there. What would that look like? So now I'm going to use this symbol here again. 

You could also use sigma hat if you wanted. So I know it's going to be a two by two matrix. Variance roughly one each or something like that. So I'm just going to put one point one. 

I don't know what it is, right? But it could be something like this. It's the same roughly along x2. So maybe one comma zero or something. Then there's a covariance. 

And my guess here is you can check is that it's around point nine and point nine here. So if I give you the covariance matrix and you look at this, then you can sort of imagine what the data would look like. That's going to be this trend and you know the sort of it's going to be sort of a positive direction. But how can you exactly find that direction along which the data lies? That means the axis essentially. What you could do is take the eigenvectors here, of course, and it all comes back to that. 

If you have this one, you can take the eigenvectors of this matrix and you can actually find that direction. That's just an aside. But it's all connected. Anyway, I've talked about PCA, but we haven't actually sort of outlined it in a recipe thing. So I'll do that here and then we're going to see how it actually works on some real data. 

That's going to be a few more things, to be honest. Sorry. Okay, so the first step and this is a really old method, really, really old. So you can think that this is new and brand new and machine learning and all of that. But this is from statistics over 100 years ago. 

Well, Carl Pearson, the guy with the Pearson correlation, he actually thought of this. So what you have to do to do PCA here and we have talked about the individual parts. You subtract the columnized mean from your data matrix. You've done it a couple of times now, such that the individual data points is the data point minus that mean vector. You can also do nominal data like a categorical one, but then you have to do the one out of K encoding if you want to do that. 

And also just talk to your CA before you do it. Okay, anyway, once we have that, we have the center data matrix. We do the SVD. We know that in V of T, we have as columns. We have the eigenvectors of the principal component directions. 

They have length one, they're all the normal. All of that still applies. We can select K of them. So remember, if I choose all of them, I get a perfect decomposition of my matrix. 

But if I am in 3D and only want to project onto 2D, I have to choose only two of them. Some K minus less than M typically. Okay, now there's a bunch of optional things here. And we talked mainly about the optional thing that's shown as 5 here. And that is project onto a subspace. You choose how big that subspace should be. 

If you're in 3D, should it be a line or should it be 2D or should it be 3D? You can try to do that. That's actually fairly fun as well and brings you some insight if you simply plot it in that new basis. Because once we're rotation of your 3D system. Okay, but we could also analyze to which degree the individual attributes, as it contributes to the dominant eigenvector. Because that tells us in combination which two of the attributes, for example two, actually contribute to the most dominant direction, meaning the one along which the variance is sized. 

So we are going to look at that as well. We can also reconstruct any point in that new basis in the original basis. So if I project, and we're going to see exactly that example, if you project from a 784 dimensional image space to 2D, can you then navigate in that 2D space and come back to an image space? 

Yes. You can simply reconstruct it using this formula here. So it's going from the B space, so to speak, that's the low dimensional space. Using, of course, the basis you have and then you'll get back into the reconstructed space. And then you need to remember to add the mean if you want to get back in the non-centered system. Oh, sorry, space. 

All right. So I think we talked about this, what's really the role here of sigma? We said it's essentially proportional, well, if you squared, then it's proportional to the variance along that direction, along the vector in V. So can we then use this to somehow quantify how much information is captured by the individual eigenvectors or principal component directions? And it turns out, yes, of course, otherwise I wouldn't have included it. So of course you can. So again, I'm going to spare you all the linear algebra manipulations over here. So forget about what's over here. It's useful if you really want to know about it, but let's see if we can get by with this stuff over here. 

So let me just show you the whole slide, I think. So I want to explain now how much do the individual components contribute to the variance, the overall variance. So you can think about in terms of percent, how much does the first principal component direction contribute compared to the second one. And it obviously needs to add up to 100. So what I'm going to do here is do an SVD as usual. 

I choose K components, I don't know, maybe three or whatever it is. And then we can measure how much of the variance is retained by projecting our, let's say, seven-on-one dimensional down to 3D. And in terms of the Fabinius norm, I can write it like this. So this is the reconstructed in the 3D, three-dimensional space versus the original data matrix. So that turns out that what we need to know now is that the variance we have retained is simply the sum of the squared eigen, sorry, a singular value, sorry, now I almost got in trouble, the sum of the squared singular values of the K components I've chosen to retain versus all of them. So it's hopefully clear that if you take a seven-on-one dimensional space, project it down to 3D, you're going to lose information or variance. 

That variance can be zero if everything else is noise or basically not informative, but that rarely happens. But so that was for K components. And the important thing you may want to just intuitively think about, what about one component? If I choose component one or two or three, so only project down to a line, how much variance do I retain? And that's captured by this expression where we take the, what's it called, the singular value squared for that particular component in the sigma matrix and then take divided by the sum of all the squared singular values. So that tells us how much of the variance have we actually retained in the whole system. All right, you get to do this now specifically think about why, how much variance you have actually retained if you, what is actually the question? Yes, so of course you do an SVD like usual, so you do this. And you do that and then you're informed about some of the elements in sigma. You're also informed about this one. Do you remember what it is? It's basically written here, I would say. 

So if you remember that this thing here is simply the sum of all the, the singular value squared. So let's write that. And then try to go ahead with this. This is sort of a prototypical exam questions that pops up in combinations with others or something like this. 

So this is something where, yeah, I need to be able to identify what's actually in these matrices and how do I get it out. So spend a few minutes on this and then we'll, we'll move on. So let's say five minutes actually. All right. All right. Who thinks it's C? 

Who thinks it's something else? Of course, now you've been warned. It's C. Okay. What do you have to do here? So of course it's a little bit tedious and you have to actually divide by things and all of that. 

So if you take the correct answer first, so let me cross that one. If you try to do that and I didn't do the computations here, so I'll rely on my notes. So what we have to do, we have to see that we need to look at the first three PCA components account for less than 70%. 

Okay. The first three, let's sum all the squared singular values for those three components. So we look at, oops, we look at this one squared plus this one squared plus this one squared. 

And not the fourth one. And then we take the fact that we have been informed about what's actually the sum of all the squared singular values. That's what we need to divide by. So we get a sort of a small computation that's 40. Oops, I need to be careful here. 

Give me a bigger screen. 40 squared plus 33.2 squared plus 28 plus one squared divided by 5,780. I think that's 67-ish percent or something like that, if I remember correctly. So if you try to do that with the other ones, then hopefully you'll see that they're wrong. I'm not going to spend time here doing it, but try to plug that in. 

So it's about matching basically what we know to this equation here or this one here. All right. Let's move on a little bit and actually see how we can use this for anything useful. Before moving on, I promised one of your colleagues down here to say a little bit about how representative are these questions and maybe the questions in the book for the exam. And they are to some extent very representative because they are old exam questions. They tend to be also on the easier or the medium difficulty of the questions you can expect. It means you're going to see something of that difficulty, but you're also going to see some that are harder than the ones we proposed as homework. 

So if you want to make sure that you have covered the whole span down here, then also look at the old exam questions. Then you can see what's the variability here. And there is great variability in that every year we are very proud that half the questions are actually new, or they used to be at least. That means we come up with new questions that you can't pattern match with what you've seen before. 

Of course, it's now no age, so that means that there might be a little bit more sort of overlap. But think about it in terms of the rough rule of thumb. Half the questions are something where you actually have to know what to think about how to problem solve to actually get the right answer. 

Did that answer the question? You will get the solutions to the old exam questions as well, so you can go in and look at those as well before the exam. Think about the questions also here in the lecture. They are easy, medium type questions because we don't want to spend 20 minutes for you to have to think about it. 

I will put in some harder ones as we go along. Alright, what can we actually use PCA for? I've used the Iris dataset as an example so far. You remember that by now and you're already tired of it, so you can imagine my view on this dataset. We have flowers, botanists measuring things about the flower, and we've collected that into a table, into a matrix. 

And you need to remember that we have the three classes, and we've seen those already today. Typically we color code the points by classes, but we don't use the classes per se to find the directions or find the projections. So what we are wanting to do here in this slightly warped figure is that we are looking for, actually I'll show you this one first and then we'll come back to the other one. What I'm interested here is that one of these optional things we could do with PCA, we want to project the data onto something. So here I took basically the three attributes that we've looked at so far, and if we actually plot those in 3D we saw that at the very beginning. That could be a little bit confusing. Now I'm interested in plotting them in 2D, but retaining as much information as possible. So I'm interested in a 3D space to find a 2D hyperplane and then plot the data points in that 2D space. 

What do I do? Well, I do follow the recipe. I subtract the mean from my data matrix to get tilde or x tilde. I do the SVD. I know that my eigenvectors or principal component directions are in V, and then I project my data matrix onto V1. That means I get the coordinate, the scalar coordinate of all my data points along V1 or the principal component 1. So for each data point I get a coordinate here. I do the same for the second coordinate. I project my data onto the second column in V, and then I get a coordinate here and I plot them against each other. 

And then I see something. I see if you remember back, the separation is actually a little bit higher here than it was when we just plotted the raw data against each other. So we found a hyperplane that captures a little bit more variance here than just removing one of the attributes. Okay, but what are these V vectors? What defines this direction? 

And of course we know that because it's in V. But if we actually look at it here, we see that the first eigenvector of principal component, sometimes called principal, that defines the principal component direction, it sort of seems to be a lot of the first one here. So if I have a very high value, it means a very high positive value of length, and you would imagine that would always be positive. Remember we have actually standardized it. So the mean of length is now zero and you will have, it's not length, it's a standardized length. So some of them, the smallest one will be negative. 

So if you have a very high value of length, so it's very high positive, when I then put that onto this one, I need to multiply, right? So let me actually do that. Let me do this one here. So I take something that is an X value that is very high positive after being standardized or centered. I'll write that as a plus. So that means high plus. 

Okay, then I consider, let's say the corresponding value in the vector I need to take the inner product with. So that's minus 0.39. It came from here. So I need to multiply that value by what I have up here. So a high positive value times 0.39. Okay, what about the next value? Let's just assume I have a very high positive one again. I've measured that for that flower. Okay, but now I'm multiplying by something that's not really that high. So this particular measurement of that flower is not going to contribute a lot to buy B1 coordinate. 

Okay, what about the third one or the final one? So this is minus again, 0.92. Let's imagine that I again have a very high value here. 

So I had, what was it here? 0.09 and I have minus 0.92. And I need to do the inner product between these two, right? So I need to take a very high value times this one, very high value times this one, very high positive value. Times this one and see what I get. 

What do I get? I get a very high negative value for B1. And that came about because this one up here was relatively high and this one was also relatively high, at least compared to this one. So my B1 is not influenced, hand waving here is not influenced a lot by this particular value here, which relates to the C-pull width. So that means essentially my B1 direction is only defined by only, let's say, sensitive what's going on in terms of C-pull length and petal length, the first and the third one. Not so much the width one because that value is very low. 

So when you actually do this inner product, you can sort of figure out what happens. So this is going to be, let's say, very negative. So I'm going to write negative here as sort of an indication that it's going to be very negative. 

Okay. What about the other way around? So of course, if I measured anything that would be very low relative, that means it's going to be negative. So I'm going to have a negative number times a negative number, negative number times a negative number, and then almost zero here. So then I'm going to get a positive number. So try to actually work this out by hand. I'm going to force you to do it in a minute. So just think about this. 

But at the end of the day, what have I achieved? I have achieved being able to plot my 3D data points in a 2D space. And it gives me a little bit more separation than I did in the original space where I just removed one of the attributes. Now we have to compare with that 3D plot I did at the very beginning. 

Why do we get more information? It's because instead of just projecting everything in 3D down to this plane, like we did in the beginning, I can now tilt that plane a little bit to actually find the best, let's say, angle of the plane, so to speak. And the plane is defined by whatever is in this V matrix here. Okay, we also talked about me being interested in knowing, but if I do this projection to 2D, how much information do I then lose? And how much information do I have if I only do it onto the first principal component? And that's what's on the previous slide. So I've done the SVD again and all of that. And now I'm looking at the elements in the sigma matrix. Remember, that's what defines how much variance is retained. So if I look at it here, I look at if I keep only one of the directions, that means I basically encode all of my flowers, even though they are 3D here, I actually encode them using only one number. How much information do I retain in terms of variance? 

Well, quite a lot it seems. If I now also include what I retain if I project onto 2D, I have almost all the variance in the whole system. And the third one, obviously, I keep everything. All right, so this kind of plot is something that you need to do basically to be able to understand. So now we've done the PCA on the IRIS data. And I would say, of course, we can sort of look into the vectors and what they mean and all of that. And I think the best way to actually understand that is that you actually sit down and do it by hand. 

We haven't made an explicit exercise about it, but please do this. So when you multiply in Python or whatever you use, when you multiply a data point onto one of these vectors, please try to do it by hand. Of course, if it's reasonably sort of possible, at least pick it apart and then do it manually, this vector, this vector. So what I've shown here is essentially just what do these vectors look like in the original 3D space. And you can sort of see that here, right? 

You have the green one, that's the one along which the most variance, the one orthogonal to that one is the one that retains the second most amount of variance. And that's what's shown here. And onto that plane, we projected things. And that's, of course, what's shown here again. 

And in here, you can also show the directions of the original vectors here to indicate which one of these is actually more dominant in defining that direction. All right. So I think I'll let you think about this one for a little bit. 

Actually, let me just think. Actually, I've now done a small example on the... I've done a small example now here, made up on the spot with a very small example. So I think, why don't you do it? I'll finish in five minutes. Then you go do it in the exercise session before the TA comes and they'll explain it to you. Okay, because it's a little bit tedious. But it's about thinking about... Let me show you. 

It's about thinking about what's actually the meaning of this. When I take the inner product with a vector defined by measurements of these attributes with this vector, and the value is very, very low. That means negative. Very high negative or very high positive. 

What actually happens here? And then there's one more thing you need to worry about, and that's an easy one to discard. It's whether or not these component directions are guaranteed to be orthogonal. I've told you many, many, many times that they are orthonormal, that also implies orthogonality. So they are perpendicular to each other. 

So you can rule this one out. Okay, so try to really go through these ones, and the TA will explain it to you. So the main point is I want to get through this sort of in a sensible speed here, because what can we actually use it for other than these sort of small toy datasets where we measure three or four things about something. 

And I've alluded to this image data, so we've seen that before. We take the handwritten digits. We sort of take the, let's say, the image sort of row by row or column by column. We concatenate them and put them into a vector. So in this case, these are 28 by 28, I think, and that's 700 something dimensions. So you have one row here would be 700, but it would have 700 values, essentially, encoding that image. And now we're interested in doing SVD on this, or some would call it getting the eigen digits here. So we do the usual stuff. 

You've seen that many times now. Remove the mean, do the SVD, figure out what's in here, project onto these vectors, and see what happens. So what actually happens? Well, if you look, if you project our 700 dimensional images down to 2D, that's what I've done here. So all of our images actually does not true. It's only the ones and zeros. They're now in 2D, so I have one coordinate here, one here. Now everything over here, they are zeros and everything over here are ones. So at least by doing that, taking from 700 dimensions down to 2, I can actually at least sort of exploratory, I can at least sort of discriminate between the two in two dimensions. I just want to emphasize, it's not the main reason we're doing it to hope to see that we can separate in lower dimensions, but it's both the fact that we can now visualize things easily, so now we can actually visualize ones and zeros in some space where we can understand it. And then we can try to understand what's actually driving these things here. So here we see that once we project, we sort of get this discrimination along this line here. 

We can discriminate between ones and zeros. That's a good thing if that's what you're interested in. But we also see something else. 

So here I plotted it again with some examples lying here. So if you look, what actually happens if you go right, we see this discrimination kind of aspect. Over here we have ones and over here we have zeros. So what happens here? 

Something's happening. At least if you sort of hand-wave a little bit. Maybe the tilt of the letter changes a little bit. So that means when you go from here up to here, the tilt of the letter changes. So you can identify these different patterns by doing an analysis like this. 

That's a slightly different way of looking at it. We can also use it for something else and I'll show you an example more in a bit. But I projected stuff that lives in 700 dimensions. So that means you have a camera that takes pixels or images with 28 by 28 pixels. I projected that down to two. So now I have two numbers representing that image. How good is that? 

How much information have I actually lost? So I can look into my space here. So that's B. So remember that to get back into my X space, then actually the centered one, I need to do B of K times B. 

And then I need to add the mean that I removed. So the B here would be a sort of a two-dimensional thing. It would be a two by one. 

That's wrong. It would be a one by two. One by two. And this V of K would be a what would it be? It would be a 784, I think, times two. So when I take my B, where is it? If I take my B coordinates for this point here, squeeze it through this equation here. That means I use the basis again. And then project back into image space. 

And try to actually show that data point as an image. I get what's down here. So over here you can sort of see, actually, we do get something that resembles zero. I got that from just two numbers. 

So I've compressed, in some sense, my image down to two numbers and back again. And of course there are issues right here. So in here I get something that seems to be in between. But I'll show you what happens if you actually now use, if you now actually use more than the two components, but you use two, five, 20, eight, 50, and 100. What then happens? 

So these are the original images. I do the S3D, project all the images down to 2D, or 5D, or 20D, or 50D, or 100D. What happens? Well, it turns out for 2D we've already seen that. You get something that's slightly blurry and not really that precise. But when I use 100 components, that means instead of 784 numbers, I use 100. Then I get something that, I don't know, sensible. 

If you were to communicate this via some communication channel or save it on your hard drive because you're running out of space or whatever it is, maybe saving 100 numbers instead of 700 is a good idea, or useful in some cases. Again, it's not the main purpose. It's the main purpose. But the main purpose in this course is to do this exploratory thing here, where we take some high-dimensional, something-something, project it down to some lower-dimensional, let's say, space where we can visualize it and explore it and see, do we actually find some of these patterns? And you probably need to, well, you should do that for your own dataset for the project reports as well. So the final example I want to just bring up, it's very similar. So these are pictures of people, some of them famous, I believe. Yeah, slightly old dataset. But the idea here is to do exactly the same we just did for the images, sorry, for the digits. 

Can we find some of the eigen phases? I mean, can we actually encode these images into some low-dimensional space and see what happens, both for the exploratory part, but maybe also if we want to compress these images, you could do something like that. So here we've got color images. So just an example of what we talked about last time, we can transform this. 

So we can take the original, let's say, image, and we can look at the red channel first, and we put that into a vector, and then we concatenate it with the green one and the blue one. So now we're just living in a 22,000-dimensional space and plus a bit. Okay, if you think you can imagine 700, now it surely goes wrong with so many dimensions about what's going on here. So we do a data matrix, and now we have 1,000 by 22,000 data matrix. We remove the mean, we do the SVD, and we project onto 2D. What do we find? 

Well, this is what we would do first, right? You're not color coding your data points or sort of inspecting them based on anything, you're just plotting them. So you see any structure? Not really? 

I don't, unless you're into reading tea leaves or something. But there's really no structure, it's just a blob of points. But if you now actually look at some of the data points, remember they correspond to individual images, these data points, or these points in 2D, and then try to reconstruct those and see what happens. 

It's a little bit difficult to see here, but essentially we are just reconstructing some of the data points here and plotting them. So let me show you, so maybe we can see a little bit that, I don't know, if you look at the skin tone or color, maybe there's a little bit of structure there. And of course remember here, I haven't actually got an attribute called skin color or something, it emerges due to this variance aspect. 

So maybe skin color a little bit up here, I don't know, maybe there's a little bit about female, male sort of appearance and stuff like that. So it's a little bit difficult to see, but we could do something else, because we can systematically take, oh, so let me just, we can systematically take points throughout the space, reconstruct those images, remember we can navigate in the 2D space and reconstruct, then what happens? Now I'll give you a few minutes just to arrive all this and see, can you see something happening? So remember this is PC1, this is PC2. Okay, oh, sorry to interrupt you already, but I think what I've heard here, you're definitely on to something. In that there's definitely something about the skin color here, I would say, along this one. It's not entirely clear, it's the only thing happening along PC2. Then something along PC1, I don't know, I mean, you would have to look a little bit more careful here, but there's definitely something going on here in terms of the background. So both PC1 and PC2 are simply encode something about the background. So all of these aspects, this sort of insight is something that we obtain simply by doing a relatively simple PCA analysis in that the main variance in our dataset is first of all due to the background and maybe the skin color. Okay, maybe there's a little bit about gender if you look at it very closely and squint your eyes about male versus female here, there's a direction and something. But remember, we found this by exploratory analysis. And we can explain what the variance in our dataset, where I thought originates, sorry, in terms of the original pixels by doing something like this. So these were just the original images overlaid here. Good, let me see. So I've already covered that one. 

Then the second last slide here. So PCA here is sort of an example where we take a data driven approach to finding, let's say, new features in the sense that these Vs or these eigenvectors that we found, namely the directions, they're made up as linear combinations of what we have actually observed. But there are also others where you can think about it in exactly the same way. Some of you would come from electronic engineering or even math or something like that. But we can think about this basis. We can actually put in a Fourier basis into matrix form and do Fourier analysis that way, or discrete Fourier analysis that way. So the fast Fourier transform can be formulated in the following way. And then you simply project your data, in this case time series, onto that basis. So a very neat way of thinking about, let's say, doing these kind of things, if you're not sort of into the signal processing, let's say, terminology and all of that, could be to think about this simply as matrix multiplications. And the idea of having a basis. 

In one case, it's just a standard basis, and then you could do a frequency-based basis. All right, so let me just summarize here the important things, because there are a lot of things going on. You need to remember this slide where we talked about PCA. You need to remember the elements of those matrices we talked about. That means you need to be able to identify these and know the properties of it. A popular exam question could be related to the elements in a vector. And then maybe you have to figure out, do a small inner product and say, is it actually zero or not zero? 

What's wrong with this, or these kind of things? And to do that, you need to know the structure of this decomposition and how it relates to what we, when we talk about variance. So remember, variance, singular value squared divided by n minus one, these kind of things. And you also need to remember the, let's say, the explained variance equation. So that means how much does one component contribute to the overall variance? And the idea about projecting down to subspaces. 

So the projection equations, essentially, be able to do that without using a computer, looking it up on Wikipedia, whatever, is actually being able to operate this and apply it. Then I would say, I just want to finish one thing, because it's important, sometimes people go away and say, this PGA, well, I mean, what can I use it for? And I think hopefully I've tried to give you some intuition here. Maybe not what I can use it for, but when should I use it? So should you use it for your projects? 

Yes, because we asked you to do it. And this is mainly for the exploratory and visualization aspect to see, can you find sort of, let's say, a 2D subspace where whatever data that you're working on can be the classification problem where you can discriminate in that 2D subspace. Maybe you're now talking about a very easy problem, because simply by doing a projection, let's say, from high dimensions to 2D, okay, you can then just solve the problem by a simple straight line. You don't necessarily need to use a very complicated method. And these are some of the insights that you need to take away from this. 

Think about it in terms of visualization and exploratory analysis, originating in the need to do dimension-altim reduction. That's all next time. As I said, we're going to do probability. So read that and then see you next time. Thank you. 
All right. 

Speaker 1: Okay. So as you have figured out, can you hear me in the back up there on the cheap seats? Yes, in the back. Okay. Good. As you figured out, there's something with the lights in here today. So I can't do anything about the brightness of the screens. I have asked for two years about this. It's not possible. 

Apparently at a technical university. Okay. What I can do is recompile the slides with a slightly dodgy background. So that's what I've done here. So it's a little bit better than what it would have been had we not done that. 

It's not perfect. I will go back and ask the AV guys again. But hopefully if it really starts to be a problem, then you start blinking and crying or whatever, then just let me know we'll have a break or something for a few minutes. And I can make these slides even darker if we need to. Okay. 

Good. The red screen of hell here. So the title, we are in week four of introduction to machine learning and data mining. And today we're going to talk about everyone's favorite topic, namely probability and probability density functions and distributions. And trust me, this is really, really interesting. 

And it's also really, really important. So what I'll give you today is an overview of these things. And I'll highlight a few learning objectives. And then, yeah, by all means, make sure you read the, oops, make sure you read the chapter and think a little bit about this. And then also test at the end of the day or when you get around to actually reviewing the material, make sure you make sure you also attempt the problems here, the homework problems. And you can do that during the TA sessions later today. 

Okay. So, yeah, remember the feedback groups. So I have actually read through what you wrote the last couple of weeks. And I'll come back to that in a minute. But again, as I said, this is the last lecture sort of in the, let's say, warm up phase where we review something that we do expect most of you to know. We do know that some of you haven't had a lot of experience in actually doing this. And then for those of you who do know it already and experts in probability theory, if you, if that's a thing, if you can find someone who is that, then hopefully I'll present a slightly different view than what you've actually been used to before. Hopefully they'll put things in context and I'll show you how we can use it in machine learning. And that should be the takeaway here. 

Good. Next week, I think it's still my turn to talk about the first sort of thing about supervised learning that we're going to talk about. So classifiers, decision trees, and linear regression, specifically linear models. This is where the linear algebra and some of the stuff we talk about today come into play. 

All right. So there's a lot of things going on today. So basically, it's sort of a combination of two lectures where we split half and half, along with some other stuff in the other lectures, obviously. So it's the first time we sort of try to give you all of this in one go. So my expectation, and we'll come back to what expectation means here, because that's important in probability. So my expectation is that we might go a little bit beyond time, but let's see how it goes. Okay. 

Good. So that's a lot of sort of, let's say, semi, new learning objectives here. So there's something about understanding the basics of probability theory and stochastic variables, random variables, and we'll give you our perspective on that. 

And hopefully that triggers something where you want to learn more about this and then go into this. Then there's something specifically about, let's say, discrete random variables, understanding basic stuff such as expectations, which I just mentioned, independence, and then a specific distribution or probability mass function, namely the Bernoulli distribution here. And then we're going to understand how we can actually use this in a machine learning context. Why is it even relevant to talk about this in a machine learning context? And we're going to come back to very fundamental principle called maximum likelihood. We are going to talk about a full blown Bayesian view on machine learning, and then reduce that a little bit again to something called the maximum epistorio estimate. 

So this is all about learning. And you can say everything we're going to do from now on falls under that umbrella. But of course, we're not going to cover everything that you can possibly think of within the Bayesian framework. But I'll come back to that and explain what you need to focus on. Good. And what you need to focus on is specifically the last one. 

How can we actually use the notion of, let's say, Bayes theorem to derive so-called cost functions that we can operate with, even without necessarily going full blown Bayesian statistics? Okay. Good. 

So this is the plan. So please do remember when I say something like approximate here, it means approximate. I have not given this specific lecture before. So in reality, I don't know. I have an expectation about why that it might take a little bit more than two hours. But again, there's some uncertainty about the length of this lecture. Okay. Think about that. How big do you think my uncertainty is? 

So I've given you some context. Is it like going to be three hours or maybe one and a half? Probably not given that. I think it's a little bit longer than usual. So already here, we need to think about uncertainty. And we need to think about expectations. Just for me putting in this little thing here. And of course, that comes down to everyday probabilities. And we're going to talk a little bit about various kinds of everyday probabilities. 

Good. And then of course, the exercise session here where you can talk to the TA about concepts you haven't understood. You can do the homework problems and you can do the exercises. All right. So here's a lot of stuff here and one in particular, which we have tried to address a little bit. But some of you have already brought this up and I have already emailed people in the right places. There's nothing I can do. But I'm going to keep, let's say, chasing the DAV guys and the building guys to actually help with this because it's annoying for you that I have to spend my time on it. 

It's annoying for you because you get blurry eyes and can't concentrate. All right. Most important one is that of course, there's a project deadline on the 6th of March that you hopefully you already make progress on that. So remember to submit that. 

I believe that's a Thursday at one o'clock. Yeah, go ahead. I have just done that. 

So right now it's at 40%. So the suggestion here is to change the background. I have done that. I can, in the break, I can do it. 

I can put it even more grayish. Yeah. Okay. We can do it in a minute if you want to. But I'll do the first bit and then at some point you and I need a break and then I'll change it. Good. All right. Then there was some comments about ad hoc notes not being reviewable. 

They are reviewable. I upload the presentation slides sometimes. Well, usually right after the lecture, but you can actually see the scribbles I make on the slides. 

So you can see those. Then an important thing here, namely, what are the important questions or equations? Sorry. So I have so far not sort of explicitly summarized that at the end of the lecture. 

And I'm maybe not going to do it today either. But when we move forward in terms of the specific machine learning methods, we will highlight specifically at the end of the lecture, what is it, the specific equations you need to know. But there's also an element here of learning and taking responsibility for learning in that you actually reflect on what are the important equations that you need in your notes. 

And that is something that comes from actually doing the exercises, doing the homework problems. Okay. So if this keeps being a problem for a majority of you, then we'll put some effort into maybe highlighting that a little bit more. Good. Then what else? Nothing new here. I'll have some news next week. And then no printed notes, usually. So nothing new there. 

Good. Then on to why we need probabilities. I've already alluded to the fact that you may actually have some uncertainty about the length of this lecture. 

Okay. That's a good reason for you to think and reflect a little bit about probabilities. Because probabilities is a way to deal with uncertainty and incomplete information in the world. And it's a coherent and consistent framework for doing that. There are other frameworks. Some of you might have heard about posse logic and all of that stuff. We are not going to go down that road in this course because right now what seems to work is probability. 

So that's what we're going to do. And it provides a language with some very simple rules that when we manipulate and apply them becomes really helpful in for the machine learning business. Then there's a few other reasons why we may want to think about probability. So you think about a sort of a binary classification task and we've already looked at these flowers, the iris flowers. They are sort of a discrete event and they can be zero or one belonging to a category or not, for example. So of course that distinction, do we belong to something or not? That's binary. 

Yes or no. But the probability that you belong to something that's continuous. Optimizing something that's continuous is inherently, oh well, maybe not inherently, but it's typically easier than optimizing something that's discrete. So there are some algorithmic reasons for why we also want to think about probability. 

Okay, then there's convenience and I'm not entirely sure I want to call it convenience. But anyway, there are some principles that sort of falls out of the probabilistic framework where we can actually learn or derive learning principles in the machine learning business. So how do we learn from data? And as I've already alluded to, there's a very important one called maximum likelihood and we are going to talk about that today. 

So think about this, how do you deal with uncertainty on your labels? If you go out into the world and miss your stuff, the length of this lecture, of course it only happens once, hopefully. So how long is that going to be? Is there any uncertainty about it? Is there any noise on that measurement you make? Is your watch broken? Do you have noise on your sensors? So uncertainty is really key to a lot of things in understanding and maybe even more importantly operating in the real world and, of course, learning from data originating from the real world. So here you see the difference between the white background and the gray background. 

I'll make it even more distinct in a bit. So I've already said why we need to think about probabilities and there are potentially, historically, two different schools of thought here and you have probably, or I would guess at least, that the most of you think about probabilities sort of coming out of a series of experiments, possibly infinite repeated events and something like that. So when you assign the probabilities to something, it's because you've seen a coin being flipped a million times and half of it, roughly, comes up, hits, for example. So there's this notion of having this repeated experiments and that's what's called the Frequentist view on probability. Now there's also another school of probability called the Laplace or the Bayesian view and it's slightly different. There's no notion of, let's say, repeated events necessarily. 

We can have that but it's not a requirement to talk about it. So if you just think about naturally that if you have only observed something once in the world, I mean, one assign probabilities to some event, I don't know what can you think of some crazy, let's say, president in the US or Russia or whatever, they make the civilization end. Hopefully that only happened once so there's certainly no notion of repeated events here but you can still assign belief in the event, meaning how probable is it that this thing happens. So that has nothing to do with sort of having to repeat this to actually assign what we call degree of belief in an event and that's the view we're going to take here and that's perhaps slightly different from what you've been presented with before in that typically in a statistics course you're said, okay, here's a series of events and now we count how many times it was one and how many times it was zero and now we assign a probability based on that. 

So basically you count how many ones do you have over the total number of outcomes. That's a pure, frequentist view. Now, as I said, what we're going to assume here is that we can assign degree of belief to a so-called binary proposition that means it can be true or false, given that we already know that B is accepted as true. So that means given that you know that B is true, Trump has been elected president, what's the probability that he's going to end civilization in some way? 

It's not good that I'm recording this, but anyway, so you can do that, right? This number is between zero and one and it's probably not zero to be honest. It's not one either, hopefully. 

Let's hope at least. Okay, so what we have here is some way of assigning degree of belief to something and the way we're going to start out here is to say that their binary propositions, as I already alluded to, and they represent the state of knowledge. They represent your degree of belief in something happening in the world. Okay, so a few more examples here and how we can combine, let's say, binary propositions to actually say something useful. So let's say, for example, you're in the courtroom and you have a binary proposition G here. It says the accused is guilty. That could be true or false. Okay, then you've seen some evidence. These are also binary propositions. You say a car similar to his was seen at the crime scene. We assume that to be true, if nothing else is stated here. 

And then we have E2. It says a large sum of money was found in his position, also true. Okay, and his fingers print were found at the door of the bank. 

Apparently, he brought the bank. So we can combine this into propositions and we can evaluate the probability for these binary propositions and compare them. So in this case down here, we have combined the binary propositions, so E2, E3, and E4. So all of them being true, we know that. That's called E here. 

So we can probably say that, let's say, the probability of G being true, given E, that means having seen all of these or knowing that these are true, is probably higher than this one here where we have only observed evidence. No, but two. Okay, so these are ways of talking about binary propositions and using them for combining them to say something sensible. Okay, so a little bit more about this. So you have to bear with me here because we're going to build on this binary proposition view until we get to something that's called a random variable, stochastic variable, which is probably where you started before. 

So we're just going to build it up. So, I mean, these are just examples of other things that you can sort of formulate a binary proposition around. So maybe if you look at B here, it's something about the sensor. 

It says, okay, I have an acceleration sensor. My binary proposition is now that it's higher than something, a particular value. It could also be lower, obviously. It could be equal to a specific, I don't know, value or something. 

We'll come back to that one. So all of these things can be used to say something about the world. Now, I'm sure all of you have no logic and you've seen logic somewhere in your education. So what applies here is basically basic logic rules. So we can combine them and say, A and B are, what's the, let's say, we have a binary proposition saying that A and B, that means both of them A and B, that's true if A and B are both true. If one of them is false, it gets false. 

We can also do the OR operation here. If either A or B are true, then we get a true statement here or evaluation of that binary proposition. We can negate them and say that the negation of A, it seemed to be true, is true if A is false. So we put this sign over it, on top of it, if we negate the logic statement here. 

All right, then we have sort of two things, and these are a little bit technicalities, but we need two more, and they are sort of boring ones. They are the logical one. It means a proposition that's always true, and this is a technicality that we'll use in a few sentences. Okay, and then we have one that is always false. 

So if you have a proposition that's always false, you live in a world where Trump wasn't the elected president, doesn't mean to talk about the probability of other things related to that. But anyway, if we have the following identities here based on this setup here, then we say that A, you can say times and, it would, the meaning here would be A and one, something that's always true, that's A. If we have A plus the negated A, that's one. If we have double negation of A, then we have, we are back to A again. Okay, there's also this distribution rule here that applies to as in logic, nothing new here. So A, sort of, if you have the proposition A, and then say, and with this whole proposition, which is made up of all statements, then we can distribute this A into this one, and we'll use that in a bit. So hopefully this is relatively clear. So now I'm just going to, well, have you actually think about this? 

How can we make binary proposition about the world based on these logical statements here? And in the meantime, I'll try to compile some slides, which are less bright. Okay, so have a think about this for a few minutes, then I'll see if I can get this working over here. Go ahead. I should say that this was in a much, much harder version of the course, where you had to hand in three reports. Okay. 

All right. All right, as far as I can tell, most of you got the correct answer that this is C. And what you have to put together here, the difficult part, is not necessarily looking at the equations or a simple expression. It's actually reading what's going on here. So what we're looking for is the student hands in report one and two and three, all of them. What is then the chance of passing? That means not failing. So we're looking for the probability of failing negated. That must be pass, I guess. 

Okay. Conditioned, this is what this sign means, that you have observed this thing here to be true. So C is the correct one. Now all the other ones might be sensible and they can be evaluated. And then usually there's a question, is this really true? That if you hand in all the reports, the passing, let's say, fraction of students is 90 percent. It's not way off, but it's not entirely correct. 

So maybe it's maybe 80 percent or something like that. Okay. So anyway, I hope this makes sense. 

It's just to make you think about how do we actually put up binary propositions about something in the world. And we can combine them. We can add them together. We can order them. 

And we can even leave out some. So basically the question you want to ask now is what's the probability of not failing, given that you've handed in report one and report two? What is this? I guess that's for you. That's the important thing, because you don't have a report three. Okay. And that's what I said. It depends. But I would say around 80 percent. Okay. 

So good. Then there are some rules that we need to look at. And to some extent, this is everything you need to know from this lecture, plus a few more details. 

But this is something that if you don't know them, can't recall them, then you need to know this. And I should have put a big sort of box around this because this is the fundamental of, let's say, probability. So aside from the fact that we need some probability between one and sorry, zero and one, then these two rules are the important ones. So it says that the probability of A, given that we have observed C to be true, plus the probability of not A, negative A, given that C is true, should sum to one. That means, and this is important, everything on the right hand side here, sorry, if we sum across everything on the right hand side, so that would be a one, for example, and a two, or two or fours, should be one. 

Okay. That's one rule and that's the sum rule. And we're going to derive a particular useful rule from this one in a bit. Also using what's called the product rule. So the product rule says that if the probability of A and B given C is known to be true, we can factorize that as the probability of B given that A has been observed to be true, C is true, times the probability that the binary proposition A is true, given that C is true. 

Now there's another version of this. Let's see if I can do this correctly, but this is also equal to the probability of, I should write it down, sorry. So the probability of B given C, probability of B given C, like this. So now if you remember a little bit, so I'm going to jump a little bit here, but if you actually remember from before, if you had sort of, if you put C to one, like the logical one, then you probably get something you've seen before. And that is basically what's written down here in this box that you get A, sorry, the probability of B and A equals the probability of A given B times probability of B. I think most of you would have seen this. 

We've just included up here to make it a little bit more general. Now, so the interpretation of these probabilities is as follows and we've already alluded to that. This interpretation here is that if B is true, then if the probability of A is zero, then it means that we are absolutely sure that A is false, given that B has observed to be true. Okay, the probability is zero, so we're absolutely sure that A can never happen to be true if B is true. And the other way around here that if the probability one, then we're absolutely sure it happens. And that of course, using this slightly weird notation I had before, it also means that if you actually put in B being the logical one here, that simply amounts to the probability of A. 

These are slight technicalities, but we're going to use some of them in a bit. Then there's this sort of bold claim here that someone put in, I didn't put it in exactly, because I think there's more to this course than just probability. But when we're talking about the probabilistic view in this course, then essentially from these rules you can derive whatever you need in a pinch. Okay, so if you can't remember and haven't put them in your notes, everything else, all the other equations we're going to talk about in the next five slides, you can, and that's my claim, you can derive everything from what's on this slide. Right, so surely there must be a ring saying now maybe these are two equations that I need to know about. 

I'll have some slightly more operational ones in a minute. Okay, so these are the two rules we just talked about and I'm just going to write down the other version of the product rule that I just put down. See, okay, so being able to actually manipulate this in this way, like without thinking, is sort of very mechanistic in some sense. And if you know these rules to be true, then you can manipulate them and you should be able to do that. Okay, now I'm not saying you need to do this in an exam question or anything like that, but what we're going to do now is to derive what's called a modernization rule, this word up here. So we're going to say that if we have the probability of b given c, then we can do something interesting, namely that I can in a very complicated way, right? First of all, I just repeat what I just had. 

So what was this thing, the stuff b must be one. So in a very complicated way, I've written one here. Okay, it serves the purpose. 

So bear with me. Then we use this rule that we can distribute this one inside to this term here and this one here. Then I'm also going to exploit that the product rule to actually write this as follows. 

So now I get two expressions here, get one probability of a and b given c, probability of not a and b must be true given c. Okay, right? Well, what can I do with that? Well, now I can actually expand it. So if I use the rules we had up here, so I'm just going to plug instead of this, I'm going to plug this stuff in here, I get this and then also for the other term here, just instead of writing a, I actually put in the negated a. So now I have something I know to be true. 

And what have I achieved here? Well, here I have achieved something where on the left hand side have something that depends on a. I don't have that on the left hand side. So by marginalizing out a, I can go from a distribution where I had, so if you just write, maybe write this one, this expression here. 

Actually, we already have that. So this expression here, I have a and b given c over here at some, the other, let's say option, the other event outcome for a times b given c. So simply by summing these two, I can get rid of a. So I have marginalized out the binary proposition here a, okay, to get an expression that's given by this. So how is this useful? Well, if you are given something like this, but you in a minute, you need something like this, how can you get rid of a? Well, you just sum over the probability where, well, you consider the two different outcomes for a. Okay, that's called marginalization. That we need to remember. Now, there's another, well, unfortunately, I actually put in the name here on the slides. 

Otherwise, I usually ask about this. Who has seen something like this before where c is one? So let me write that version. So the probability of a given b, so I set c to one here. So c equals one, logical one. 

p of b given a, p of a, p of b given c, or given just p of b, sorry. Who's seen this before? Yeah? Space theorem. 

Who knows what this is? Just, it's okay if you're a little bit in doubt, you can put this other halfway up. Don't do like Elon or try something like that. But yeah. Okay, so most of you have seen it, but you're a little bit unsure, right? 

So the reason for this is that you probably haven't been, let's say, trying this out enough. So that's what we're going to do now. And you need to really make sure you'll get this. And I know it can take years to be really comfortable with this. And even I sometimes struggle a little bit with the iteration here. 

Okay? So it is tricky, but we have some rules that we can apply when the world of the uncertain world sort of plays a trick on us. Okay, anyway, this is essentially what I'm deriving down here. And the way I'm deriving it is by creating this version of the product rule with this one. So that's, I've just basically moved. 

Let's see what version I have. I have p of a given bc. That's this expression. So I just divided what I have up here with this stuff here. Now the problem is this stuff here, I might not have that available, right? 

Because you're sort of looking at something that contained a. So now down here, I'm actually going to exploit this thing here we derived up here, the modernization rule. Okay, I'm going to plug that in here. That's what we get here. And this is base rule or theorem. 

Okay, for binary propositions, we're going to generalize this to general random variables in a minute and even continuous ones. All right. So there will be some small exercises you need to do here in a minute. But before we get there, I just want to maybe do that in the context of stochastic variables and not binary proposition, but essentially comes down to the same thing as it turns out. Okay, a few more things to think about. We've been worried about binary propositions. And if you throw a, let's say a fair die here, so it can have six possible outcomes, can I put up binary propositions to describe that outcome? 

Yes. Well, you observe something about the world. I'm throwing a die. I can put up a binary proposition called a one says that sort of the side with one turns up on the top face up. Okay, it can be true or false. That's a binary proposition. There's also another binary proposition or you can think about an event here or atomic event even. 

This also, okay, sorry, it's actually correct. I thought it said two here, but if you have a three, then it's the binary proposition that this side is face up and so on so forth. So we have six unique binary propositions. 

Now we need to keep track of those. Okay, then we have saying that, well, if they are mutually exclusive, then they can't, these two propositions cannot be true at the same time. So you've probably also seen this before and you can try to derive that using the rules we already have actually and that's doable. Just says that the probability of A or B is the probability of each of them individual, then you need to account for the probability of them happening at the same time. But of course, if you have a mutually exclusive binary proposition, then this is going to be zero. These are technicalities you probably need to remember a little bit about. 

Okay, so what else do we have? Yes, okay, if you have mutually exclusive events, then this proposition here, namely the probability of each of them individually or the all proposition here simply the probability or summing all the probabilities individually. And you have probably seen this before. Then if you have so-called exhaustive events here that relates to the individual events, we can also sum this up here. It needs to sum to one. Okay, so that was a little bit technical, so don't worry too much about this, but think about what we had here when we had to describe this thing here using our notion of binary propositions. We have to write up six different ones to actually account for the outcome. 

We have to check six individual binary proposition to characterize what's going on here just from a simple die. Now you can imagine what it happens in the real world. Okay, so that's essentially what. So that brings us to a convenient, let's say, semantic trick. It's one way of looking at it where we're going to introduce what's called random variables. But given the binary proposition story I've been rambling about until now will allow us to think a little bit more, let's say, fundamentally about this. So let's say we measure the number of children that a person has. 

I have, okay? So zero, one, two, three, four, five, six, seven, eight, and if they're really busy, eight, nine, ten, eleven, and so on. But in principle they're going to have a lot of children. And if you remember back to the die example, well there we had six possible outcomes. Here you have, let's say, the maximum number of children we can have is 15, so zero, and then plus the 15, so 16 possible outcomes. So you need 16 binary propositions to actually characterize how many children a person has. Okay. So what we're going to do now is come up with a construction where we don't have to actually put up all of these binary propositions, but instead think of them as simply as constructing a variable that we can assign to the actual numerical value instead of having to check 16 different binary propositions. 

Okay? So the idea here is that we take what we had before, so you can think here x, I don't know, 14, that would indicate a binary proposition that a person has 14 children. And of course we have many of these. But we can also use another notation, so that's what we have here. So we can also use the notation I alluded to here, where we simply, I wouldn't say wave our hands because that's strictly not what we are doing, but we have now have a variable capital X that we assign to a certain value. So here I would have x equals to 14. That would be a different way of thinking about these 14 different binary propositions. 

Okay. So over here, we're talking about binary proposition. Over here, we're talking about stochastic variables or random variables. 

Okay. So over here, we talked about the probability of an individual binary proposition. How do we do that over here? Well, we talk about the probability of that variable equal to 0, 1, 2, 3, 4, 5, 6, up to 15. And then we're going to say, well, actually, that's just a, let's say, function in x. So often we use this shorthand here. And of course, it's annoying that we use capital P and lowercase p. And usually we would just expect you to actually get what we mean from the context. And that's unfortunately also the state of affairs in the literature. Okay. But anyway, we also had the conditional here. So if we look at the probability of x subscript x, that's the binary probability of this binary proposition, given that another binary proposition y is true. 

Okay. How would that look like over here in this stochastic variable setting? Well, if you look at a value of y, what can we say? A random variable describes something related to number of children. Let's say number of cars in the household. So if you have, I don't know, 15 children, you can ask, what's the probability of you having 15 children if you only have three cars? Oh, I guess you can fit 15 children. If you only have one car, okay, needs to be a very big car, basically a bus. But anyway, you can put up that statement and say, the probability of you having 15 children given one car, given that the random variable describing the number of cars is one. And again, we use a shorthand notation here to actually describe this. 

Okay. So over here, we derived from the sum, from the product rule, we sort of in a very quick way, we describe what's called the modernization rule. That you need to remember. 

We have the base theorem here. You definitely also need to remember that. But of course, in a pinch, in an exam, you can actually derive both of them from these two rules and your notion of the knowledge of So what happens over here? 

Okay, as it turns out, you can do exactly the same. So even if you talk about random variables or stochastic variables, we have exactly the same set of rules. It's just a slightly different notation here, where we don't have to keep track of all of these binary propositions to enumerate all the possible outcomes in the world. So in a nutshell, a random variable is actually a function from the set of atomic events. These could be the 123456 on the die to some other variable. The random variable for the number of children, for example, could also be what's the probability of you having an odd number or even number of children. The random variable could also describe that particular, let's say, observation. That could be true or false. 

But that's a mapping from actually the number of children or binary propositions to some other variable x. Okay, good. I hope this is clear and I hope you've seen that. 

I hope basically you've seen something like this before. And even though if you haven't, that has been the foundation of everything you've been doing in statistics. I hope that from actually forcing you to listen to me talking about binary proposition, you can now actually sort of make that gap and see what is a random variable, essentially. And a random variable, yes, in Python or in MadLab on R, you can define a variable, but now you then have to actually assign some probability to it. That means a probability mass function. 

That means that there's a certain probability to each possible outcome of each possible value of this x or probability density function if we are talking about continuous random variables, which we'll do after a few slides. Okay, now let me just take a sip of water here. Okay, so I'm going to put up two different, let's say, problems here. 

So first of all, we're going to do a deal with something that you think is, oh well, then be very careful, that typically is conceived as being easy in the sense that it relates very much to your frequentist view of the world. Okay, so if I ask you about what's the probability of an orange if the bowl is red? So if you're in this one, we can probably say what's the probability of being orange? Have to count how many do we have in total? One, two, three, four, five, six, seven. One, two, three, four, five out of these were oranges, so maybe that's easy enough. It's five over seven. What's the probability of the red bowl if the fruit is orange? That's a slightly more difficult question, but I think you can probably get it from just reasoning a little bit, and especially because it's so visual that you can sort of count everything. 

Okay, but let's try to actually do this using the rules we've just put up. So now I have a random variable. What would be the random variables here? It would be the fruit type, that's one random variable, it could be orange or apples. Then I have another one. 

Let me write this up. So I'm looking for a so-called joint distribution in the first instance elite. Fruit type, and then I want to ask, remember the way I went from the binary proposition sort of logic and to talking about random variables was simply to put a comma here that means fruit type and something else, the outcome of something else. And in this case, it's the color of the bowl. So I have two random variables now, and I want to figure out what's maybe I can observe this, which turns out to be true. And I want to make some inferences based on that. 

And you remember all the rules we had? We had the sum slash marginalization rule. That's where we can remove one of these variables simply by summing over the possible outcomes. So I can get the probability of a fruit type, the end, no color, simply by summing over the probabilities where color is red and blue in this case. 

I can also talk about the product rule, that means that this joint distribution here, I can factorize that. I know that to be true one way or the other. And I have base rule where I can so to speak invert. This is not entirely true. Where I can invert probabilities, this should not be there. 

Okay, where I can actually invert probabilities and say what's the probability of one of the variables giving the outcome of another observed. That's the joint. That's this one over the marginal here, as we call it. All right, let's see what happens here. So what's the probability of an orange if the bowl is red? 

So I have observed that the bowl is red. That's true. It's known to be true. 

Okay, so I put that on this side here of this conditioning sign. So I ask what's the probability of an orange? And if you carefully look before, it's sort of what this joint distribution here. 

And remember that this joint here, maybe I should have written that. It can be written as the probability of, let's say, a red bowl given an orange times the probability of an orange. That's another way to ask the same thing. But do we need that here? No, because what you're given is the probability of, well, so two out of a total of 12 were apples in the red bowl. So that would give you a probability of two out of 12. So we can evaluate this one here simply by looking at the numbers. 

This is where this frequency's view comes in. So I can now, what's the probability of a red bowl and an orange? And it's important word here. So that's, oh sorry, orange here. So we have five out of 12. That was what we have in the numerator. 

In the denominator, we have this marginal thing here, but that's conveniently easily available here. So what's the probability of a red bowl? Okay, well, that's simply seven, right? Seven out of the total of 12 is in the red bowl. So this is easy. I'm counting and dividing a little bit and I get these probabilities. 

And I get five over seven for that one. I think we did that on the previous slide manually, right? But what would be another way to actually get this, let's say this marginal, if I say, by using our marginalization rules? Well, all I have to do is sum over the two possible outcomes for the one variable I'm not interested in. 

I want to get rid of. So I say, what's the probability of a red bowl given orange is zero or not an orange? And the probability that it is an orange, okay? I need to sum those together and I get seven over 12. That's also what I had here. So that would be this number divided by this. I can keep manipulating all of this and let's maybe jump to the interesting one here. 

This is where I'm using Bayes theorem to get to this same thing. Let's not jump, actually. Let's do this one. What's the probability of a red bowl given that I have an orange? 

It's the joint distribution over, I'm just using the product rule here, essentially, this is the joint distribution of red bowl and orange. We had that from before, right? It was five over 12 in total. So red bowl, orange, count how many times did that happen out of a total of 12. So that was exactly the same we did up here, but now I need to divide by the probability of an orange. That's not the same it's not the same as dividing by the probability of being red here. 

All right. So we get the probability of being in a red bowl given that we have an orange. It's simply five divided by six in the end. 

What would another way to be here to do this? It would be to use Bayes theorem and then instead of looking at this conditional here, I'm simply looking at the, sorry, instead of looking at the joint distribution, that means these numbers here. I'm simply looking at the conditional ones. 

So I can do that. So the probability of a red bowl given that I'm an orange, well, that's also the same as the probability of an orange given red times the probability, sorry, red bowl times the probability of red bowl divided by the probability of an orange. All I'm doing here is just repeating myself and just looking at it from different perspectives. 

The real difference here is sometimes you're given the joint, sometimes you're given the conditional. So you need to be able to manipulate these. All right. Hopefully this is relatively easy. I hope it's something about counting and dividing by some numbers. Now I'm going to give you something that's maybe not as easy. So have a look at this for just two seconds. 

Don't worry about this for now. I'll show in detail how we manipulate this. I want you to think about this up here. 

Just for 20 seconds on your own. And then I'm going to ask you to close your eyes and say whether it's above 50% or below 50%. Just from the text. Okay. So I'm going to ask you to close your eyes just for two seconds. Do that now. It's just so you're not influenced by your neighbor. 

Close your eyes and put up your hand if you think that this probability of you having disease is higher than 50%. It's a few. Okay. Who thinks it's lower? 

There's a few and the rest of you don't care. Yeah. Okay. So of course there's a resemblance very much what's been going on a few years ago. If some of you remember the COVID stuff. 

So this is a little bit of a surprising result at least previous to COVID. Can anyone say exactly what it is without having worked out the math? I would find it difficult to say is this maybe 25% or is it 40% or 50? 

It is below 50 by the way. And that is maybe a little bit surprising because you went and took a test at the doctor. It told you you have the disease. Yeah. Sorry. That's wrong. It was positive. 

Okay. The question is whether or not it's the probability of the test being positive given that you have the disease. It's the same as the probability of you having disease. And of course the answer is no. So let's try to work this out maybe even in a little bit of detail. 

And I've spent half my weekend trying to colorize this equation. So I hope you appreciate it. Okay. Let's work out first. What do we need to identify? 

We need to work out what's the probability that you have the disease given that the test was positive. You're not looking. This is what we are asking you to do. It's not the other way around. That's the conflation that maybe people make that you're there. 

Okay. The probability of positive given the disease must be the same as this. That's not always true. 

And of course the tricky thing here is that we observe something about a test or it could also be symptoms which has a course, a root cause, which is that you have the disease. So let's try to work through this. So what we need is this one. So let's start here. And then we know something, namely that we are given that in the general population from which you assemble uniformly, there's a 1% chance that you have the disease. Now we know the sum rule. So we also know that not having the disease must be 99%. Then you know something about the test. It says that it only identifies the disease. That means it gives a positive result in 99% of the cases where you know that you have the disease. So that says something about this probability here, namely we have observed you have the disease, which we can't in general, but we can characterize the test of course. And then it gives a positive result. So the probability of positive given that you have the disease is 99%. It says something about the test. It doesn't say anything about you having the disease or not, not directly. 

Okay. Then we know something else, namely that it's incorrect in 2% of the time or 2% of the cases. So if a test is incorrect and it turns out positive in that case, then it says, what's the probability of the test being positive given that you actually don't have the disease? Yeah, these are the false positives in the world. And of course there were issues with the COVID test that this, even though you might not have the disease, but the test still told you so and everyone ran to the test center and get tested and so on, which was probably a good idea. But anyway, let's move on. 

Good. So we have some probabilities we can identify from the text. I can only encourage you, if you see something like this in exam, to do that because this is where things go wrong. Everything else that follows here is mechanical. It's about turning the cogs basically of the wheel. So let's try to do this. And I said, actually before, I said something about sometimes you can use the joint. So do we have the positive test and the disease? Do we actually have this probability, this joint probability, like we had for the simple fruit case, or for the fruit example? So we don't have this one from this text. 

So we can't use it. Which one? Sorry, which one do you mean again? Yeah. So is this thing wrong? 

Is that what you're saying? The blue and the red one. So okay, let me, let me, let me, it's important to find because that might be a mistake. I hope not. Let's see. So you said this one. Yeah. And then what's the other one? This one? It's not, oh, well, it's, okay. 

So the question is for the people on Zoom, is this one the same as this one or not? So it's probably due to my pure, poor handwriting here. So let me be careful to conf, what I've written here is something, stuff we don't have. 

So let me write it carefully. So the probability of a positive test and the disease, do we have that probability? We don't, right? 

We have the probability that given the disease is known to be true, then you are informed about the probability of the test being positive. So it's different. Does that make sense? 

So it's just my handwriting. Yeah. Okay. 

Sorry about that. The point I want to make here is that you don't have this joint distribution that we explored, exploited before, but you're often given something like this, especially in the medical domain. You're often given, let's say, what's the probability of some symptoms or a test given the root cause? You might know something about that. But what we're really interested in is the inverted probability, namely, what's the probability you have the disease given that you have observed these symptoms or a positive test here. 

So Bayes gives you an opportunity to, we say, invert the probabilities, but it's essentially nothing more than manipulating the product rule back and forth the way you need it and maybe marginalizing out things when you need to. So let's do that. Right. I have this probability here. I have this one here. I do not have this one. 

What's the probability of positive from the numbers up here? So I now need to employ the machinery or modernization. This is where I look at the, well, now I've actually written the joint down here. I need to look at the probability of, let's say, the disease there and then not disease. So I need to consider these two cases and some of the probabilities. But anyway, I somehow magically made the point here that we don't have this joint. We now need to factorize it. So that's what we're doing here to get exactly what we have here in the numerator. 

And then we need to consider the other case where we do not have a disease. So now I just chunked the wheels, oh, sorry, the cogs and made it work. And then I just plug in the numbers. Okay. So I plug in all the numbers here and I've color coded them, hopefully correctly, so you can sort of identify where the numbers come from. So here I just plug in the numbers. So the probability of being a positive test given the disease, this one, probability of a disease, that's this one, plus the other case where I do not have the disease. The probability of getting a positive test if you do not have the disease, that's 2% times this one here, that was 99 over 100. So that's one minus this number here, indicated here to be very explicit. So what do we see? So now I remove the 99 stuff here and we're lucky we can do that. 

We move the hundreds by multiplying by 100 in numerator, denumber, I get 1 over 1 plus 2, that's 1 over 3. I would not have been able to just deduct that from the text. Right. So here I think at least 99% of us in this room need to actually turn into and apply the rules we have available to get this number. 

Okay. Is this surprising? I think it is a little bit surprising because, well, you have the positive test, so surely you must have the disease, but that's not the case. And why is that not the case? It's not the case because first of all, the disease is not very prevalent. So if you're a random person, that's, they're not that likely to actually have the disease. But also, we have this problem here of the test not being very good to some extent in that even if you don't have the disease, it still gives a positive result. 

Okay. So that turns down things because if it had this been zero, then you would get a one. There would be a one to one correspondence. 

Right. If you take the test, it would, yeah, you would have the disease. But we have this problem here that the test is not perfect, that it, it actually turns out, let's say even though if you don't have the disease, it actually turns out a positive test. Okay. Make absolutely sure and now we didn't, luckily, see any mistakes in the slide. Go through this and go through it again and make sure you actually know what's going on here. 

Yeah, go ahead. But the negative, so what's the disease, what's the probability of not having the disease giving a positive test? It's one minus this. 

So yes, I can do that. I cannot immediately say what's the probability of having disease given that you do not have a positive test, right? Because I don't know that these two numbers should sum to one. But I do know that the probability of a disease given its positive test plus the probability of a no disease giving a positive test, that should sum to one. Stuff on the left hand side of the condition sign should sum to one at all times. Well, be careful here. 

But in this case, certainly for binary things here, the probability of a disease, the probability of no disease, that should sum to one. Not the other way around here. Make sure this intuition is there. Yeah. Good. You have seen this before. I just want to emphasize it's tricky. 

And if you want to make a computer do it, the computer can certainly do all of this for you if you give it the right numbers. The hard part here was actually reading the text and understanding how to find the correct, first of all, what is this probability distribution, which we tend to call the posterior, and actually identify the correct numbers, meaning identifying the conditionals we are given and the marginals we are given. And possibly the joint like we saw in the fruit color case. All right. Now it's your turn over during the break. So you get to actually crunch these numbers a little bit during the break, and I'll give you until 10 minutes past. 

So that must be 14 or 10. All right. Have a seat, please. Okay, so I'll help you a little bit here. Thank you. So I wrote up some stuff here, and this is something that anyone can hopefully deduct from what's written here in the text. 

All the spelling errors aside. I didn't do this problem, I should say. Okay. So the what I've done here is just list the probabilities I have available here, essentially from the text. And then I don't know what it is I'm looking for here, but not yet at least. So the difficult part here again is identifying what is it, what's the probability distribution I'm interested in. So it says what's the probability based on the wholesale data that a customer that spent above the median consumption on delicacy, whatever, come from visible. So we're looking for the probability of you coming from Lisbon, given that you have spent above. So I'm using above to indicate that you are above the median consumption here. So that's a random variable. Are you above something or below something? Okay, that's a binary proposition. 

As it turns out, we are treating that, which we know we can as a random variable. Okay. Good. Then what do we need to do? Well, you need to then plug in what we need from Bayes theorem. So anyone can remember this by now. 

So I'll write up the generic one. I've done this a few times. So but I do hope that you will learn to do it as well. 

Product rules equate them move stuff around. Good. Then we're looking at a probability for Lisbon giving a so we need something in the numerator. Are we given? 

So let me just write this again to make the same point as before. This is the same as the joint of A and B divided by P of B. We're not given the joint, but we are given some conditionals. So how about we then use the version I wrote up here? I can write what's the probability of being having above consumption given that you're from Lisbon times the probability that you are actually from Lisbon divided by something here. And now it gets a little bit tricky because now we don't need to sum two things. We have three possible outcomes here, and I'm not going to write this out, but I'm going to write the general form here. 

So I need a sum over something where I have the probability of being above, then giving some city and probability of some other city. Okay. Of course, here it's given. Here's a conditional on that city and here's the probability of that city. Then we put in sort of maybe a sign like this where we say we need to sum over all the possible outcomes so Lisbon or Porto, other. And now you get a number which I can't remember, but I believe it's B. So let's just check in a minute. So let's move to the next slide here without me making a mistake. So that's essentially, so here they call this being above something. 

They call it deli H so high. Yeah. So I used A here. Here's deli H as a random variable. 

Okay. Then I plug in my numbers and I get a certain thing here. So I guess the question you should be asking also now given the discussion about calculator, could you do this with a calculator and not use Python or Maple or whatever you're used to using? 

Yes, of course you could. It's about multiplying basic numbers and stuff like that. Right. 

So it would take a little bit of time pressing the numbers in your calculator, but you don't need Python or anything to do this. Okay. That was just that was an aside. Okay. 

A few more things you need to be aware of and you've heard about this before. If I have two random variables, X, I and Y, J here, we say that they're independent if the joint probability of the two random variables can be written as follows. So factorized without any approximation whatsoever, just turns out that they're independent. 

Now we're talking about a slightly more difficult one maybe that we're going to use when we talk about so-called naive base classifiers in, I don't know, one, two, three weeks or something. This is what's called conditional independence. So here you look at this expression here and you see you have X, Y and Z and then it can be factorized as follows if it's conditional independent. 

What does this mean? Well, if they're conditional independence, then knowing something about X does not tell you anything about Y if you know Z. Yeah. So that's different from the independence we have up here. You need to know Z for this to hold true. 

So conditional independence does not imply independence. Okay. So this is something we are going to use a little bit more and way more, I would say, going forward, but it's not so important for today's topic, I think. 

So anyway, good. Then they're coming back to expectations also about time. So I could update my beliefs about what the length of this lecture should be. 

It's not looking good, but anyway, we can definitely need to talk about expectations here. How many is comfortable with this? How many have seen something like this before? Just to show off hands? 

Recently. And can operate it. That means if I give you a probability master's distribution, give you a function, then you can actually evaluate it. Okay, so you seem semi-confident. Let's see. Okay, so of course this is basically a definition of this. It means the expectation of this function is sort of depends on a function of my variable and the distribution over the outcomes of that variable. So what could be a concrete example here? I just made that up with some help from your colleagues here. So if you throw a die, like we've talked about dies before, what are the possible outcomes? 

One, two, three, four, five, six. What's the expected value here if I assume that p of x is actually, let's say, uniform? So what I have to do here is essentially use a specific version of the expectation, namely what amounts to the mean of that random variable. So I need to now take all the possible outcomes, so one times the probability of that outcome, one over six. Okay, so I do that for all of them. So I get, let me be very explicit here because I think, so I need to take the sum of all of these, so I need to say one times one over six, plus two times one over six, plus blah, blah, blah, blah, plus six, oh sorry, one over six, that's the probability of that outcome times the value of that outcome. 

What is this? It's three and a half, yes. So that means that the mean value of that die that you throw is three and a half. Okay, that's the expected value that you can expect on average, so that's another way of thinking about it. The problem arises when this if function here, here it was simply the mean, so if was the identity, so it's just x of i. But what happens if we, for example, define if of x of i as this thing here, so it's the outcome minus the expected value for that random variable squared. And then we do exactly the same, we now take that expectation. 

Well, then you get the variance. Okay, so really think about this, and I'm just going to put down a little bit more notation that you might see not a lot in this course, sometimes you will see something like this, of if in this case, you might also see something like this. And the reason why we sometimes do this is to make it very explicit about, in regards to this expectation, are we actually doing this? 

And you'll see it on a few slides why we need it sometimes, not why we need it, but you'll see that we sometimes use it. Okay, but anyway, if you just use the uniform probability over here, then we get for the mean, we get something we've seen before, it's just the average essentially. And then we get to the average, sorry, here's the uniform probability of each of the outcomes, so each outcome has the same probability, one over in. Then we have the mean here, it's essentially just the average, right? 

In general, that's the average. How's this different, and we get the other, let's say, statistics down here, the variance of that random variable? How's this different from what we saw a couple of weeks ago where we talked about the empirical estimates of averages and variance? 

Okay, before we talked about something, there was a result of a sample from, let's say, the height in here, we computed what's the average height of you guys. Whereas here, I'm talking about what's the expected value of a random variable, and that's slightly different, of course they're related, in the sense that these expressions look pretty much the same. In that, if I want to compute the variance, well, I take overall possible outcomes, I take what's the outcome minus the mean, square it, and then divide it by one over in, and now I'm not doing this one over in minus one. You need to read your statistics books if you want to understand why. Okay, but this is hopefully something you've seen before, and if you haven't, then make sure you really get some feeling about what is this expected value, and you can do it by examples like I just did here, or you can try to look into the theory a little bit more. So this is for discrete random variables. 

In a minute, we're going to see what happens for continuous ones. Okay, so here we're going to play with Lego, and the reason why we're going to do this is that a lot of machine learning is about combining different blocks of things to build a model, and then we do so-called inference or learning of the parameters of those models. And the models and assumptions, we typically bake those into our models using distributions. So we've talked about discrete random variables for now, like we have the opothal, lizard-borne, or other, that's discrete random variable. You can belong to one of these categories, but we're also going to talk about the continuous ones in a bit. But for now, we need a distribution or probability mass function over a random variable that's discrete. The simplest one I can think of is a random variable that has two possible outcomes, 0 and 1. You can also think about there's two or four, but let's just say 0 and 1 here. So that could be throwing a die, and if we say it's in that b here, that we call that the outcome, it can be 0, 1, hits a tail. So if we have hits, we call it 0, b1 to tails, that's arbitrary. Or it could also be another case that we'll look at in a minute. It could be we assign 0 to people who are ill if they come into a hospital or kindergarten or whatever, and b1 if a person is well. So we have these possible outcomes, 0, 1, and they describe a lot of things in the real world. 

We've seen that already. Now, we're going to put up the possibly simplest, or it's not possibly, it is the simplest probability distribution we can think of, namely the Bernoulli distribution, and it's written as follows. So let's just make sure we all figure out what is b, we talked about that. That's a random variable or the outcome here, so it can be 0, 1. We're talking about a parameter theta that somehow enters into this expression. So how about we try to write out what actually happens if b equals 0, given some value of theta. What happens if we plot this in? So theta raised to b raised to 0, that's 1. 1 minus 0, that's raised to the power 1, so we get 1 minus theta. That is if b is 0. Then we have b equals 1, what do we get? 

Anyone? Theta raised to 1 times 1 minus theta, 1 minus 1, that's 0, so all of this is 1, so we just get theta. So what does theta mean? Theta is simply the probability of you getting a positive outcome, or an outcome of 1. 

Nothing more, nothing less, so this is just one way of writing this in a perhaps compact way. So if I give you a theta, you can evaluate this probability, right? And it's exactly the probability of observing a 1 if you flip the coin, it's simply the probability of seeing a 1, which in this case corresponds to tails for the coin example. This is simple stuff, hopefully, in the sense that, okay, let's just think about what does this actually mean, and please do this when you look at expressions like this and think, okay, what does theta actually mean, what is beta? Make sure you dig into that. So now we're going to make it a little bit more interesting, because as it turns out in machine learning, we don't just observe one event, not like throwing the coin once and seeing what happens. We actually observe something, let's say over time or in a sequence of something. So we can think about this as a data set, so you observe a sequence of binary events. Let's just write it, it could be 0, 1, 1, 0, 1, 0, 1, 1, 0, blah, blah, blah. Let's just call that our data set. Okay, we have went out into the world and observed this about something. And here the example that's given is that a hospital, let's say, assigns the value of 1 if the person is ill or whatever it is, or will. 

Let's see, how has it turned around here? Yes, okay. So it really doesn't matter in the sense that it's arbitrary if it's 1 or 0. So we can assign 0 to the patient if they're ill or 1 if they're healthy. 

Let's do that. Regardless, we can use our Bernoulli distribution, right? That describes what's the probability of observing one single event. And now we can maybe put that into something where we can learn something about the probability of you being ill. So that's exactly what we're going to do here. 

So we're going to write up a joint distribution over the full sequence and condition on theta on the probability. Now what is that? So what we're going to make now is an assumption. And we're going to assume that given that we know this probability, that's a parameter in our machine learning model, if you will, then these observations are conditionally independent. So given that I know the probability, it doesn't matter. 

I don't learn anything from knowing whether the person before me in the sequence was ill or not. That's the assumption. Sometimes it's true, sometimes it's not. But here it's a convenient assumption. 

So sometimes you need to go out and check that. But then what we write here, and some of you might not have seen this symbol as I learned last week, this is simply like the sum sign. So we simply write out, let's say we have three events. Then what does this become? It comes to probability of B1 given theta times probability of B2 given theta times probability of B3 given theta and so on. 

So it's exactly the same operation as sum. Now we just talk about products. Okay, so this is convenient. In a sense, now we just have to multiply the probabilities individually. And we have made an assumption that allows us to simply use the Bernoulli distribution. So we plug this in and do a little bit of high school math here in terms of, well, where do we end with the high school math? 

Maybe not. Let's do this one. So essentially, when you do these products, then you multiply a bunch of these thetas together and that amounts to actually just summing what's the probability of you being, I guess, if you say one for healthy. So we sum together the number of B, or the Bs, essentially. So that's the number of times you saw one. That's theta raised to that. 

We'll call that M as it turns out. And then the other one is simply, of course, the number of observations in your sequence minus the number of times it was one, they must be zero. So now we have a very convenient expression that characterizes what's the probability of observing a sequence of binary events given that I have a parameter theta. Okay, so you observe some data. Now we have a parameter. 

How do we learn that? Okay, let's see what happens. Okay, so if we actually turn this around, this thing here, which you may have heard about, go ahead, a question. It's the same. So, I mean, all of this is the same in the end. 

Come see me afterwards if you want to find the details. But anyway, so what we have here is essentially just writing out the Bernoulli distribution, the simplest possible distribution, and then we end up with an expression here that's essentially the Bernoulli. Okay, so my question here is not so much about the mathematical details. It's about this fundamental principle that we now have a probability of observing some data given the parameters. 

How do we find the parameters? So in this case, if we plot this function here, this thing here, as a function of the number of n and n minus one, so the number of zeros and ones you have observed, then that's done up here. So in this case up here, we have observed two observations in a row. 

We observed that they're conditional independence, so ordering doesn't matter. And then one of them turned out to be one. So the other one must be zero. So how does this function here look like in that case? 

Well, it looks like this. So if I actually want to find the best parameters in that case, what would I do? If I want to actually maximize this probability or this likelihood, as it's called, I would simply put the theta equals 0.5 here. That makes sense. If you've seen zero and one, then maybe the probability of seeing a one should be 0.5. That makes sense intuitively as well. But the principle we use here is maximum likelihood. Okay, remember that word. 

Because we call this a likelihood function, and now we are maximizing that likelihood function by fiddling around with theta until we maximize this expression. Now we do another one over here. So over here you've observed a hundred observations. A hundred people come into your hospital and you observe, are they ill, 0, 1, 1, 1, 0, whatever it is? And you simply evaluate this expression here. Okay, so what would the function look like here? This function here, the product of all of these individual terms, sorry. 

Then it would look something like this. If you evaluate for different values of theta, so which value of theta should you choose? The one that maximizes this expression. 

So maybe this one. Okay, around 0.5, a little bit off. And so on here we have something in the middle, and you see this value here, and maybe we should select the value theta here. And essentially this is a small, very small textbook machine learning problem, in that you're given a sequence of data. You're given some data. We have a model. It's a simple model. Now we are then looking to actually optimize the likelihood of seeing that data, given the values of our parameters. 

All right. This is a fundamental concept that we'll use for regression, classification, deep neural networks, and it underlies everything you hear about. Let's say, LLMs at the time, they all start with the maximum likelihood principle. These billions of parameters you have in your model, the goal here is to formulate a model. 

So a likelihood of the observation, given some parameters, and now you optimize the parameters. Okay, fundamental. We're going to see this a lot of times. So this is, of course, a textbook problem here. So it's a little bit simplistic. 

All right. Probability density functions. We have talked about discrete random variables, up to now. 

Okay, so we saw zero one outcomes. We've also seen a categorical one in the sense that you have a portal, and this is one or whatever. No, sorry, or portal, this is one another. That's a categorical one. And you can think about eye color as well. That's also a categorical variable. It can take, I don't know, blue, brown, and gray or whatever it is. Blue, yes. Anyway, good. We need to also be able to talk about things that have a continuous value in nature. Let me see if I can get this to fit again. All right. So it rains a lot in Denmark, as it turns out. 

For those of you coming from abroad, you've discovered this. So if you actually want to think about what's the probability of a certain amount of rain per day, what we cannot do is actually talk about the probability that it'll be exactly 0.2 millimeters of rain. Assuming your measuring device has infinite precision, so to speak, because that probability will be 0, given that there are infinitely many other values it could take. But we can talk about something else, namely the probability that there'll be between 1 and 2 millimeters of rain. And that's essentially what's shown down here. So if you look at the rainfall, then the height of this essentially just shows what's the, well, if you normalize it, what's the probability that you'll see on any given day that the rain is between 1 and 2 millimeters. 

That we can talk about. And to make that operational and actually being able to operate with that kind of stuff mathematically, we're going to talk about what's the probability density function. So the probabilities can be represented by as integrals. So if you want to talk about stuff falling in an interval, what we can do is define a so-called probability density function that's the black one here, that's the P of x in here. And then we can simply integrate, in this case you can think about rainfall. We simply say what's the probability that you fall in this given interval? Okay, that's given, sorry, in this interval, sorry. That is given by integrating this curve from 2 to 3 in this case. 

That's written up here. Okay, so all we have to do is define a probability density function, loosely talking distribution, how's the probability distributed. And then we simply need to integrate that to talk about probability. But often we don't actually need to talk about the probability. 

We simply talk about the density function itself. So if you actually try to do this, then you say what's the probability that you fall in this interval? Well, a crude approximation, what you've heard from real analysis or just calculus basically, simply just to look at the width that's called dx here. Here, and so we have x and then we have dx, just slightly, you add a slight dx to x. So now you have an interval here on the x-axis. 

And then you can simply measure the height and then you have a very, very crude approximation towards actually the area under this curve, which is the probability. And of course there's something important I forgot to say, and that is that this thing here must sum to 1. Like we said that the probability of all possible outcomes for the discrete random variables, they must sum to 1. Similarly here, the function that characterizes the probability of how it's distributed must also sum to 1. So if we do that, then we can talk about that this area under the curve corresponds to the probability of you falling in that interval. 

That means that the value, that means the random variable falls in that interval. Okay, so as I said, the slides are going to be ugly because I changed the background here. So bear with me, or at least just ignore the aesthetics of the slides. So this was for 1d and I think most of you would have seen that before. 

What happens in 2d? As it turns out, we can do exactly the same. We can say if you focus on this stuff down here, now we have two random variables. So let's say temperature and, I don't know, wind speed outside. So temperature in here, wind speed outside. Ah, that's a bad... Well, actually maybe they're not totally independent in the sense that various sensors and stuff actually influence, based on the wind speed outside, actually influence the temperature in here. So maybe they're not totally independent. Okay, but anyway. So we have two random variables. 

You can come up with your own example here. I'm looking for what's the probability that, let's say the temperature outcome falls in this interval here and this interval here on the wind speed falls in this interval. All I have to do is apply the same machinery in that I say, well, if I have a probability density function, sums to 1 now over x and y, then I can write up the probability of falling in that specific area here simply by integrating over this 2D function here, or this probability density function. So if I do that and do exactly the same tricks as before, sort of a very crude approximation, I get the volume under this thing here. And if we turn the, let's say, the mechanistic thing of binary proposition a little bit again, then we have that probability of you falling both in this interval, or falling in this interval on the x-axis, in this one on the y-axis. 

Then it must both be true. We can use the product rule here, and if we then make our approximations, then we get basically a rule, a product rule, for the continuous case similar to the discrete one. So it means that if I have a random variable, what did I say, temperature in here, wind speed out there, I can factorize it as this. 

Of course, now I need functions here and not just sort of numbers, so to speak. So it becomes a little bit more tricky. If we then, again, try to write out the rules we can deduce from this, then we have the sum rule, as I said, it still applies. It must sum to one over the stuff that's on the left-hand side of the conditional sign here. Remember, that can be something from minus infinity to infinity, potentially. 

So now we need to do an integral, not just summing things. That adds complexity and worry and sleepless nights and whatnot. But for some cases, no, not for some cases. In all cases, if you have a valid probability density function, sorry, then this should sum to one. And we, I'll just argue very hand-wavy that actually the product rule also applies. And then base rule can be derived from that. 

And what do we get now? Well, we get that the so-called posterior distribution is typically what we say is the likelihood times what's called a prior. And then we need to also make sure it's a proper distribution. So we need to normalize by this expression here. Before we had the disk stuff came from adding over all the possible discrete outcomes. That was a sum. 

Now we need to do something that's really, really nasty. And it almost never works out except for textbook examples and very trivial models. You actually need to compute this as well to be able to find this one. So I'll show you a few examples where it does work out. But the end of the game here, this is the one slide you need to be able to remember, I would say. And if you can't remember it by heart, maybe write it down in your notes for an exam. So you're absolutely sure you have the equations that you need. For discrete random variables, everything had to do. We had the marginalization had to do with sums. We can remove a variable from a joint distribution. Here I'm removing x or I shouldn't say dx here, of course. Here I'm removing x from this equation by summing over, from this joint distribution by summing over all possible outcomes of x. 

And then I get p of arguments in. Product rule, we've seen that base theorem. Yeah, we tried that out a few times. And it turns out we also have the same rules for the continuous random variables. So now we all set to do proper statistical inference as it turns out. As I said, the nasty thing is actually computing these integrals. Okay, but that can be done in a few cases. 

So let's see where that brings us before we just get to that. I just want maybe also highlight that the expected value actually also applies here. So the expected value of a continuous random variable. Well, that's the mean of that variable, essentially. And we, well, similar to the sum we had here, we replaced it by an integral. 

So everyone is happy, right? Just take these equations or expressions we have with a sum, exchange it with an integral and hope for the best. But it does work out, I should say that. But this is basically just to remind you that before we had the expected value of random variable, the discrete one, there was the sum again over things. Now we have an integral over the value of x, so that you can think about the temperature, times the density of that variable at that particular value. So again, sum becomes integrals. And everything else applies here and just again a reminder of something we've seen before. The covariance can also be written in terms of expectations. We talked about it previously as averages. But of course, if we actually have the distributions here, we can compute this expected value of a random variable. What's the expected value of the temperature in here? For example, the same we can do for variance and the standard deviation follows. Okay, so this is to link back a little bit to what we did previously. 

That brings us to two important equations or, sorry, probability density functions, namely the multivariate normal density. This is the single most point in this lecture. It's in the exercises that is the focus of this week. 

And everything we've been doing is to get to this point. So make after this, sure, to listen now and try this out in the exercises as well. Then I'm going to introduce you to a beta distribution that I need to make a point in today's lecture. And then we're going to use it a little bit later when we talk about the precision of our machine learning methods. But for today, focus on the multivariate normal distribution. Remember this, I think I spelled it wrong actually on the slides in the first lecture, whatever it was, second lecture. 

So I'll fix that now. And just remember that the Mahanloubi's distance takes into account this covariance matrix that we talked about. So remember the covariance matrix expresses generally the linear relationship between variables. If one variable goes up, temperature goes up, what happens to the wind speed outside on average? 

That is what we've been through a couple of times now. But of course also contains the variance of the individual random variable. So just to remind you of this plot, it essentially says that if you use the Mahanloubi's distance, that means you account for the linear relationship between variables. Here you have two variables. If you account for that, then the distance along this direction becomes small. 

Distance along this direction becomes big. So it's a way of sort of taking into account what's actually the shape of the data. On the other hand, if you specify a covariance matrix for me, I can also manipulate the, let's say, the shape of the data if I want to sample from a probability density function. Okay, you'll get to do that in the exercise. So this is a multivariate normal distribution. 

How many have seen this before? Okay, not a lot, but it is really, really key to a lot of things we're going to do. Okay, so what is it really? It is simply a distribution or density function that describes the, let's say, how the probability is distributed across not just one random variable, but several in this case. So x can be a vector, and then we have some parameters mu and sigma. So this is the probability density function we saw before. And you've all seen this for the 1D case. You all know, hopefully, what a normal distribution looks like in the 1D case. That means I am now going to evaluate this in the 1D case. 

Okay, so I have, let's just call it x1 and just say we focus on x1. That's our random variable. If I specify that I have a mean here, whoops, that's an occly mean here. Okay, that's the mean parameter that I'm going to plug into this expression here, or more simply, this expression here. This is the normal distribution in 1D. This you have seen before, I'm sure. Okay, so what is actually the shape of this thing here? Of this thing. Well, it's a belcher, right? We've seen that, and I'm going to attempt to draw a belcher. It's probably not going to go so well. 

Something like this. It's supposed to be symmetric around this mean, and then it's going to have this standard deviation. And this is the P of x, given, in this case, the mean and the sigma. Given the mean and the sigma, this curve is going to look roughly like this. 

It's going to tell me how is the probability, and we have a total probability of 1, how is that distributed across the possible values of that x variable. Now, you can imagine, I hope, what would this look like in 2D? I hope we can sort of imagine that it's going to look like a bump function in 2D. And depending on what are actually the parameters here, let me actually go back. So keep in mind here that what we have up here in this expression, that is the Mahanloubi's distance. 

So we've been through that a couple of times. So this one sort of scales the directions, or the distance in different, let's say, directions, depending on the linear relationship between the two variables. This one measures the distance between your x over here, so the part you're going to look at, and the mean of your distribution. And then you get this Mahanloubi's distance that you need to evaluate here. And then you need to normalize by something. This thing here is called, is the determinant of that matrix. That matrix needs to be positive, semi-differentate, means that it needs to have full rank, essentially. 

So they can't be linearly dependent. Okay, good. So let's see how does it look like in 2D? And again, you're going to spend some time on this in the exercises. 

Well, unfortunately, this is not a particularly great figure, but if you look at not just x1 as we saw before, we also have x2 now. I need to specify a mean for each of these two. So I'm probably sort of, I don't know, in the middle here, that's the mean value. It's this vector here. It has the expected value of x1. 

That's what this expression means, if you remember back on the slides. So this is going to be the mean of these random variables. Then we also specify here, of course, you may want to think about how can we learn these parameters, or otherwise, think of them. But here we have a covariance matrix. We know that in diagonal turn, it describes the variance along the two directions. 

So along x2 and x1. This stuff here you looked at before. It tells you, again, if you change one, how much do the other one change on average? Does it go up or down? 

Okay, so in this case, if I go, let's just do one here. What happens to the other variable, it tends to go up. And what you need to remember here is that you've looked at contour curves before, where you slice this function here. So this is a function of two inputs. 

Then you get one output, the p of x, given my parameters. Okay, if I slice that, then let's see if I can draw that somehow. Then you sort of evaluate it on this curve here. This is what we've shown over here, roughly. Maybe this one corresponds to this curve here. So this is going to, this contour line is going to give you this shape. 

And it's going to indicate if one variable, how are the two variables dependent on each other. We are going to spend a little bit of time on that. And the way we are going to do it is that before we do this quiz, I'm going to show you the fact that we actually do have as, maybe, maybe we have a small demo here. Let's see what happens. 

Yep, good. If you have not seen these demos before, please go look at them. You can sort of find them via this link, not this link. But there are various demos and we usually refer to them in resources. So here you can play around with this. So here again, I have the, one, the, oops. 

Here I've got, no, of course I can't, it's getting old. So what we have here is x1 and x2. So we still have the two variables. 

Now I'm looking at a normal distribution that's given by some mu down here. So the center is zero and zero. And I'm looking at the scatter plot here. And then an indication of the shape of the normal distribution. And then I think if you're lucky, we can click on one of them and you can play around with this and get an iteration for what this means. 

So if I stop here and point at this, ah, maybe, there we go. If I now look at this, the mean is still zero. But the covariance matrix is definitely not zero, right? I have some variance in the top left element here, 5.93. It's the variance along this direction. This 5.36 is the variance along this direction. Then I have covariance and they need to be the same. It tells me it's positive. 

So it must go in that direction. How much? I don't know exactly, but what I can do, like I alluded to last time, you can actually throw this matrix into an SVD and you can get the eigenvectors. And the eigenvectors will point in the direction of these two orthogonal matrices here. 

Then it's the wrong link. Okay, so, okay, slides, where were you? Good, good. So you actually have to do this now and it's perfectly fair to actually look at this link here. So I'll give you, I won't give you a long time here. So just think about this for two minutes in the interest of time. And then you can go look at it afterwards. So have a look at this very quickly. I do need to tell you one thing before you can, hey, shh, I do need to tell you one thing before you can actually solve this. And that is, let's just assume that this blue one, contour line corresponds to 0.01 or something like that. Now you can solve the problem. 

So have a look at this. Actually, let's do a short break here until, I don't know, three minutes to three. Come back if you need to leave and then come back. 

Then we have about 15 minutes left or something. Thank you. All right, we almost there people. So we only have one section left after this one. And so as I expected, we will go a bit late. My expectation currently is like a quarter past. So that's when the T is show up. So that's perfect. 

Almost good. So who thinks the correct answer actually doesn't mean, yeah, well, the correct answer is D. So let me put it that way. And why is that? Well, maybe let's pick a few of the ones that are wrong, all of them actually. 

What's wrong with this one? Remember, variance, variance, covariance. So positive covariance, that's my first point. 

It would definitely show a shape like this. The main problem here is that this matrix is not positive semi definite. It means that it has not full rank. These two column vectors, they're not linearly independent. So it's not a proper covariance matrix. Try to throw this into an eigen decomposition. 

See if we can find the eigenvalues. You'll see something interesting. Good. What about sigma C here? So it's one, one and C. Well, there should be a positive covariance, right? So one variable should depend on the other. So this should not be zero. Okay, so this one's wrong. Wrong. 

What about B? Well, we know that the covariance, first of all, the covariance of between X1 and X2 is equal to the covariance between X2 and X1. These are not equal. So that's the first sign. 

Okay, first of all, so it wouldn't even make sense here to begin to talk about this one. What about C in this case? Well, actually that shape aligns with sigma C because you sort of have a variance along here. There's this one that seems about right when I gave you this contour line thing. 

You need to go check it exactly. One along X2, sorry, this number here. That's X2. So that seems to align. 

Of course, 0.9, you can't be actually sure without maybe checking. So you can find the eigenvectors of this matrix and I can tell you they will point in this direction here, the predominant one, so that would be the principal component direction. The other one will point in this direction, explains second most of the variance, that direction. All right. Then so that was also wrong because of this thing here. 

So wrong, wrong. What about C? Option C. Well, there's something that says that the variance along X1 for B, if the B option should be 10. And of course, if I hadn't given you just at least an indication of what the contour lines represent in terms of the probability density function, you wouldn't have been able to answer this. 

But here it doesn't seem like, from what I gave you, that there's a variance of 10 because that's huge. Okay, so maybe it's one. Actually, it turns out to be the correct answer here. So also C is wrong. Otherwise, it actually looks feasible in the sense that if I change the scale, then it would also have been feasible. But in this, whoops, that wasn't you. 

Okay, that's not possible. So the correct answer is D and let's just double check. Option A, if you don't have any covariance, the two variables do not depend on each other. You'll get a circle here. So if I can sample points from this distribution here, then if you sample from it, there's more mass in the middle along the mean. Option B, it has higher probability if you look at the interval around the mean. So the points sampled from that distribution are much more likely to end up here in the middle. So you see many more points in here and fewer out here and non-out here. Think about the sampling perspective as well. Then we have, well, so zero here, zero there. 

There's no covariance or linear relationship indicated by the shape here. So that's viable. This one, well, we have a variance of one, one, and then we already covered this one. This is correct. This is correct. 

Any questions about this? Try this out once, twice, three times, four times. This is really, really important. And you'll also see a lot of the exam questions relate somehow indirectly or directly to covariance matrices to the multivariate norm distribution or to properties of it. So make sure you know this. 

Good. What other distributions can we think of? Here we talked about a distribution that's defined. 

Let me just go back to my slightly ugly drawing here. It's defined from minus infinity to plus infinity in 1D. Can we think about a distribution that's only defined from zero to one in 1D? So we're going back a little bit in the sense that we're now not interested in distributions over the full set of reals over from minus infinity to infinity, but only between zero and one. Can you think about variables that may be between zero and one? 

We talked about a few today. Probabilities, they are between zero and one. And we'll see how that works out a bit. But anyway, just think about this as another function. It's a beta density, beta distribution. The likelihood of observing, let's say, theta given some parameters is given by this slightly nasty expression. So if you just focus not on this gamma function, which is related to the factorial, this is used to normalize the distribution. 

So make sure it integrates to one over theta. If you just look at this bit here, we've seen something very similar today for the Bernoulli distribution. So, yeah, well, actually the binomial one that your colleague referred to, or you referred to the Bernoulli, I referred to the binomial. 

Anyway, you've seen a distribution very similar to this today, an expression like this. So if alpha is one what we get, we get theta, so on and so forth. So here are some parameters. And remember, we had a parameter before, like this theta value we had to infer. So if I set these parameters, I can sort of get different distributions here. Like I can get different shapes of the multivariant distribution, or normal distribution in 1D. I can also get different shapes of the beta distribution. So I'm evaluating for given theta, what's now actually the PDF or the probability density function at that point. And I can get something that looks like a uniform distribution on the 0 to 1 interval. I can get something that has sort of a higher density up here, close to 1, and one that has a sort of a preference for both of them, for 0 and 1. Okay, then I can write up an expectation to find what's actually the expected value of that random variable. 

That's the mean. It depends on these parameters in a certain way that you don't need to remember. This you don't need to remember either, but it's just to show that, well, the variance, that means an expectation, actually depends on the parameters here. So it's the mean, like for the normal distribution, you have that the mean coincides with the mode, that means the highest point. 

That's not the case for a beta distribution. Just to trigger your mind a little bit. Okay, so depending on the different parameters here, you can get different shapes, and I've shown you three on the previous slide. 

You can get different shapes here depending on what you want to encode, or what you infer about the parameters. Okay, so why am I talking about beta distributions now so late in the lecture? Well, I want to end this lecture with sort of a, not a rant or anything, but maybe a little bit of a motivation for why do we need to think about probabilities and how it relates to essentially this course, but also other courses, both in statistical inference and in Bayesian machine learning. 

So I'm going to give you the full sort of palette, so to speak, of what you could do here, and one instance of it. It's not going to cover everything you need to know about Bayesian inference or even machine learning, but it's going to be a motivating factor, hopefully, why you need to learn about probability density distributions and probabilities, and Bayes theorem, as it turns out. So this is not a very great slide, but that's essentially what it just said, that in machine learning, we tend to have training data. 

Here you can think about the training data being that set or sequence of repeated binary events that ends up with a Bernoulli distribution. Okay, then we have a model. That was this probability distribution we had, the Bernoulli one. In here we need to learn some parameters to make a prediction rule about something. So there are some parameters involved in our model that we have formulated using probability distributions. And then that gives us, if we know the parameters, we can now predict what happens in the future. 

That's the general idea here. And if you think, one way to think about this is a statistical inference. That means figuring out what should these parameters be in a statistical model based on some data. So don't be afraid of statistical inference, because a lot of modern and also the machine learning that actually works is built or comes from this notion of inference. 

There are other ways of building machine learning methods, like hacking stuff together and writing up custom loss functions and coming up with ad hoc rules for everything. But at the end of the day, at least the view we would like to, let's say, promote or advocate for here is based on this very nice, very elegant way of dealing with assumptions and data and likelihoods and models. So I'll give you that view. And it's not something you need to remember per se. It's more about giving you a broad view of things. Now, let's try to come up with an example here. 

So there are two cases here. For some reason, your friend got a dog. The dog can either be inside the doghouse or outside. You don't know if it has a preference for either. But anyway, the first four times you come by, you observe that the dog is in the doghouse. Okay, so you have four observations now. Then what's the chance that the dog is in the doghouse tomorrow or the next time you come by? Okay, that's one case, one use case here. 

Then maybe not, well, we can say it's the same friend. He buys a coin. And coins have already been sort of assumed to be relatively fair that if you flip them, they end up equal probability on each side. 

But anyway, let's see. The coin cannot come up either heads or tails. The first four times you flip it comes up tails. 

Okay, what's now the chance that the coin comes up tails next time you flip it? What's the difference between these two scenarios? Because if you actually look at this Bernoulli likelihood we wrote up, it would be theta raised to four in both cases. If we assign, what was it, tails here to the S1 and down here, also you did the dog being in the doghouse as one and down here the tail event S1. 

There will be this same likelihood of observing that data in both cases. What's the difference? Do you think it's the same? Should we make the same inference about theta? Should it, the theta would be one in this case, right? Because you have four outcomes. You're only observed four outcomes of one. 

Should the probability of seeing an outcome of one be one, like 100%? Well, you're not sure about this case, I would imagine. You don't know a lot about dogs and they tend to be relatively irrational and random things. 

You don't really know what they do and do we know they have a preference for being in the doghouse or not? But we do have some, let's say, prior knowledge about what should the coin actually be when you flip it. If I give you a coin, I'm not going to, but if I give you a coin and you flip it, what do you expect the probability should be based on your knowledge? 

50% or something, roughly there, right? So maybe the fact that you saw four cases here in a row where it ended up tail, that's just coincident. Whereas for the dog, you really have no idea what the dog is doing, right? Is it coincidence or not? 

Maybe it actually has a really strong preference for being in the doghouse. Now, if we did what we did before, namely come over to the Bernoulli distribution here, sorry, the binomial distribution, I should say to be entirely correct, where we have a number of counts and so on, well, what would happen? What would theta be? It would be one, right? 

Because you've seen four observations each time it came out one, so you would imagine the parameter should be one. The probability of seeing one is 100%. If we do what we did before, namely this maximum likelihood principle, where we write out this probability here and maximize theta, that's our machine learning model now. We maximize theta, that would give you a one, 100%, probability of seeing a one, based on four positive outcomes. So is that really what we want if we think the coin is fair? Well, how about we just trust me on this one and maybe try to think about what have we learned today in school, so to speak? 

What about if we actually try to crunch a base theorem here? So we have the same case here, so now I've flipped the situation. It doesn't really matter, but let's just say that the coin comes up one, whatever that means, and zero, that's the other way around. Then we wrote up this likelihood here, this probability as it turns out to be. Well, the probability of observing M out of N positive or ones. So we've seen that before. 

Now, what happens if we actually plug this into base theorem? So if I want to make an inference about what the parameter theta should be, based on my data, that's the sequence of binary events, if we say that, okay, if I can write up what's actually the probability, or we generally call it the likelihood, of that data given our parameters. Well, I can also put a prior on it. We call this the prior distribution. 

This is where I can encode my belief or knowledge about the world before seeing any data. Now I need to, as I've alluded to a couple of times, divide by this or normalize by this nasty equation here. And if I put a beta distribution on this one, so P of theta here, that's the beta distribution because it's a probability. We know that, so it must be between zero and one, and I'll put a distribution over it. That distribution, the parameters in that, should depend on my beliefs about the world. Okay, this one I already defined here, that's to some extent my core model, or that's my likelihood. That's my assumptions about the data. Then I have assumptions about my parameters in that model. Now, this is essentially the full mechanism you need to do inference, statistical inference, Bayesian inference. Now, of course, there's a reason why we have at least two more courses in this topic. 

So you're not going to be tortured by having to do all of this in this course, but it gives you the general framework, and then we are going to do a subpart of it. Okay, anyway, so the Bayesian thing here in this case can be worked out. And if you actually do this, and again, I'm not expecting you to be able to do this in this course, you get a so-called posterior distribution here, which depends, of course, on your data. 

So we've learned something based on the data, based on our parameters that encodes our assumption. Now we now have a distribution over theta. So let me just visually think about having a distribution over a probability. That's the beta distribution, theta here. After having seen the data, it's a distribution. 

It might look something like this, and we'll see a few examples later. So now I'm not just finding this one point that we did for maximum likelihood. I'm actually inferring a distribution or parameters in a distribution that encodes my beliefs about what the parameter should be after having seen the data. This is beautiful in my mind. If you don't see this, then really think about why this is beautiful, because it allows you to encode your belief about the world, allows you to put up an explicit model about what you think about the data given the parameters, and, well, Bayesian tells you what to do. 

You can do it analytically in very few cases. This one turns out to be one of them. So if you have a beta distribution with the Bernoulli distribution we talked about, a binomial distribution, then it turns out that this posterior distribution, that means this expression here, is also a beta distribution. So if you look carefully at the expression I showed you for the beta, this is essentially the same in that we've just added m and n minus m somewhere, a few places. So this is, again, just a beta distribution. 

The posterior is a beta distribution. Beautiful. Yes. So what happens here in this case? So here I put the prior. That means the expression I think about the parameters before seeing the data. 

Okay? What happens to the posterior distribution if I see, I only observe one data point and it turned out to be one. What would happen in the maximum likelihood? Again, we would think that theta should be 100% or one. But here, because I put this prior on it, that's uniform across 0 to 1, that's p of theta, then it ends up simply just, it modifies this prior belief about what theta should be, not to give you sort of a very peaky distribution here, but it somehow tells us a little bit, namely that there's very high density here for the probability being one. 

There's very little density here for the theta value. That means the probability of being zero. So it means that we've seen an outcome of one at least one time. So now we can rule out the probability that, or we can rule out the fact that, or the option of it being able to be zero, so to speak, if that makes sense. So the probability of theta or the likelihood of the, of theta being down here is essentially zero. What happens if I then, let's do this one, observe a hundred of these binary events, 51 of them turned out to be positive ones. 

Okay, what happens? Well, I took my uniform distribution and I turned it into this peaky one. So I've learned that, okay, maybe the, let's say in this case, the expectation turns out to be around here, so does the mode. And then the variance has shrunk around that. 

We say that it's concentrated around the system. So of course I can choose to then pick, in maximum likelihood, it would be, essentially the estimate would be 0.5. That's the maximum likelihood estimate. 

Okay, well, okay, I can also see that from my distribution as it so happens. Okay, what happens if we then combine this with our dogs and coins here? Remember we had a slightly fuzzy case where we don't know a lot about the dog. So how should we encode preferences for the dog? Well, in the prior, that's the beta distribution, I can say, okay, I think there is a preference for either being in or outside. 

Okay, I can sort of give in that preference. I can combine that with the likelihood function and I can get the posterior. And the posterior now says, okay, yes, that's a very high likelihood that you're going to be in this area. 

That means you're going to be in the doghouse, whereas not being in the doghouse is down here. It has very low probability if you integrate it up here. Okay, so it modified that prior by observing some data into this distribution. What about the coin? Well, you told me before that you think it should be around 0.5, but maybe you're not sure it was an old coin, so it might be magic or unfair. So maybe you think before this happening, you think that it shouldn't be 0.5. Well, maybe in expectation it should be 0.5, but you're a little bit unsure about this, so you're actually going to give me around this expectation of yours, you're actually going to give me a little bit of variance. How unsure are you about this? 

We're not going to observe that I've seen four, I can't remember if it was a hit or tail, but four ones. What should then happen? Well, if you look at this, what is happening here is that we turn it from something that's symmetric around this sort of 0.5 estimate. We're turning it up a little bit up, so we're moving it this way, so we're becoming a little bit more sure that it should be 0.6 or something, I think it is in this case. 

But you're not moving it all the way up to 100%, because you still have this inbuilt prior belief that it should actually be fair. Now, what happens if I throw this die a million times and it comes up one? You are actually going to end up with a distribution over here. 

But the key point here is that in this patient inference setup, or statistical inference, you get a distribution and not just the point estimate of what the parameter should be. Now, this is very, very complicated. So let me give you another idea, namely that you... Let me be careful here. 

Let me give you another idea. So instead of finding the maximum likelihood, we still want to include some prior knowledge. But instead of finding the full distribution here, I simply find the maximum of this distribution. 

It turns out I can do that easily using what's called maximum apperiori. So now we're going back one step. We've folded out the full Bayesian machinery. 

Now we're going back a little bit, because we think it's really difficult and maybe it's a little bit beyond what we can do in this course. But let's start out with the full Bayesian machinery here. So this is the Bayesian thing we did before. Here you get a distribution out if you do what's in here in the square brackets. Okay, what if I now simply put an optimization around this where I say, okay, I'm actually only interested in that one point where it's maximum, where we get the maximum of that distribution. 

I can write that as follows. So given that what is the maximum of my posterior distribution, that means of P of theta, given my data, so these are the number of observations and number of positives, what is then the maximum value? And if we crunch this a little bit and first we say that actually maximizing something is the same as minimizing the negative. If we also allow ourselves to do the log so we get rid of this fraction, what happens? 

Then you get an expression like this. This relates to the likelihood. So if you just optimize this part, you'll get the maximum likelihood estimate, which we'll do in 99% of the models we're going to look at. If you then add some, maybe 90%, if you then add some prior knowledge or assumptions about your parameters, then you add this prior and then you need to normalize here. But because we're only interested in the point estimate, you don't need to worry about that part. 

It disappears. So now we're back to an optimization problem, but there's something related to the data, the likelihood, something related to the prior or the assumption about the parameters. So this thing here is called the maximum of my posterior approach, and this is something we're going to use. We're going to, in this course, do the maximum likelihood. That means focusing on this part. We're going to build regression models and classification models around that, even deep, well, no networks around that. 

For regularized linear regression and regression and logistic regression, we're going to add what essentially corresponds to this part and end up, it was called, which regression and regularized models. I, well, this is just to summarize essentially what's happening here. So what is important here is that, I don't think, well, this is to tell you a story that there's something beyond what you learn in this course, but what we learn in this course can be related to that, to a broader sort of, let's say, view on things. 

Whereas we're going to focus on certain parts of this broader view. That is really, really elegant and really beautiful, but it can take a while to understand. Anyway, let me just summarize. 

Okay. What is important here? I assume when you went into this course that you knew about discrete random variables, I even assumed that you actually know about the Bernoulli distribution, also the binomial distribution that we actually derived. Later, we're going to talk about the categorical distribution for multinomial classification. 

So, classify, models that can classify into more than just binary groups, essentially. Then we're going to look, I also assume you knew a little bit about continuous random variables, at least in 1D. And then the really key point here is that the multivariate normal distribution is something you should take, go away today and know what is and feel comfortable with. 

The exercises are going to help you with that. I introduced the beta distributions a little bit quickly, but that was to tell a story about how we could combine different distributions in an elegant way and simply use base to mechanistically, in a mechanical way, to get the posterior. And if we do that, we get the full distribution, but maybe we're not interested in that, so we get the point estimate. So, that is essentially what we end up with here. The important equations for this week would be the multinomial distribution, covariance, and the expectations as well, and that you know what they are and how to operationalize that. 

If I were to ask you to compute the expectation of a random variable, I expect you to be able to do it, at least certain random variables. Okay, then the view I started out telling you about binary proposition is part of this book here. And this essentially tells a very cool story. It's a little bit of a thick American book, but, oh sorry, not American necessarily, but it tells you a story about how probability should be viewed. 

In a slightly different view than what your statistics professor told you, namely focusing on these binary proposition, building it up to random variables. And I think that link is also really important to understand. That's all for today. See you in the exercises. I'll walk around. Peace. you 
Audio call. 

Speaker 1: All right, let's get started. Can you hear me in the back up there? Hello, anyone in the back seat? Can you hear me? Yeah? Great. Good. Welcome to week five of the introduction to machine learning and data mining. And today we are actually going to talk about specific machine learning methods. As usual, make sure to read, make sure to attempt the homework problems or do them during the exercise sessions or after class. As we have talked about, you cannot expect to do well, I would say, on this course unless you actually spend the time we require you to, namely about nine hours per week. 

All right. So, as I said, we have about, sorry, we have entered sort of the second block of the course. This is where we talk about supervised machine learning, arguably the most important block where we start out today presenting you with some, let's say fairly intuitive methods. You might have seen them before, or at least you can sort of intuitively deduce them sort of from logic. 

What would you actually do to classify stuff, for example? And then we're going to talk about linear regression. And we're going to present that in a version where it builds on what we talked about last time, this abstract narrative I tried to tell you about in the third part last time. We're going to build on that and try to put that into action for linear regression. Probably something you haven't seen before, but I'm going to link it to something that some of you might have seen before. 

Good. And then, of course, decision trees, which is a sort of classic machine learning methods for doing classification, but we're also going to talk about regression trees, where we can actually do regression with that method. So, today, we're not going to talk a lot about evaluation, but the next couple of weeks you're going to hear a lot about evaluation and a few more specific methods. And next week it's going to be George's, my colleague from Compute, who's going to be in week 6, 7, and 8. That's the plan for now. 

It might change a little bit. I'll come back in week 9 and complete the course with you. But for now, this is the end of me, so to speak, and you just have to survive two more hours or so. 

Good. Then a few practicalities. Of course, I hope most of you are well underway with project one. 

I have also made project two, a description available on DTU Learn, so you can already start thinking about that now. Don't stress too much about it, right? I can't even remember when the deadline is, but there's several weeks to do that. So, don't panic just this week. Just, okay, there's something we need to do. 

Perhaps let's read up on that a little bit. Exam date, I think last week I wrote Currently, which confused people. And the only reason is that I'm not in control of this date. It's 101, the administration, so if they decide to change it, they'll change it. 

But the probability of the exam date being on the 28th is going to be 99.5 or something to that effect. So, you can surely count on that. Good. Then I have a little bit of news about this programmable calculator. And I know you're not worried about it. 

I'm a little bit worried about it because I know all the emails I'm going to get about what is an unprogrammable calculator. As it turns out, well, let me give you the backstory. We are governed by a certain DTU rule, which you can find here, and now you're getting my story about DTU politics. So, maybe close your eyes if you're not interested. Okay, so we are governed by some rules here. And it says you can bring a, it's a no age. You can bring a calculator and a dictionary, like book form dictionary. 

It also, of course, we have the additional option of having two sheets of notes as well. So, that's what we are governed by. The problem is that the rule says non-programmable, and no one knows what that means. Now, in general, of course, you should bring a calculator to the exam, but we do not expect you to bring something really, really complicated that can draw graphs and all of that. That does not define what a non-programmable calculator is. I'm just saying what you might want to bring, namely a calculator that can add stuff and multiply simple stuff, not matrices, but can do the exponential function or the log function or something like that. But not something where you can put your own programs and potentially notes on your calculator. 

There's no need for that. But in the end, if you're in doubt, don't ask me, please, go directly to the study administration, tell them that I have sent you, and you are now looking for an answer whether your calculator is a programmable one or not. I am not allowed to tell you if it's one or the other by my newest sort of study board hit. This is a technicality to some extent, but there were some issues last year that some students, they actually took this literally. So, they didn't bring their graph calculator from high school or whatever it was. Whereas others, they brought a very simple scientific calculator that can do exponential log and add stuff, but not draw graphs, not actually have custom programs on it. If you're in doubt, that's the end here. 

Go to the study administration and ask. Is that clear? Good. So, I'm not going to include this anymore, unless we're going to talk about it in the final week, probably. But for now, I hope this is clear. Are there any questions or comments about this for now? 

No. That's good. No reason to panic about it yet. I'm just saying, okay, this is what you should do if you're in doubt. 

Good. Then onto something actually useful, hopefully. So, the learning objectives. As I said, we're going to talk about supervised learning today. So, now we're sort of in this sort of really core of sort of our full pipeline in this course, where we have worried about data and we've worried about data preparation and how to do similarities and also probabilities from last time. And so, now we're ready to actually talk about the core data modeling aspect. And of course, then on top of that, there's going to be a lot of evaluation also based on what we talked about in terms of visualization a couple of weeks ago. 

Okay, and then of course, you're going to put all of this together in your projects. But today, supervised learning, we're going to kickstart that, so to speak. And we're going to talk about that in the context of regression and classification. 

So, a few intuitive simple examples. And we're going to be able to evaluate classifiers. Today, it's going to be a relatively simple evolution. And then we're going to build on statistical ways to actually compare two different models, for example. Not next time, but in the week after. So, next time, you're going to hear about how do we actually need to split our data and stuff like that. 

And I'll try to motivate that today. Okay, then we're going to look at specific methods, namely decision trees and a specific algorithm here, HONDS algorithm. And then we're going to also talk about specific linear models for regression and classification. So, that sounds like a lot, and it is to some extent. I'm not going to go into depth with everything, but I'm trying to give you the overall just of things as usual. 

And then I expect you to either ask me or ask via piassa or ask the TAs if you're in doubt. Okay, so supervised learning first, then decision trees, then regression trees, and then linear models for regression, which we call linear regression. Then linear models for classification, which we call logistic regression. So, this is confusing to some. Just accept that logistic regression is something we use for classification. And I know that sounds confusing, but we have to live with that. That's terminology. So, I'll go through all of this. 

Good. So, we're in the supervised business the next many weeks, five, six weeks, I believe. This is where we observe something about the world. We typically call it X here. And in this course, you can think about as vectors. So, points living in a vector space. And we know how to deal with points living in a vector space. You use linear algebra. And we're also going to talk about an output. That means you can think about this as our flower example, where we measure four different things about the flower. And now we want to classify the Y, namely which type of flower is it based on these four measurements. Or it could be generally, let's say, measuring aspects of a patient's physiological number of red blood cells. Or it could be, I don't know, blood pressure, these kind of things. 

And you want to classify are they ill or not? Or do you want to actually predict the temperature from those measurements? So, there are many applications that falls into this very general, let's say, framework here. You observe something about the world. You want to classify something about that observation. 

Okay. So, the general setup, and we've seen this before in various shapes and forms, is that we've got our input vector X. We talk about a model in machine learning, a machine learning model, typically. It's an abstract function to some extent. And in the next coming weeks, we're going to see what kind of functions can we plug in here. And now I'm going to outline a few today. 

But of course, these functions are parameterized by something. And today we're going to call it W, at least for the linear models. It's a little bit less clear what it is for decision trees, but we can talk about that. And then we have a certain output. So, in the regression, sorry, classification setting, this is going to be a discrete category. So, it could be like we talked about before, are you ill or not? Or it could be the iris flower type. So, that's a categorical variable there. Four different options in that case. 

Okay. Then, yes, so we have our model and a lot of machine learning, at least core machine learning is about that. Figuring out different models and how specifically how they work in different contexts, and how do we actually evaluate them. And actually to figure out how to train these models, that means figuring out the Ws, we typically come up with a general sort of term called a cost function or loss function or error function. 

Basically, it means the same. We want to minimize something to find the parameters. And we write that up in terms of a dissimilarity between our observations. That means what you have observed in the world and what your model predicts. Now, we're going to look at, okay, is your model actually trying to, or is it capable of predicting whatever you have observed? 

Like our Ws. Is our model actually able to predict what is the class of a new flower given that you come with measurements about that flower? So we write up this cost function and we're going to see a few today where we measure with D, similar to what we talked about before. We measure the difference in some way between our observation classification. We're going to look at that in terms of the Bernoulli likelihood that we talked about last time. 

In regression, we're going to talk about this in terms of the normal distribution or the squared loss, as some of you might have heard about. Okay, so this is the overall framework. Fairly simple, but of course the devil is in the detail. Okay, so we're going to start out with classification and we've already alluded to aspects where you may want to actually classify stuff. So imagine you're the Google and let's say Facebook and Amazon or whatever they are. 

Let's say Facebook and Twitter or X or whatever it's called these moments. So when people upload something like an image, they want to internally classify is this actually misinformation or not? Is it spam or not? 

I guess misinformation is not a big thing at the moment, but at least we can think about spam or not. You can classify whether or not is the dog in the image or cat in the image that you have just uploaded, meaning should it go on one's timeline or not. So there's sort of a predictive element of this, meaning that when a user brings something new, a new observation, like for example, a new flower that a botanist has gone out to measure, are you actually able to predict the correct class for that particular new example? So that's typically called, or at least here we call it predictive modeling. And also think about forecasting, but that's typically in the sort of sequential time series domain you would call it that. But here we call it predictive modeling in the sense that you want to build a model that can do well on new data that you haven't seen before. Now there's also another aspect of why you want to actually do classification or build classification models and that is to actually understand and explain the world you live in, meaning that if you measure stuff about a particular Irish flower, one of these four geometric aspect, are you actually able to say which one actually is the one that discriminates the classes the most? So that's about understanding how the attributes contribute to the final classification. Of course, maybe for the flower example, it's not great, but you can think about in the medical domain where we talked about measuring blood pressure, number of red blood cells. 

There you may want to actually figure out, okay, which of these many attributes that we measure about a patient is the one that's indicative or mostly indicative of you being ill or not. So this comes very much into all sorts of classical statistics. But in machine learning, we are a little bit agnostic to sort of what you call things. We just do, we have these specific tasks and some of them are relevant in some cases. And maybe for others, the predictive modeling is more relevant, for example. 

So the predictive modeling is probably what people mostly sort of associate with machine learning, but the other aspect, descriptive modeling and understanding things, it's very much into this whole explainable AI that some of you might have heard about these days. But anyway, overlap with statistics here in case you're in doubt. Okay, so of course, some of the methods we're going to talk about today also overlap with statistics. So regression models is very much a statistical concept as well. But final word, they are maybe not always particularly interested in the predictive aspect of the model. They're more interested in the descriptive aspect. Good. 

Let's talk about the first. So let's say real machine learning model that we are going to look at here, which implicitly defines a function. So I guess you know the game, 20 questions to whatever your friend or the professor or whoever it is. So simply by asking very simple questions about some concept or in a game like setup, you can actually very quickly narrow down what is that person you're asking questions to? What are they actually thinking about? So in this case, it's sort of the setup is that you are thinking about a particular tiger in the end. 

So one of you is thinking about a tiger and I ask you 20 questions to figure out what type of animal are you actually looking at. So if you think about this, this is actually, well, you can argue it's a more or less random way of asking questions. But I think if you actually played this game, you're not going to be random about it. 

You're going to think about, okay, I'm going to ask this question. Then afterwards, well, if it's an animal, well, then you've already said that it's probably not a, let's say, a human on this. Well, it's not a car at least. 

Let's put it that way. So you've already narrowed down the type of object that person is thinking about. And then you're going to ask questions that are relevant to that sort of concept that you have already narrowed it down to. 

So this iterative of a cursive way of asking questions boils down to a particular algorithm called a hunt algorithm and it gives us what's called a decision tree. So here you can see a tree and it's literally a tree here. And of course, that's not how we represent stuff in in our programming languages. But nonetheless, you can basically think about this as building the tree. You need to build the tree by asking certain questions. So we started the route here. Sometimes you'll show it upside down just to confuse you, but it will start at the root here where we have all the objects in the world that we interested in. 

And then now we're going to ask a question. And of course, I mean, you don't have people in your setup. So the way we ask questions in machine learning is typically going into a data set. So here we've shown a data set. You have, I think, 15 different animals. 

You have various attributes. So one, two, three, four. And then you have the particular label here. And the task is to classify whether or not that animal is a mammal or not a mammal. 

So essentially what we do here is we want to build this tree and we start at the root. We have all the mammals there, all the data points that we ask is the particular mammal cold blooded. Then you go into your table over here and you see, OK, which ones are actually cold blooded? 

OK, so if it is cold blooded, the one animal you come with here, then it goes into this leaf here. And it turns out, well, then you are non-mammal. It means simply by looking at the simple question we have already determined that stuff here should be classified as non-mammal, at least. Namely, the majority of people, I should say, in this leaf are non-mammal. Hence, everything that falls into that leaf should be classified as non-mammal. 

The same, well, now you sort of split the number of, or you split the question, so to speak. So if you end up here, you have already determined you're non-mammal. If the answer to that question was no, then we go one further level up and we ask the question, do you lay eggs? 

Basically, you're asking the animal, so to speak, by looking at its properties. If the answer is yes, you end up in this leaf over here and the other way, you end over here. So how should we actually build this tree? And that's what we're going to try to do now. So we start out, remember we had 15 animals here and some attributes for each of them. 

It's not so important which is which because we'll show the numbers that are needed here. But we start out at the root and then we say, okay, well, all the data objects that we have in our training set now, that means from which we can actually build our tree, well, they must belong to the root. And five of them turns out to be mammals. If we look at this, these five here and the other 10, they are non-mammals. Okay, so now they're all at the root. Okay, then we find a good question. 

And I'll tell you how to find a good question in a bit, namely that you need to find a good question, a relevant question, informative question here. But for now, I'm just randomly or arbitrarily, I should say, I'm choosing to ask a question, are you cold-blooded or not, like we saw in the final tree. And depending on whether or not that's true for each of the 15 data points, they end up in one of the leaf nodes here, which we'll basically do by partitioning the animals we have here based on the response to that question here. So in this case, you see you get five animals that are mammals, two that are non-mammals. And in this leaf node over here, this branch over here, you get zero that are mammals and eight are non-mammals. Okay. And then you build that leaf based on this partitioning. 

So in this case, we are now to some extent done over here because we have what we call a pure leaf or a leaf node with zero impurity, meaning that if you fall over here, you are guaranteed to be a non-mammal. Okay. How about the other one over here? 

Well, here you're not sure about if you actually go over here. You're not sure whether or not you should be classified as mammals or non-mammals because there's a mix. There's a majority of mammals, so maybe you could stop here and say, okay, well, everything should be mammal if they fall into that branch. But anyway, let's try to continue here. So I said, if you notice the final tree, so we ask one more question and do exactly the same. Now, for these seven animals, I'm going to then partition them based on their answer to this question. 

Okay. So that's what happens here. So here I've got five animals that goes over here. So now I have a pure branch over here, meaning that there's no confusion about what should happen. And over here, I've got a pure branch as well. So over here, non-mammals, over here, mammals. And now I can finish my tree because, well, in this sort of cartoony setup, I now have pure sort of leaf nodes, so to speak. 

And also, to speak, I have pure leaf nodes in the sense that all the animals in that node, like green stuff here, they come from one of the two classes. Okay. Good. 

So now we build a tree. But I sort of said, okay, let's just pick an arbitrary question and ask that and see what happens. So how do we actually pick out these questions, the good questions? Well, there's a particular algorithm for that, and it was on the previous slide. It's called Hunt's Algorithm. This is essentially a recursive algorithm where we try out, forget about what goes on here. 

So the key stuff is here. So basically, you try out a number of different splits. That means different questions. So at each node, you ask all the possible questions you can, potentially. And then we compute something called a purity gain. That means if I ask this specific question, how much pure, by some definition, I'll come to, how much pure do these leaf nodes become? 

Or the branches, how do they become? And I, well, I ask a bunch of these questions, or at least I try out a bunch of different questions. And then I pick the one that gives me the highest purity gain. 

Okay. So that is essentially what's going on here. So if you look through this algorithm, aside from this, the only other thing is that there's a stop criterion. So I need to stop at some point. So, and that is essentially what we talked about before, that you can stop if you have a pure leaf node here, then you should probably stop. There's no reason to continue and split just to get every single animal assigned to its own leaf node. 

All right. So when we have to determine this step here in Hans' algorithm, rhythm, then we actually have to figure out which of these two questions should I ask at that particular level. So you can sort of maybe intuitively have a look at this example here. So here I'm asking two different questions. 

Of course, the root node is the same, right? So over here, what is going on? Well, it seems like if you just look at a number of examples that goes into this sort of branch and the ones that go in here, then I have, I don't know, five and 10 over here. So, well, that's at least a discrepancy here in sort of the number of animals that goes into each branch. So it's not balanced in that sense. Is that a good thing or a bad thing? 

Maybe not. But then we actually also look at there's some confusion here, right? If you go into this branch, you're still somewhat confused about what should happen here. It's a little bit better over here. 

So is this a better question than this one over here? So if you look over here, you get a pure branch here, meaning that there are only mammals in that sort of branch here. And you get one that's almost pure in the sense that there's only one example that's not a, sorry, that is a mammal. So there's an element here of, well, is how many do you assign to each branch? 

And how pure is the resulting branch or partition? So how do we actually account for that? Well, that's something where we define the purity gain now. And it defines how well, how good is a particular question or how we were also generally called it a split. And we consider only a binary split for now. So with a binary split, we have the root and then we have that corresponds to a partitioning of the dataset. In this case, every single animal is in that partition. And then a partition up here where we have, let's say, ended up at five of the animals and over here, 10 of the animals. Okay, for a particular split or particular question. 

So how do we actually work out how good this is? So we're going to define what's called impurity for each of these partitions or each of these, let's say, branches, if you will. So we're going to call this IR in this case, so the impurity of the root, impurity of this vertex and impurity of this vertex here, or the branch, essentially. 

Good. So what is impurity intuitively? Well, I think we've already covered that for some reason, but try to think about it. If you have an impurity of zero, that's low, it must mean then you're pure to some extent, meaning that there's only one type of animals in that partition. If you're very impure, then maybe it should be 50-50 between the types of animals. So mammal, non-mammal, five of each. That would be a very impure branch or split. Oh, sorry, partitioning, I should say formally. Good. 

But I mean, it doesn't help a lot. If you have a very pure partitioning where over here you end up with, let's say, just one mammal, and over here you end up with the other 14 ones, and they're still very confused about what it should be. So we also need to somehow weigh that impurity with how many examples did we end up with in each of these splits. 

That's exactly what's listed here. So we have the impurity gain. Oh, sorry, purity gain. Yes, it's how much pure are you getting? 

That's defined by the impurity at the root. And now we are subtracting the weighted impurity for each of the partitions. So that means we account for this, let's say, case where we end up with only one example in one of the partitions. It's very pure, but essentially it's not really helping us a lot, because for the other partition we are still stuck with, let's say, example 14 animals, and we still have to ask a lot of questions. So have a think about this in two, Julien. 

Go ahead. So the question here is for everyone on Zoom, the question is why is there sort of a sum here for K2 from 1 to 2? And the only reason it could be a general setup where you have more than two branches here. So that could be three or four. I'll talk a little bit about this. But for all intended purposes in this course just think about binary splits. So yes, no answers that you have to ask at each level. 

But good question. But what I haven't told you is how do we actually define this impurity? I gave you some intuition about, OK, maybe lower impurity is good. 

Zero is good, whereas higher impurity, then at some level in the tree, you're still very confused. So how do we actually define this? And now we're coming back a little bit to these probabilities that we talked about last week. But there are three of them here, and we're going to work a little bit with them going forward. But the key thing here is that in order to actually compute either of them, let's do the genie one here, you actually have to compute this thing here. So remember, we compute one of the genie of V here. We need to compute that three times. One for each of the partitions here, the blue partitions. Oh, sorry. 

They're not blue. But for the root and for V1 and for V2. So what goes into this one? It says the probability of a class given that you are in that particular, let's say, partition here indicated by V. So V could be the root. 

It could be V1 or it could be V2. So now we need to figure out what's actually the probability in each of these partitions, or each of these branches, of you belonging to mammal and you belonging to non-mammal in each of them. That is basically a matter of counting like we did with the fruit and the color of the basket last time. 

We essentially just counted how many times did something occur and then divided by the total of that. So you'll get to do this now and then we are going to go through it in some detail to make sure you understand this. And just to make sure. So some people asked that I actually made these available. So I think just for the sake of, let's say, consistency or whatever it is, I'll make all of these quizzes available going forward. So I think for me to also understand what you actually get about this, then try to actually answer them by going into here. Or you can, for some extent, also do it afterwards. 

But please try to do this. Then we can sort of keep track of, you know, what are the misunderstandings and stuff like that. So I'll give you six minutes for this. Okay. 

I'm going to count in my small statistic here. Into DG you learn, I mean. All right. Okay, so I assume that the 10 people who answered this, which is not a lot to be honest. So do try to do the help me do this, so to speak, because it is informative for us to see if you're actually able to solve this in this relatively short amount of time and without a lot of preparation here. So please try to put in your answers to these quizzes going forward. 

Okay, so the correct answer is A. And we did it in a sense here that I just told you use the equation. And then I'm going to build a little bit of an intuition about what these are. 

It's not much, but anyway, so what we have to do, first of all, for all of these measures we have, and especially for the genome one, we have to identify these PFC given V. That means for all, well, for all three of these branches, we have to actually figure out what's P of V for all of them. And the way I'm going to do it to save a little bit of time on me not having to write everything is that I put that on a slide here. So let's go through it. So we need to identify these guys because they sort of enter here. 

They must be important somehow. Okay, so I'm looking at first what's the probability of mammal given that I'm in the root. Okay, so I'm in the root branch, so to speak. What's the probability of you being a mammal here? So if you are actually classified just by sort of the split here by majority, well, you would say most of them are non-mammals. 

Okay, that's the classification. But what's the probability of that? So now I said non-mammal there, but that was a mistake. But the probability of mammal given that you're actually down here, that would be five over 15. So we have five mammals, 15 in total, so five out of 15. That gives me a probability of one-third. What's then the probability of non-mammal? And without actually counting anything, you should be able to figure this out based on the rules we wrote out last time. 

Which rules should I use here if I'm not using this? Exactly. We should use the sum rule. Everything on the left-hand side of this conditioning sign should come to one. We have two outcomes. 

We have mammal and non-mammal in terms of the random variables type of animal. So we could just say one minus one-third here. That might save you a little bit of time actually knowing these small things and applying that. 

Anyway, we could also do it brute force. So what's the probability of non-mammal given that you are in the root? Well, 10 out of 15 are non-mammals, so two-thirds. 

Good. What about for the other ones? Well, up here we have one out of five that's mammal, so we have probability of one over five. A conditional probability of one over five. And probability of non-mammal must then be four over five. And then we have the probability of mammal in the last vertex here, in the last branch. The probability of mammal given that you're over here is four over 10. 

We have 10 in total and six over 10 for the non-mammal. Okay. Now the tricky part. Now we actually need to compute the impurity gains. 

But anyway, if you just look at this intuitively, which one of these branches is the most impure? Just intuitively. I'm not asking you to plug them into these numbers. But I mean, if you look at them, okay, this is certainly not pure, right? It's not like it's zero mammals and 15 non-mammals. So there's some impurity there. 

If you look up here, well, it's almost 50-50. So that's certainly also impure to some extent. But this one over here actually seems rather pure. So, well, the impurity must be lower at least compared to the other ones here, just intuitively. But of course we have some strict numbers to compute these. So let's try to do that. 

So the genie one is given up here. We need to take one minus the probability on the first class, c equals one here. Mammal given that vertices and then squared. So I have for root, I have one, that's this one, minus the first term in this sum. Okay, that's the probability of mammal given r squared minus, well, it's a sum, but there's a minus sign outside, so it must be minus again. And of course the remaining, it's binary, so the remaining part of the probability here. So two-thirds squared. And then that gives me a number. 

And I do this again and again. So I get three numbers here. Let's see if that matches our intuition here. So we said that one of them was pure, so the impurity must be low. That was this one over here. 

So we won. It has impurity of 0.3. So the measure somehow, okay, it's lower than the other ones as you'll see. So the root, it's, I don't know, it's difficult to compare, right? But 0.44. 

And then we look at this one over here. That's 0.48. So at least the relative ranking of them seems logical from just looking at the numbers. 

Make sure you get that intuition. Okay, because now we just blindly plug it into our equation. And why remember this equation here? This is where we take the impurity at the root minus the impurity at each of the branches. But we weigh it by the number of points assigned to that branch. This is to make sure we actually account for, yeah, exactly the number of points associated with that branch. And we can see here that if we just subtract it, let's say this one here, this one seems to be much lower. So if we had the majority of all the points over there, that would have, let's say, contributed more. 

But we only get in branch one five out of the 15 animals. So this low, let's say impurity is not going to contribute a lot to the overall reduction in impurity. So at the end, we end up with the answer A. The question here is, so make sure you can do this relatively quickly. The question is, is that actually better than another question here? And as it turns out, if you consider another question here, you would actually get an impurity gain. 

That means something that's good. You would actually get that this question is better than the other one. This one is the one we just computed. So it was 0.01 or something. And this one is much, much higher. 

And maybe intuitively, that also makes sense because these splits seems way more pure, even though, of course, it's still unbalanced to some extent, but now they're both actually pure, the two partitions. So I didn't get the question. No, so the question is whether or not it's a percentage and no, it's not because we are not actually computing fractions. So remember that this impurity thing here, that's something that's a little bit complicated to actually interpret. 

So you can't talk about in terms of percent, I don't know, improvement or something. To be honest, you could normalize this and make it into that, but that's a little bit beyond what we want to do here, but you could do that, definitely. All right, so, okay, we get two different splits, and one of them turns out to be better than the other one. So see if you can actually reproduce this number here. 

So overall, we consider each of these points or level in the tree, we consider a number of splits, that means questions, and then we compute this measure of impurity for all of them, all the splits you consider. And of course, that can be a little bit, let me just fix this before I go crazy. Okay, so that can of course be a little bit computational intensive, but that's fine. Okay, so, okay, anyway, I hope that's relatively clear that at each level in the tree, consider different questions, compute the purity gain of each of these questions, pick the one with the highest purity gain. Yes. Now, back to sort of what these measures actually measure. 

These are Gini and entropy, and what was the last one? What do we call it? Class error. 

It has different names, but we call it class error. Okay, what do they actually measure? And to some extent, they measure the confusion about what's going on in one of these leaf nodes. That's intuitively what they measure. 

They do it slightly differently. Some talk about entropy coming from information theory, you would know a little bit about that. But Gini then is some slightly odd thing here, it has a difficult to interpret. So what we can do is actually plot them as a function of the probability of, let's say, class one, given whatever branch you're in. So let's say C could be in zero and one, so you still have two options. Now I'm giving you as a function of the probability of one of them, I'm giving you what is the Gini, what is the class error, what is the entropy. And it might be a little bit unsatisfactory, but it's difficult to say which one is better. You would have to actually try to do it empirically to work out. How do we actually encode, sorry, how do we actually determine which of these measures is the best one? As it turns out, selecting what we would call a hyperparameter here or some aspect of the model is going to be very important going forward and we're going to talk about that next week. How do we actually do this empirically? Anyway, we talked, your colleague mentioned that basically maybe there might be other splits than just purely binary. 

He has no answers. Yes, there are other types of splits here. And the one we're going to look at mostly in this course are binary ones, so he has no answers, but also for continuous variables, we're also going to look at sort of a threshold. And that's exactly what we look at next, meaning you can also split on a question, whether the age is above or below a certain threshold, like in this case, is the age below 30 or 40, then you put people in different categories. 

Obviously, I go in the less than 31, but I don't know about risk. Okay, then let's actually try to look at an example where this happens. So this is a little bit of a complicated graph here, a plot. So you should think about this as a two-dimensional data set. So I have x1, so that's attribute one. 

I have x2, the value of attribute one. And then I have something going on here. And what you can see is that we have some red points and we have some blue points. So all of these are the coordinates or the positions I'm showing here are, of course, related to their coordinates. So I have a coordinate of, I don't know, 0.5 something and 0.2 ish for this data point here. So all of these coordinates are rows in my data matrix. 

Then along with that observation about the world, the coordinates, I also have a class to which the data point belongs. It can be blue or it can be red. So don't worry about the background for the next 30 seconds here. Okay, so I know the ground truth. It can be red or sorry, it can be blue or red. Then I also have some hypothetical function that I've come up with where maybe a good way to actually in this 2D space, separate whether you belong to one or the other could be a green line like this. Maybe that's a good one. 

And I'm just stating that now. Maybe that's a good line. We typically call it decision line. So if you're on this side, that means over here, on the green one, ah, over here on the green one, our model tells us you should be classified as blue. So you see down here that maybe there are a few mistakes here. Yeah, for example, we have a ground truth of red here, but because it's on this side here, it should actually going to be classified as blue. Okay, however, the way the background is colored here is not by this ideal green line decision boundary. It's actually by this split here. 

So I have now built a very simple decision tree. It looks like this. It tells me that if you are this node and you go down this one here, then you should be classified as red. 

And if you go down this one, you should be classified as blue. However, we have not told you what is actually the threshold. Oh, sorry, what's actually the question we are asking here? What we are asking is essentially about 100 Christians lying here along this line, whether you are below or above that threshold. So the question like does it have legs or whatever it is, corresponds here to saying are you below or above that threshold? 

So that's exactly what we are doing now. The question now is are you below 0.45 or whatever it is, then you'll be classified as red. If you're above, you'll be classified as blue, indicated by the color of the background. Okay, so the plot is a little bit confusing because it contains all of these things, but I hope that made sense. But okay, what I've done here is make a so-called axis-aligned split. I've split along this value here, below or above, but how did I come about that? Well, as I said, I tried out 100 different ones. 

This one turned out to be the best one under the assumption that it's an axis-aligned split undoing. Now, of course, this is not a very good classifier, right? Because I'm classifying all of these wrongly. They were supposed to be blue. All of these are also classified wrongly. 

So it's not a very good classifier. Let's try to grow another branch. And if we do that, it turns out that the question, so I'm asking for basically this branch over here, I'm asking, okay, a lot of questions along this line and then picking the best one. And it turns out the best sort of answer I could, or the best question I could ask, is basically along this x2 axis now. So here I have to ask, once I'm down here, right, that's what's going on in the first question. I know I'm down here now. Now I have to ask the question, are you now above or below this threshold? 

So that's the question I'm asking here. And likewise, over here, if I go down this route, that means I'm here on the x1 axis, now I have to ask the question, are you above or below this line to indicate whether you should be blue or red? Okay, so if you're above, you go down this route down here. If you're blue, that means you're below this threshold along x2 and above this one in x1, you go down here. So of course it gives you a certain way of interpreting what's going on here in terms of the values of the attributes. Good question. 

So that was a little bit confusing, I think. So how do we actually determine these thresholds? That is exactly the same as determining what questions should you ask about the animal. But the type of question you can ask now is about a continuous variable or attribute. And a natural one to ask is, are you above or below a certain value? So I'm trying out 100 different ones along this line, computing the impurity gain for all of them, and then I'm picking the best one. So it just turns out that these were the ones that seemed good. And as you can see, we are sort of getting there. 

Does that make sense? So if you continue this game, you can imagine, actually continue this game forever. Oh well, not forever, but until we have basically a tree where each individual data point belongs to a node. 

You don't want to do that, and I'll show you why now. But we can continue a little bit further and then if you're down here, if we ended up over here, we now determine which split is better up here. It turns to be this one here. So now we do one more step here. So now we already have a fairly complicated tree. You see the decision boundary we have now is this one. 

So of course it's not this idealized green one we have available. But actually it's not doing a terrible job, right? We might get some wrong, but that's okay, trust me. You don't want to do well on all of your training data. 

So what happens if you actually continue for some, I can't remember how many steps here, but at some step, you end up with this decision boundary, which is really, really complicated. You can think about these as functions to some extent, right? There's a function implicit in this tree here that defines this line. This is a very complicated function, I would say. And why is it maybe not a very good function? Think about this for two seconds, then I'll maybe focus a little bit on this. 

So you've...let me see if I can zoom in here. But you have a very, very specific rule for a few points lying here, the blue one lying here. But actually in this area, I mean, you would expect that the class would be red, right? So you basically overfit it to this one data point that just happens to lie here. 

So imagine that some botanist made a small mistake with his ruler, and then you actually think that the rule that you're interested in is something that looks this complicated. So what we're seeing here is... So what we're seeing here is an effect of overfitting, and that's going to become a theme going forward here. It is that even though you could potentially create a function, in this case, that is very, very complex, and can come up with a rule that does correctly on these data points that you have observed and trained your model on, the training set, you definitely don't want to do that. 

You may want to actually stop at this point here, because that's a reasonable decision boundary that you can expect will work on future data. Then there's a question about what goes on with these blue points. Well, sorry, you have to live with that, that you're going to make a mistake on these, and maybe actually they were labeled wrongly. 

So maybe there's some noise associated with these blue points, either due to the value of the x-axis, or because someone, an expert, just gave them a wrong label. So you definitely don't want to train to the training... Oh, sorry, you definitely don't want to overfit on your training data. That's generally not a good thing. Yeah, so you can imagine... I've worked in machine learning... Sorry, the question is what's actually the green line? I can tell you that the green line is the line that's going to generalize best on new on-scene data. So this is basically the model you should choose if you knew what you were doing, but we don't know what we're doing yet. 

We have to suffer for a few more lectures, right? So the idea is that this is my idealized green line that will give me, on average, the best generalization error. That means it'll do the best on new data that you have actually not observed before. So when I come with a new data point, I ask someone to go out into the world and give me a new data point. 

It could be here, here, here, here, or here, or here, but in general, then, on average, that's the line that's going to do best. So the question is, how can we now actually create models, I guess, in general, that doesn't overfit? That is the whole thing about this course that you're going to learn how to do that. 

So you have to come and see George's next week talking about this, but I'll give you a hint at this now. But what I've talked about here is that we are only looking at the training data here. How about we actually bring in some more data that we haven't seen before, and now we're going to evaluate our model on that on-scene data, and then we can try out different models and see which one is the better one. But of course, it also has some, let's say, there's some aspect of complexity. 

Can we actually build this model? And that comes down to a choice about the model that you're using. Are you using a decision tree for this problem? Maybe that's not the best you can do. Maybe you can use a different type of model, which we're going to see in a minute. All right, then just a few more things before we have a break. I think, oh, actually, I'm going to go through a small example here. But anyway, you can think about how are we actually going to evaluate this classifier that we have built. So remember, we build a classifier. It might not be a very good one with the red decision boundary here. How do we evaluate if it's doing well? Well, we can evaluate that on the training points that I've shown you here. 

But I can also, of course, do the same on some monosin data set, which we are going to do next week. But just imagine here, you simply count how many times did you get the numbers correctly, meaning that if you are actually a mammal, we can create a small table here, sorry, where we have the actual value of the class, let's say the mammal one, for example, and the non-mammal one, and then we have the predicted one by your model. That means are you on either side of the decision boundary? And then we simply count, OK, I know that this data part is supposed to be a mammal. What did our model tell us? Well, if the model agrees with the ground truth, so to speak, we add this or increment this value by something. And similarly, for this number down here, we can have that, well, the actual, let's say, class was non-mammal. What did the model predict? It predicted non-mammal, so we increase this. And of course, these are, let me just put some names on them, because I think you might actually need them for some of the homework problems. These are generally called true positives, if we think about mammal as being positives here. 

And you can look into chapter 16, maybe. There is a chapter in the book talking about class imbalance, where you can look up all these definitions. So this would be called true positives. This one, true negatives. 

So what do you think we call this one over here? False negatives, yes. And then there's a false positive here. And again, we're going to look a little bit more at going forward, but I think you might need it for one of the homework problems. I'm just mentioning it. But for today, what we're interested in is the accuracy. And the accuracy is simply the number of times you did correctly out of the total number of examples you have. Similarly, error rate, that's basically in terms of percentage, 100% minus the number of times you did correctly. This must be 20%, but you can compute them, summing these two numbers and divide it by the total number of elements. Okay, so this is relatively simple for now. 

As I said, in a few, later in the course, we're going to look at how we can derive slightly more informative measures based on this so-called confusion matrix. But for now, this is what you need to know. Good. 

Then a small example, and I've used that throughout. Namely, we are back to our flowers. We measure four things about our flowers. And we also have actually a class now. 

Good. So how can we build a decision tree for that? And this you probably can't see, so I'll walk you through it. So I've trained, as it turns out here, a few years ago in MATLAB, we've trained a decision tree. I'm showing you the tree going from top to bottom, so I flipped the tree, so to speak, like we did for the previous example. Good. And then I'm basically going to ask you, someone goes out, you go out and find an Irish flower, and you measure these four things about the flower. 

Now you're wanting to actually apply that train model to your Irish flower. So what's going on here? Well, I measured four things, and I have come up with some questions that are exactly similar to the thresholding questions we asked before. Namely, are you above or below, or above a certain value? So the rule here, let me read it out, is that the petal length should be lower than 2.45 for you to go down here. It should be above, obviously, or equal to 2.45 for you to go down here. 

I don't know if you can actually read these numbers. Okay. Good. Well, then I don't have to read it out loud. That's good. So let's have a look. The first rule I need to, or the first questions I need to deal with is petal length. 

Petal length was three. Is it below 2.45? No. So I go down here. 

Good. Then petal width, that was over here, it was two. The question is, is it above or below 1.75? It's above, so go down here. 

Good. Then petal length, three. Is that below 4.85? 

Yes, so go down here. Then petal length, five points. Is it less? Is it below 5.95? Yes, so go down here. 

So what happens down here? In this leaf node, at least while we are training, it turned out there were more iris flowers of the type versicolor than any one of the rest. So that means we are now going to classify this new point as versicolor down here. That's the class we're given to that so-called test example we came with here. So now we've applied, well, we trained, someone trained a decision tree. 

You'll do that in the exercises. Now we applied it to a test example. And the thing here is that arguably this gives some insights into which attributes are actually important for this classification. You can understand what's going on to some extent. Now I'm just going to warn you a little bit, and that is these trees, depending a little bit on how you train them and there are some random elements in this one and how questions are selected and so on, then the trees can actually look quite different if you retrain them. 

So that's something you need to keep in mind. If you retrain your model, there's a stochastic element in the training process that might cause you to get a different tree. And the randomness of stochasticity in this case is simply at random, I select a question to ask, or sorry, an attribute to ask a question about. That could be one element of stochasticity here. 

Now how can we actually, in this tree here, you see, it looks kind of complicated, right? And we saw that it was actually fairly quick to actually overfit. So how can we avoid that? Well, as you'll see in the toolbox or in the exercise story that you're using today, what you can do is actually limit the number of, let's say, levels that you're choosing. So you could cut the classifier here and say, okay, I'm simply going to classify based on that, or actually have it not trained to a deeper depth. I can also put in other criteria such as, I should not continue until I have a pure branch. It's okay, there's some confusing in some of the branch. 

I'll just use a majority vote saying that I'm just going to assign a new test example to the majority class in that leaf node. Okay, so just be aware that there are various ways also today to actually avoid getting into this trap here of coming up with way too complicated models. And there are some arguments you can give to your code to avoid that. The formality of that is for next week. 

Good. Then there's a break, and then I'll tell you a little bit about linear models, and then you go to the exercises. Short break. 

Audio call. All right. Have a seat. 

All right, guys, unless you want to stay here until five, then let's get moving. So there's a few comments about this tree and what should we actually get from it and why doesn't one of the features actually enter here? Let me just check. Yeah. So this is one of the sort of, let's say, special things about tree and that's exactly what I mentioned. If you retrain it, depending on what sort of elements of randomness is in your training set up and what you allow it to do, then you can get different trees if you actually train it again and even if you just leave out one data point, you'll get something that's totally different. 

So just be careful here and try to have a look at it during the exercises. That was basically, let's say, classification. And I'm going to come back to classification at the way back of the lecture. So one of the purposes here is also to make sure that actually in some cases it matters a lot if you're doing regression and classification or for some models, whereas for some models they easily just transfer to another setup if you think a little bit about it. So let's just revisit this. Probably don't want to write for that. So probably let's revisit the setup again. 

So it's exactly the same setup as before. You measure something about the world. You now want to predict something. And two settings. We have the classification that just talked about. One side of the other of the decision boundary. 

What type of ivy flower are you? But then we also have the regression case where we actually, where the y value variable is actually a continuous variable. So here it's not like yes, no answer, right? 

It's actually on a scale. You need to make a prediction. Good. So the setup is exactly the same as before. You just need to remember that the y's are now different. So we have a data set as before, pairs of x and y. 

That's basically what's going on here, right? For each x you have a y. And then you have a model. And here are written in a specific form. And the model here is an abstract function that's somehow parameterized by some weights. Of course, in the tree, it's not clear what these weights are to some extent, but you can actually evaluate the decision boundary. Then we still have a cost function. 

So nothing has changed here. It's just that a key element in understanding the difference between classification and regression is understanding the difference in terms of which similarity measure are you using? Or as I'll show you in a bit, which likelihood function are you using? Good. But anyway, you have all seen regression before, I hope. So the idea here is that we have a function. 

We want to learn in maps from an input to a continuous-valued output. Why would we want to do that? Exactly the same as before. Let's say take some observations of other world and then either predict for an unseen input. We want to predict what happens. So probably no one has measured your exact physiological measurements of blood pressure, blood cells, and age, and whatnot, and then try to predict if you're ill or not. 

So we haven't seen a specific example like you before, but we may want to make a prediction about that when you enter the hospital. So that's the prediction setup. And then there's still the descriptive setup where we want to understand which attributes contribute to that prediction. So if you think about that in terms of a simple example here. 

So here you have an x and you have a y. And the green one here is like before. It's sort of the nature's ground truth function. This is the one that's underneath everything here. 

We can't observe that directly, but we have been given examples. That's the blue dots here. We haven't been given examples from that function plus a little bit of noise. 

I'll come back to that. So typically when you observe something, when you measure temperature in here, there's a little bit of noise uncertainty related to that measurement. That's sort of what's illustrated in terms of the difference between the green and the blue, so the green line here and the blue dot. Then I have put in sort of a function here and that's our function. 

It's the one we have trained or trying to estimate. So it's probably not a very good choice here. But anyway, we can sort of put up a functional form of the red one. 

Now I shouldn't be using a blue thing here. So maybe for the red line, I could put up a function that says, well, I have an intersect. So that's a scalar value. It would be this and plus some weight times my value of x. 

This is the equation of functional form of a linear line. Yes, you have seen this before. Now this is, of course, one choice you could use here. But we probably need to look at something that's a little bit more complicated, even for this simple example here. So first, we are going to look at regression trees just to sort of wrap up what we did with decision trees. So you know, you can now also use trees for actually doing regression functions. We are going to look at linear regression, specifically linear models for regression. And then later, you're going to look at regularized linear regression or rich regression. And we're going to look at neural networks for doing the same. 

We're going to look at ensemble methods. So it's all one pick the best model kind of thing. And to some extent, machine learning is about that. It's about choosing the correct model. 

However, we are going to really build on, let's say, that the principles for actually selecting the correct model or the best model in some sense next week and the week after. But I have already indicated some of the issues here, namely that if you're only focused on the observations you've been given to train your models on the training set, then you might risk overfitting to that data because as it turns out, I can fit every single data point you give to me. If you give me a data set, I can make a perfect prediction on your training set. I can give you 100% guarantee of that because I know a model that can do that. 

If you want to know what it is, you can come ask me afterwards. But you can think about these very complicated neural networks that you've heard about. They can certainly also overfit to your training data. So we want to avoid that. So somehow between this simple line and something that's more complex, that's probably what we are looking for. But just a little bit of sort of entration about how we actually measure whether a function is good or not. 

So what we're going to do, I'm going to do it by example here. So we have the same function as before. So the green one is the one we can't observe, we just know it's there. And then we have observed some blue points here and I'm going to consider those the ones I'm using to find my red line. And this is not the best line. I basically randomly drew a line here. 

So it's an arbitrary line. I can still measure how good is it. And the way I'm going to measure this is that I'm going to take what my model says for this particular input zero. And I'm going to look at what I actually had in my data set, the blue point. I'm going to measure the distance between them. Now of course, distance between things depends on whether or not they are above or below. So I'm going to take in one case the absolute value of this. So if I take the absolute value of my data point, yi minus y hat i, where y hat i is the output of my function. 

So let me maybe write that. So y hat i is the output of my function for the i-th input. That's generally a vector, but of course here it's a scalar, right? Okay, so I can measure the things between what my model tells me and what I have actually observed. 

Good, simple enough. There are various measurements of distance and we've looked into that. We looked into the P1 or L1 norm. That's the absolute correspondence to the absolute difference. 

We've also looked into the P2 norm, which will eventually be equivalent to squaring the distance between the observation and the line, the red line. Okay, so if I have that for one point, what can I do? I can sum all of these up over all the possible data points I have here. So one, two, three, four, five, six, seven, eight, nine. 

I have nine terms in this function here. If I sum them all up and use the squared distance between things, I get the squared error, essentially. And if I take the mean, I get the mean squared error. So this is just a measurement. And all you have to do is think about the difference between what's in your data set, your observation, and what your model tells you. 

Okay, hopefully that's clear. So how are we going to use that now to build a regression tree? And I'm going to do this relatively quickly, but hopefully it'll give you some intuition about at least some motivation to think about functions as something a little bit more abstract than what you've probably been used to. 

Because the function I'm going to get out using a regression tree in this example is this one. So it's a red one here. Okay? 

That's a function. And the way I'm going to do it is exactly like I did before. I'm going to use Hunt's algorithm, but this time for continuous variables, so I need to think about how I measure impurity. But let's just start at the root down here. And let's say, actually, we need to find a threshold and ask a question, and then build up a tree based on that. So what it turns out has happened here is that, okay, I figured out that at some point in my tree, I need to ask, are you on this side or this side? If you're on this side, are you then in this interval? So if it turns out you're in this interval, you just get a constant value. Okay? 

And it's just the mean of all of the blue data points. Okay? So the way I then figure out whether or not a split here or here or here is good is to try out different ones. Here, I only tried out ones that are sort of at point 1 delta, so point 1, point 2, point 3, point 4. So the way I then measure is this thing here, a good splitting point. It's simply to measure the impurity of the new split. 

So let's imagine I actually split here. Then I need to measure the impurity of these points down here, 1, 2, 3. And I measure that based on the mean squared error when my function is the mean of those three data points. So it's just another way of measuring impurity for this regression case. But the point here is really think about the red one here in the sense that this is just a function. Is it a good function? 

I don't know. I mean, I've only evaluated it on the training set, right? So perhaps I need to actually come up with a new data set such that I can evaluate how it does on the new data. But anyway, for now, it's just a function. It's a quite odd function, right? 

It's piecewise constant. And the way we find the intervals is just using Hunt's algorithm by building a tree and then figuring out if a data point is actually in that interval. But at least I can measure what's actually the distance from the blue to the red point so I can come up with an error measure. All right, so that's regression trees. That's the only thing I'm going to say about them because I hope they're relatively intuitive. 

What we are going to spend a lot of time on now is what are called linear models. And they are perhaps a little bit more down your alley and something you've seen before, namely that it's exactly the same functional form as I showed you before. So again, we have x and here we have y or f of x. And here we have a 2D example of x1 and x2, so attribute 1 and 2, and then y up here. Okay, so if you look at this functional form that you have all seen before, if you haven't, you shouldn't be here. Okay, that's sort of a threshold question for you. If you've not seen a linear line before, this course is not for you. Good, but anyway, I have some blue observations for each, for certain x values I observed some points, some y values, so in here, I don't know, I observed temperature and observed humidity. 

What could it be? Good. Then you're trying to fit a straight line to that and we can eyeball that and it could be, it turns out to be this one, that's the best one by some criteria. It could also be another one, right? It depends on the slope and the intersect. What happens in 2D? Well, that line becomes a plane in a 3D space. Okay, that's what's shown here. So now it's a function of the intercept, so to speak, and the slope in either direction. And so we have three values to parameterize that function. 

And sometimes, just for clarity, we typically write that in this way, where we have a vector input x and a vector w. And I'll show you how we collect the weights and parameters in a vector in a bit. But who's stopping us? I mean, you know, we can just add, if you have more attributes, we can simply just add them in a similar way here. So this is probably what you've seen before. These are examples of linear models doing linear functions. 

Okay? And this is probably what you've seen as linear regression. However, there's a lot of things that people may not have told you about linear regression. That is, you can also do nonlinear functions. Because you can imagine sort of having this standard input for, if you have k attributes, what could we actually do? 

That's sort of the standard you've seen, I guess. What could we actually do to our input features to make nonlinear functions? Well, I could extend them and transform them, or do a basis expansion, as we'll call it. Well, what have I done here? Well, I took my, I only have one input x. Now I'm taking, adding a term here to sort of my standard linear line. Ooh, I'm squaring it. So I'm getting a polynomial term now. And you've probably seen this in the context of Taylor expansion and stuff like that. But here we're going to use it to define a function, simply by transforming my input. I can choose a lot of different transformation of my input, like a sinusoid or, I don't know, even a cubed transformation or like here. And I can also, of course, do that when I have more than one input x. So I measure both temperature and humidity and predicting, I don't know, wind speed or whatever it was, we used a couple of lectures ago. So I can transform my inputs to be able to do something like this. 

So here I've only measured one thing. But if I use something like, let's say this version here, where I have, let's say, up to second order terms, well, I can use, I can make a polynomial. If I use fifth order term, I can maybe do something like this. But I mean, no one's told me in machine learning that I can't do this. So this is just another function. And the whole name of the game here is to select the best function. That's what you're going to spend a lot of time on. 

Okay, so I put in something really, really important here. And that is that we're talking about linear models. We're not talking about linear functions. Yeah, go ahead. Like this. Okay, I'll answer your question a bit, right? 

Okay, so another example of this. We have the standard set up here. We have two x1 and x2. And we have the y here, or f of x. Okay, that gives, by this, we can parameterize a plane. Just something like this. But if I actually do something really crazy here, and trust me, we'll do crazy stuff later on in the course. 

But we need to, you know, do something sensible afterwards. We will fit functions that are way more complicated than this. But we're going to evaluate if they're good or not. But anyway, I can do it, right? 

And see what happens. So what happens here is that I actually get this curvy, let's say, surface in 3D space. So you might have seen this in math as well, but now you can actually use it for something. 

Because if the ground truth actually tells you that the curve should look like this, you may want to use a function that has the same shape. Okay, did that answer your question? Yes. So you can think about how crazy these functions can be, and they can be really crazy. All right. 

But it's really, really annoying to have to write functions like this. Right? I said we're going to do something crazy. So maybe even a hundred terms would be needed here. So how about we write them in vector form? 

That's exactly what we'll do now. So I'm going to write a sort of family of functions here, and these are the linear models in the following way. So I'm going to define a function f of x. Sometimes you put this semicolon w to make it explicit. That's parameterized by w. Okay, but anyway, it's sort of an offset, and then we have, let's say, a parameter associated with the first attribute, second attribute, so this is sort of in the canonical setup where we have k attributes. Okay, but if I look at this a little bit, then this is nothing more than the sum here of w times x of k, where I very explicitly say that x0, which I haven't observed, is actually 1. So I'm just putting in a constant here to make notation convenient. This is a really important slide in terms of equations. You should be able to understand what's going on, and you should be able to operate this, meaning plug-in numbers and stuff. 

Okay, but you've seen a sum like this. It's just an inner product between x and v, or v of t of x. So, hey, now we can use our linear algebra to write a function really, really efficiently. 

Can I do this also with, let's say, my nonlinear transform features? Of course. I mean, I can put a square onto this one. If I wanted to, it just enters into x. Okay, but this was for only one data point, right? 

It was for the, let's say, for x here. If I want to do that for my whole data matrix, remember we had this matrix, and now I'm going to put a tilde over it to indicate that I've somehow transformed it. So one way to, actually, let me do it in the vector form. 

How would this x vector look like? It should be a tilde on top of here, sorry. How would x tilde look like? Remember, I made this as something that w0 actually corresponds to a constant of 1. So my x tilde would need to be a 1, and then my sort of original x observations here. 

So x1, x2, up to the xkth attribute. Okay, if I now look at my matrix, so I'm going to put one line under it to indicate a vector. If I now look at my matrix, original matrix, it looks like data matrix. I have sort of x vectors lying as rows in this one, but now I need to put a, oh, sorry, it was this one. So now I need to put a 1 in front of that. What would that correspond to? It just means I have to put a 1 manually in front of all of these vectors. So essentially adding a column of 1s to make sure I can use this notation here. 

What would that allow me to do? It means that for, let's say, 100 different data points, I can compute the output of the function f of x. I can compute that simply by multiplying a matrix times my weights. That will give me my y's or my f of x's. Okay, so make sure you understand this notation. This is just convenience, but it allows us to very easily, for example, do the transformations of my inputs. So I have all my original inputs here as vectors. Now I can transform them in either way and just expand this vector if I wanted to and try out different functions. 

That's the question. Okay, so the question is, do we need an inverse function, blah, blah, blah? I'll come back to that. So I haven't said anything about how we actually learned the parameters. So I'll come to that in a few slides. 

Good. So here, just assume that, yeah, the parameters are there, the w's. Okay, but anyway, so the way I've talked about this already on this slide, right, so how do we use this linear regression or linear models in machine learning? So for the regression problem, it's exactly what we talked about before. We have some true function that we can't observe, but we can observe some data points from it. And we explicitly call these blue ones training data. Now, of course, that also eludes to another dataset called the test data, which we'll talk about next week. Good. 

So just to reiterate where we are here, so one function that I've already shown you could be one model type, could be one where we simply have a constant, and then you just move that up and down. I mean, why not? We can do that. And we can evaluate what happens. 

The same we can do for the one where we had a slope. And we can think about what's actually the difference from our model to the data points. So we can measure that using that squared error or mean squared error. We can also use the absolute error. So just basically the absolute distance. 

Okay. And we can talk about the residual error here. So that's just thinking about how, what are the errors, how are they actually distributed? So one thing to notice here is that the error is actually distributed somewhat weirdly, right? 

We say that our model makes a lot of mistakes in this area. Is that a good thing? Probably not. It means there's some systematic thing going on. And maybe we should think about using a slightly different model at least. Okay. Good. 

But if I now actually start doing some of this feature transformation that I talked about, and now I'm not writing it in matrix form just to make it a little bit more approachable here. So we have the same problem here. Now I've added sort of a second and third of the order term. Okay. 

That means I can do functions like this one, the red one. Okay. What if I continue? Like I said, we can do crazy stuff. Well, if I continue here, I get a very weakly function. 

Okay. Is this a good idea? Not a good idea. Yes. Obviously it's not a good idea. 

Why is it not a good idea? Yes. So the argument here is there's a lot of oscillation. But it's not the key point. I want to make the key point is that if I bring you a new data point, I go out into the world and measure something. Then if I bring your measurement here and remember, I'm measuring something about the green line, because that's what nature tells me. 

What would happen here? And you want to make a prediction now. So your model will make a prediction that's here. But actually that's quite a long way from the line here. Similarly over here and here and here. 

So the model you have defined because you know how to do crazy stuff and you know how to train the weights. Actually it only works on the blue points. It doesn't work on the test points, the red ones I have introduced here. This is key in machine learning. We are not interested in how stuff does on the training set, because as I told you, I can come up with a model that does perfectly on the training set. But the hard job is coming up with something that does well on on-scene data. Here the on-scene data are these four data points. 

Good. So maybe it's not a very good idea to just go nuts and add more polynomial terms here or higher order terms. Because when you are adding higher order terms to polynomials, you get correctly, you get oscillations and weird stuff at the boundaries as it turns out. So of course this is only in the polynomial family that happens. 

But in general the principle applies to other things as well. So if you actually look at the function here comparable, so one where you just have the standard linear function or line, one where you have sort of a sinusoid kind of thing with second and third order terms, and then one where you have eight other terms here, where it turns out you can fit the blue points perfectly. What if you look at the error? So if you sum up all of these absolute differences, all of these numbers here, up here you get sort of a relatively high number, here you get sort of a middle number, I think. 

And here you get zero, absolute zero. So you're doing perfect on your blue points for the training set using a crazy function, or crazy complicated function. Whereas there's sort of maybe a slightly more, let's say, sensible one, which is it can still do something that's slightly wiggly. You get something that's in between, whereas of course this model here is too simple. So it's underfitting as it turns out. So the main job in machine learning, or big job is actually selecting the correct model complexity here. And the thing here is that we're going to spend a lot of time on this, and I'm just giving you the sort of overview here that we need to worry about the stuff that's not in our dataset. 

That's what we care about, not our blue training points here. However, I have not told you anything about how we actually find these weights. I've just given them to you, or somehow hand-waved a little bit. So how do we do that? I'm going to show you one slide with stuff that people might have seen before, and then I'm going to go crazy and show you how you should think about this. 

Good. So we have a way, well, first of all, we've got some data. So we've got a data matrix, and we've got some wires. Well, you can transform your x values. You can add a 1 in front of it, and you can even do transformation of the values, square them and put polynomial turns on them if you want to. And we get a function like this. Here I've made it sure to explicitly say it's actually also a function in B, and it's actually linear in W. Okay, that's the requirement. 

Good. We've already talked about how can you actually measure the discrepancy between the, let's say, what your model thinks and what you have observed in Y. That's exactly what's written here. I think before I wrote Y hat I or something, I can square that, and I can sum them all together. 

So we've done that already. Okay, but if you think about that in terms of something we've already seen, it can be written in nice linear algebra like this. You take a vector Y, you multiply x tilde onto W. That gives you also a vector of your predictions of Y. You take the difference and you square them and you sum them all up. That's the mean squared error, and it's like, because I take the average here. If you come from statistics, it's all called RSS, or the residual sum of squared errors. 

So it has many names, but don't worry about terminology here. Think about what's actually going on. Good. So we can do this, right? It's simple stuff. 

Good. How do we actually find W now? Because you gave me an X, you gave me a Y, but how can we find W? So if I want to do this, I can do two things. 

I can put that into Python or Matlab or whatever, so it gives me some weights. It might be a good idea to know a little bit about how that actually happens. So what do you have seen before? Maybe. Let me just circle around a little bit here. 

Who has seen this expression before? Okay. I don't know, 20% or something? Something like that. So don't worry, we'll spend a lot of slides figuring out where it comes from. What I can tell you is that what we want to do is find the Ws that minimize this error function here. 

Remember, the error function is what measures the discrepancy between my observed Ys and what your model, that means with a certain set of Ws tells you. So maybe we should just make a very simple example here, actually. Let's see where this takes us. So I have observed some points. Oops. 

Some points here. I now want to find a model, and actually the way I'm going to parameterize my model is very naively. I'm going to write a model that looks like this. What does that mean? It means I can fit constants. I can fit a function that has a constant value. That's something like this, or something like this, or something like this. 

That's a very simple function, but it's a function. And what am I trying to find? I'm trying to find the offset here. So there are three different values here, or options. So I guess I would choose the dashed one in the middle. I think that's the best one for my balling it. But I balling stuff in high dimensions, we know that's not easy. So if you have four attributes, you can't do this, right? 

But I'm going to make a guess and say that I think the middle one is the best one. Actually, I've defined that by example. But let's see here. 

Let me pick another color. Okay, so this was a function in X, right? We have X, and then we have F of X or Y, depending on what you want to call it. Okay. 

Then I want to actually figure out how does this error that I have defined, how does that depend on the value of W, W naught? Maybe we should have a big slide on this somewhere. Come remember if we do. Anyway, so I have now something on the X axis here, which is the value of W naught, W zero. 

Okay. How is that going to look like for different values of W naught? So what I have on the Y axis is of course, or maybe not of course, but that's the error evaluated now in terms of W, but just one parameter, W naught. And I measure that using this stuff up here. 

Essentially, I take the difference between my observed values, the X values, and one of the lines that I'm now looking at. Okay. So how will that function, so the error function here, or loss function, is a function in W? So how will that look like? Does anyone want to venture a guess? I know the shape. I don't know exactly. I was going to, it's a little bit sloppy here, right? 

Does anyone want to venture a guess? That's correct. So someone, a colleague of yours, drew the correct shape. So if I know that the correct one is here, W naught, let's say it's, I don't know, five. Around that, it's going to be a second-order polynomial like this. I know this by looking at the shape of this, and basically you don't have to know the shape here. 

You just want to find a general gist, or understand the gist here, in that for given W naught, which is the best one, the one that minimizes the difference between what I'm model tells me, regardless of how complicated the model is, and the original data. That's what we call W naught star. It has a certain value, W. Let's say that I was actually correct, and this one is W naught star. So that is the best one. It is, in this case. So that's this one here. Now I change W naught a little bit. Let's say I change it out here, so I make it smaller. That would correspond to this line here. But it has a higher error. When I measure the difference between my data points, and what the function tells me, it's going to give me a higher value, which is bad. I want a low value here. 

I want to minimize this error. Now, we are actually lucky in this case that it has this particular shape. And you can think about what happens when you have multiple W's here. It becomes this shape in high-dimensional. So we are lucky in that there's actually a closed form solution to finding this W naught star, the best one. So how do we normally do optimization of finding minimum, maximum, and high school? We take the gradients of something, of the function, that means the error function. We take the gradient, and don't worry if you haven't seen this before. You can look it up in a matrix cookbook or something. Or you can just do it coordinate-wise to check that this is actually correct. 

But anyway, I can tell you that the error of this function, or sorry, that the gradient, that's the partial derivative of this error function, with respect to all the parameters in your model. That could be one, it could be many. So this is a vector. That needs to be zero. And to solve that, we move things around a little bit, and you get an expression that looks like this. 

And this is the standard ordinary least squares solution. So what do we have to do? And this is where your colleague before said that, okay, something needs to apply here numerically. That means we need to be able to do this robustly. 

And there are ways of doing that. So it shouldn't be a concern of you in this course. So we need to take the transpose of our data matrix, transform, so we have the ones in there, and maybe feature transformation as well. Multiply it by itself, not transform. We need to take the inverse, and that's the nasty bit here numerically. Okay? Then we need to multiply it onto the transform itself, and then onto y. 

Okay? So this is a closed form solution. A lot of the models that we look at in machine learning, they do not have a closed form solution. There you actually have to do something else, and I'm just going to give you a little bit of a hint. So some of you goes up to the mountain there in the middle and looks around a little bit and says, if I need to go down the hill, which direction should I look at? You can't see the whole hill, but you can certainly sense that going down towards here will get you to a lower point. So if I start here in my error function, just looking in the closed neighborhood here and figuring out, actually, if I go this way, I'm going down. How about continue to do that until you get down here? That's essentially just gradient descent, and you'll have to apply it, but you don't necessarily need to understand it in full. But that's another way of doing this. 

But in general, you actually, in this case, we are lucky because we get this closed form solution. Go ahead. Okay, the question is, okay, I'm going to a little bit off, let's go track here. 

So let's imagine that we actually have a loss function that looks like this. It should be blue here. And then we have w nodes here for some reason. 

That turns out to be the case. How do we actually deal with that? We deal with that in neural networks when we come to it. That's the short answer. So let's not worry about that today. Just worry about that. We know that these linear models, in this case, have this nice shape. 

So we know a few things here. It has one unique optima, one, it's a convex function. And we know that there's a closed form solution, namely this one. 

Okay, that's the important things you need to know about this. You don't need to be able to derive this, but as you've seen, five, six slides, you need to actually use this. So let's try to use it. Okay, good. 

But this is basically one slide about how every other machine learning course in the world presents what this should look like, least squares. I'm now going off the rails again and telling you this nice story I tried to start last time about how we think you should think about it. Because what's the problem here? You've made no assumptions about our why's, explicit. 

I said a little bit about maybe it's the green one plus a little bit of noise, but can we actually make explicit assumptions about our observations, about the noise on our data? And that's what I'm going to do now. So remember this map approach I talked about last time. So what I'm going to derive now, or just give you the gist, is basically based on that. So let's build it up. 

So we have the same function as before, same functional form as a linear one. The answer remains, how do we learn the correct w's? Well, now we're going to do something a little bit special, and I'm going to jump a few slides. We're going to make an assumption now. So maybe before you look too distressed here, we're going to arrive at this in a few slides. We're just going to do it in the proper way according to me and a lot of my colleagues and a lot of statisticians. So what we're going to do here is we're going to make an explicit assumption about how our observation there arose. 

The assumption here, let me just jump to the slide, otherwise I'll forget, the assumption here is this stuff that's written in the middle. It tells you explicitly that your observation, the y, the temperature you're measuring, is a function as follows. It's a function that you have parameterized. So it's the output of your model plus some noise, epsilon i. And it's noise specifically about that data point. And this noise does not depend on other data points. So now we can use what we talked about briefly last time, namely that we can actually assume that this noise comes from some distribution. 

So we can make explicit assumption about our data. And of course you know that the normal distribution, oops, that was a bad, the sort of standard normal distribution around zero, it looks roughly like this, symmetric of course. And this is P of epsilon i, where P is the density function, density probability function over epsilon i. And it has a certain variance sigma squared. Okay, so now we have a normal distribution, that's this bit here. But I made an assumption here saying that actually it's my function plus some noise. So what I'm interested in here is actually the density for my observation. Okay, now it turns out that's actually the same as talking about the density for epsilon i, because we can write epsilon i as follows. Just move with what do I need to move. 

I need to move this stuff over there. Okay, so that's what I've done here, I'm plugging into the normal distribution. And in the end what I get is this really, really important thing, that we explicitly see that our observations, the density over our observation, why, is simply a normal distribution with a mean that's given by your function, evaluated at that input xi, and some noise. I have now made an explicit assumption about my noise, and it's explicit what's going on. 

I don't have to do post-talk, let's say, explanation about where it came from. So what does this mean if I try to draw it? And I'm now going to use the same colors, hopefully, as before. So let's assume that this is sort of the, you can see this, yes, this is the green function, this is what nature, this is ground truth. We can't observe it directly, but we can observe that some noisy observations from it. And then I have my red line, which is my function here. This is what I've basically fitted to my data, or sorry, not fitted yet, but given some weights I can actually do something like this. Let's assume that that's my function. Okay, and that was based on some observations of the function plus noise. That means actually my ground truth function here, and some noise. 

So I have some observations lying around somewhere, maybe this one, maybe here. Okay, now I can evaluate what's actually the likelihood, or what's actually the probability density function for a given point, x of i, and that should be a normal distribution, or I should evaluate a normal distribution at that point. So that's what I'm doing now, and the mean of that normal distribution must be the output of my function, the red one. 

And then with a certain noise. So if I want to evaluate my, let's say, density at this particular point, what do I have to do? Let me pick another color. Well, I have a normal distribution that has a mean here, right? That's f of x given w. It's a normal distribution, so if I sort of flip it, I can try to draw it. Maybe it looks like something like this. Ah, should be centered there, right? That's the center of the normal distribution. But if I want to evaluate what's actually the likelihood of observing my data point, that's the blue one over there. 

It's fairly low here, relatively. But the key point here is that we made an assumption about how should the noise look like, namely, it's normally distributed and it's independent, because we have a separate xi for each data point. And now we can write up a likelihood, the p of yi given xi given w. We know from last time maximum likelihood, we know how to use the maximum epistorio area. So let me start from that. So remember, if we have a likelihood everything is good, the world is good, you have clearly specified what's going on. The likelihood here is parameterized by the function f, and your assumption is about the noise. So we're again just reminding you about the map estimate here. So we have the data and we actually have a functional form of mapping from your input to the output. So we can assume something like this, like before, something like this. And what did we do last time? Well, we said if you have a likelihood, that's what we're writing up here. So you have made explicit assumptions about it. Across all of our observations and data points, we can evaluate this density that we saw before, which we call a likelihood. 

We can make assumption that they're conditionally independent, giving our weights. And now we can essentially take the product of them, and we want that to be as big as possible. That means we can fiddle around with our w's to make this thing here as big as possible. Okay, then we can plug it into the whole Bayesian machinery and just cross the numbers and hope things work out. 

But we can also simply ignore the fact that we don't want this full distribution over our weights now. We just want a point estimate. And that is what we're doing here, that we need to write up an error function in w based on our likelihood now. But we know how to do that, because last time what we said was, okay, if we simply discard this idea of getting a full distribution, we can take the maximum of the distribution, simply by minimizing the minus log to this expression. 

Then we figure out that this thing down here actually doesn't matter, and we are left with this. This is our likelihood, or the log to our likelihood. So if I take the log to a product, I get a sum, I take the minus to be able to minimize. And then typically what we do is we discard this part here, but that becomes important when we are then going to talk about which regression and regularized regression going forward. So it's there for now, but essentially what we are doing is discarding this one today, so we are back to what we call maximum likelihood. 

Okay, back to where we came from. So we have now a distribution over our observation given our weights. We know that if we take the product of all these individual terms across our y observations and maximize that, we're all good. We can do that by writing up a simpler error function in v, and then we can see what happens. And what you need to do here are a few things, and hopefully you see we get back to something we've seen before. We get back to these equations simply by putting in these explicit assumptions. 

So bear with me for now. Okay, we have the error function. We are going to assume a flat prior here, so it's not going to matter when we want to find w, because it has the same evaluation at all of the w values. Now, if we didn't actually do what we did before, namely taking the minus log to everything, like actually computing this thing here, then we end up with what's going on down here. 

So let me point you in the right direction here. So we need to take the log to the probability density function. The probability density function is written up here. When you take the log to this stuff here, then this becomes a constant, because it doesn't depend on w. Let me write it out. 

So when we take the log to 1 over square root 2 pi this, and then we have the x minus 1 over 2 sigma squared, and then we have this complicated thing in here. I'm just going to put yi minus fxi. I'm going to write that whole thing apparently. Okay, and then this. If we take do that, then log to a product. That's the sum of the logs. So log to this part. Log this part here, and then we get plus log to an exponential. That cancels. It's a natural log. 

Okay, then we get minus. This part appears to be 1, 2, sigma squared. Yi minus fxi w squared. Okay, so this part here, that's what's in here, doesn't depend on w. So it's a constant, and we can cancel that already, like we canceled the prior. Then we'll live for this part here. 

By the way, you need to do this on your own just to understand why it happens, I think. So now we live for this part here, the red part. But I forgot about this minus out here, so it becomes positive now. And then I move this one still here. So what I have here is essentially what's in this one here. And now I need to sum over all the number of observations. 

So this gives me this expression here. But it came from, and we should put some more details in the slide. It came from actually assuming that we have the normal distribution as the likelihood. That's important. We had an explicit assumption about our data. That's important. 

Good. But if we do this and continue a little bit here, you end up pulling out these constants, that just constant, it's not going to change the solution to w. And then we are going to end up with something like this. So the error is proportional to this. This was exactly the same thing we written up before, like on that one slide showing the least squared solution. But, okay, so if I take the derivative of that, it's going to be exactly the same as before. I'm going to miss out on a constant, but that's fine. And then I'm going to get the exact same solution here. So what happened? 

I showed you a slide that I said this is probably what you might have seen before. This is what most people do. The problem is here, I did not make any assumptions about my noise in my data. If you do it in a proper way like we tried to do here, then you actually need to make assumptions about your data. You end up with a likelihood. 

Yeah, it's a little bit nasty actually going through this, but in the end, there's nothing magic going on. We end up with a likelihood. It had a specific parametric form. Take the log to likelihood with some over everything. And then we end up with an error function we can minimize. We take the gradients of that, put it to zero, create, and then we are home safe. Then we get a solution again. It's the same solution. 

Obviously, it's just that we arrived at it in a different way. So try to appreciate the different views of doing these two things. One, I just put up a statement saying that it is a good objective. In the other one, I put up a statement about the noise and the assumptions about my data. 

And that's an important thing if you really want to get into this game. Okay, in the interest of time, I'm going to skip this one and have you do that on your own. I'll probably in the video, I'll put in my words about this one because I just want to, oh, my solution to this one. I just want to spend five minutes on how we can actually use these type of models in logistic regression. That means for classification. 

So if you now do not have a continuous variable like we do for regression, but we are back to the game where we want to make a classification, you know, does it have legs, yes or no? We can't write up a normal distribution for that, but we do have another distribution from last time we can actually use here, namely the Bernoulli one. And we used it for the Bernoulli and the binomial and all of that stuff for flipping coins and all, what not. Now we're going to use it to actually write up what is the likelihood and principle, what's the probability of observing a yi that can be zero or one, given some xi, so measurements about the world, and given some weights. I'm going to actually assume the same functional form as before. 

Nothing has changed and that's the nice thing about this. So some people say this is regression, but it's really not. This is just a linear model. Okay, but anyway, back to the likelihood. So we have a likelihood for observing some particular y given by the Bernoulli distribution. And remember here the theta is the probability of observing a one. You have the option of zero one, this is the probability of observing a one. Because when you put in yi equals one, you pick out this part here, so this theta i is simply the probability of one. 

Okay, good. So how do we actually map from a function like this to a probability that is between zero and one? Because we want to use our models to model this probability. Okay, like before we used the models to model the mean of the normal distribution. Now we want to use it to model the probability of seeing the outcome one. 

Okay, so there's a problem here and that is that this thing must be between zero and one, and of course this thing here doesn't necessarily. So what we can do now is come up with a very clever function called a logistic sigmoid. And this is this function here, so the sigmoid or sigma symbol. It's now a function, it's not a variance or anything like that. 

So you have to be able to deduce what we mean from the context, which is usually more or less easy. So if you actually try to draw that function, sigma of z, for example, and it's defined here, what does it actually look like? Well, it looks like the following. So we have 0.5 here and then you have something going from almost zero down here and then it goes up and then it goes up, meets here zero as input and goes to 0.5 and then goes up to one. It should be symmetric around this axis. 

So what's going on here? So I'm taking a set that's on this axis now. It could be the output of this model here, so it could be f of x w that we define as z. We squeeze it through something to make sure that it actually is between zero and one. That means we can now interpret the output of this sigma here as the probability. 

And that's exactly what we need here for the theta. Okay, so if we do the same thing now as we did before, we have the likelihood, we know we can use this maximum likelihood, the map or even the full base, to do our learning of the parameters. And the parameters I want to learn are still w, still the ones that ensure where I'm at here on the set thing here, right? Because you see the probability depends on x that's fixed and on w. So you can fiddle around with w until you sort of get a certain value here, either higher or lower. So the probability of y equals one. Okay, but if we do the same thing, we just crunch the numbers. We say, okay, in this, let me find the equation. In this thing here, when we write up the map or the maximum likelihood this part here, I need to take the log to my density or to my likelihood. Okay, let's do that. Okay, I take minus log to a Bernoulli distribution that has a certain probability parameterized this way. 

Squeezing my linear, the output of my linear model through the sigma function here or this logistic sigmoid to get something that's between zero and one. So now it is the probability. It can be interpreted as such. Okay, then I can, well, I know that Bernoulli, it has a functional form like this. If I take the log to this, you've done that in high school, you get something like this, also called the binary cross entropy, if somewhat interesting. But anyway, the important point here is that we simply formulated a likelihood for the binary classification where we had zero or one labels. 

If we formulated the likelihood, we simply need to take the log to that after, of course, having parameterized it somehow. Okay, good. So that way we actually end up where we came from. 

So what I just said is all of this stuff here. So we have one way to write the minus log to our likelihood of observing zero or one, or actually why I? Okay, we simply crunch the map procedure like before the maximum likelihood and we get a cost function here that we can maximize or minimize. Okay, we just need to remember that the probability in the Bernoulli was parameterized somehow. 

So we need to fiddle around with these weights until we minimize this expression here. Now we have simply used this functional form, this f of x given W, T, X, to parameterize the probability of observing something. And we can put that into the likelihood principle. And now we have a way to actually find the weights also in a classification models using linear models. Okay, now before we had a closed form expression for this, we don't hear. It's still convex, but we don't have as closed form expression. 

So you need to use tools in your MATLAB setup to actually, oh, sorry, MATLAB Python, obviously, to actually find these weights, but there are ways of doing that efficiently. Okay, so I think in the interest of time, then we'll skip this one, but I'll talk over this as well in the lecture, or in the recording. So you can have a look at how to solve these problems. My final slide is on this one, because I hope you have now realized that we took the linear model, this f of x W, T, X, tilde. So linear model is not necessarily a linear function in X. It's linear because it's linear in W. Okay, we took this functional form and we created both a regression model. We assumed a normal likelihood. We created a logistic regression model for classification. 

It's just terminology for binary classification. That's what we did here, and we simply did that by assuming different likelihoods, assumptions about our data. And we ended up with these two expressions that we need to maximize somehow. But there's a general way here called the generalized linear model. This is where you'd not necessarily have, let's say, binary observations or continuous observations where you assume they're normally distributed. Imagine you observe something that's between 0 and 10. That's not a normal distribution because it doesn't have support beyond, let's say, in the minus domain or in the plus 10 domain. So you need a different distribution here. 

But as that turns out, you can arrive at this sort of a general expression where depending on how you define the likelihood, you end up with a different discrepancy, how you measure discrepancies between your observed data and what your model tells you. But the name of the game here today, important thing, is this functional form. You can use that to parameterize regression models and classification models. 

And then, of course, there is the idea of classification and regression trees that you also need to know about and Hunt's algorithm. Okay, that was a long rant from me. So I think we'll end here and then you go do the exercise and talk to the TAs if there's something you're in doubt about. See you next, see you in four weeks. 
Speaker 1: Hi everyone, I'm Joseph Alvarez. I'm an associate professor at DTU Compute and I'm a colleague of Bjorn. And you're going to be with me for the next three weeks, for this and the next two weeks, okay? And we're going to discuss some very important topics in machine learning. So welcome in the sixth week of Introduction to Machine Learning Data Managing. Today we're going to start a discussion talking about evaluation, how we can evaluate our models, and why this is important as well. We're going to see a very basic category, but trust me, today what you're going to hear is going to be one of the, if not the most important thing that you have to remember after you finish this course. There will be some other time, but you will understand why at the end, I hope. 

Excuse me. Okay, so about the first project, you have submitted that and in a couple of weeks, I guess you're going to get back the evaluation, the feedback. And then you're going to have an extra week if you don't pass, to resubmit, such that to fulfill the requirement, you know, to pass the course. Okay. I hope Bjorn will explain what is the process. So we have to wait for the feedback and if not passed, we have to resubmit. Cool. 

And for the project to now, the description is already online, so you can start working with this. If you want to change groups, is it okay? You can change offline, but you have to wait. The best tactic is to wait before you receive the feedback and then you go and you change officially your group there. Okay, so you can start working now with your new group if you want. And then you can change your registration on the DULAR. Do it after you receive the feedback because the feedback goes into the group channel you have in DULAR. 

Is it clear? Okay. And another question that you might have is that if you want to change the data set for the second report, yes, you can do that. And advice from my side is to spend maybe one page to do what you did in the first report. 

Just try to make a small data analysis for a new data set to be sure that you understand what the data looks like. Cool. So, I'm sure you know, is this list about the people who are supposed to come online and give us feedback? 

Of course, everybody is welcome to come and give feedback. And these were the chapters that you had to read for today. And I hope you did. 

And I kind of suggest or advise you for this week and the next week is very, very good for you if you spend some time reading the material. And if you didn't do that, then the promise that you're going to do that afterwards. Okay. 

Cool. So, work-sex, over-filling cross-validation and error-stable methods. This is very important and this is very important. Okay. The other thing is also very important, but it's more intuitive. That's the, you know, what I said that the project is already online so you can start working with this. 

In the exam, you can check when is the date online and at the moment this is the day that the exam is supposed to happen. So, let's start with a learning objective for the day. We're going to start the discussion about what the error is. You have seen already more or less the turning data. So, when you train a model, you have to make it good enough, such that to understand your turning data, which is very, you know, direct. 

We don't have to explain more about that. If you can't solve the problem for your turning data, then there's no clue what is going to happen with some new data. And speaking about new data, this is where the test data and the examinization are coming to the game. Just briefly, it's an issue now is the model is supposed to do in the future on the same data when we live in the world to work. But this time is something that we will compute in practice to be able to estimate that as a quantity of contrary compute. We're going to use this technique, the practical technique is systematic technique that we can use to estimate this number. And we're going to see two versions to evaluate the performance of a model and how to select the model. Of course, you're going to see a problem that this method has. So that's why we're going to see the two layers cross validation. We're going to say method to select features and then the cameras table methods. In my opinion, this is the most intuitive method in my study and it's very useful. But I hope it's going to be clear why is a intuitive. 

Same thing. We're going to introduce the arrows. We're going to see the basic cross validation for performance evaluation for model selection. And then the two level cross validation that tries to solve some problems. These two methods have the basic cross validation. Okay. 

And then there's neighbors. So I should propose learning models what we have some data. They come like we have some dogs and cats pictures and then we want to build a classifier to recognize for a new picture if it's a cat or a dog. 

The question problem with the size of the house and the price of the house and then we want to train a model so in the future when somebody comes to tell us the size of the house and then automatically say a good price for the house something like that. And typically we have some data that they come into some some some way. We have seen vectors. Of course they can be different forms but we'll focus on vectors that's the most classic approach. And then here we have to define a mathematical model. 

Right. We have to define a mathematical model. This vector into account and then use your number. 

This number can be zero or one if you have classification problem or can be continuous number if you have a regression. But that's where we want to build using training data. It was a very simple example where we have as I said this could be the size of a house next viable. And then here we have that one viable could be another price of a house. Maybe it's a stupid example with this data but you can think about something else if you want. And then we're learning about the blue points and the green function is a true function. 

The physics the world that generated this data set. Of course we don't have access to that one. We have access only to eight points. What we can do as machine learning engineers or scientists we can define a function in this case is just a linear function as two parameters W0 and W1 takes the X and has to predict the number a continuous number. And because it's just a line we can fit only just a line on the data. Okay. Of course this is a very simple model. 

So somebody will say yes it's good. Yeah we can predict some trend but this girl like that maybe not a very good model for this data. So what we can do you have seen how we can take this linear regression idea and make it nonlinear by including transformations of our features. 

Like a second order or third order or whatever. So now what happens we'll have a better fit kind of approximates the data. Let's say a new point here. We're going to come here and we're going to give this prediction which is not that far from the true function. Of course we don't know it. And also the error between our pet. Our trend function. 

And the rule though there is not that big in most of the times you know the time we are small. This seems to be a reasonable model right so the job does the job quite well. But somebody can be very extreme and check it I'm going to use a degree polynomial. So. 

Something real like just use it for a customer. I'm going to say so because there are some cases like maybe here. So if this point comes now into the game and then we try to predict the value. We're going to predict this value which is very far from the real function. 

Again we don't know it. But it gives an idea that it will increase too much complexity is probably not the idea. Of course depending on the problem we have the data we have the number of data we have. In general complex models is not always a good idea. 

On the same time very simple models is not a good idea. So somewhere in the middle is the switch spot. Let's say similar thing in classification what we have here. We have a classifier decision tree. So we give the data here the blue the blue points and the red points and we want to find the decision boundary that separates them. Let's say the green curve is the true decision boundary. 

Whatever comes here is blue whatever comes here is red. And of course we have some outliers we call them. It might be that I don't know this is cheap houses these are expensive houses. And this is you know a house that is very nice but somebody wants a low price something like that. Okay. 

And I want to try and send some model. So we start doing splits. Access align splits with decision tree. This is a very basic split we just split across one variable. Yeah. 

Okay. We separate all the data but we make a lot of mistakes here in this area. Like more splits in the other axis as well. And we can find this decision bother which seems to be working quite well. We do a few mistakes of course. But I would say we approximate the overall trend of the data. We can separate them quite well. Again we can increase the complexity too much. But now we can see that we have this previous regions here. We see that the actor this blue point only constructs a whole new land of blue area. And the same thing here. 

We see that this points over here. They construct red areas into the blue class. So maybe again that the high complexity is not a good idea for this data set. Of course a very simple example but you can imagine. 

What can happen if I had a blue point here probably the whole area here is going to be turned blue. And what we can do for a decision. Yep. So going back to our problem we had a decision tree. And we were saying that we can get a problem like if we go too far with splits. So we can make like a super super complicated tree that every node is fully pure. 

It's just one class in the node. Maybe this is not a good idea. So what we can do to fix this problem. 

We can maybe. I say that while we're constructing the tree we don't want to make super pure classes. We want to have some noise inside the class. 

So it could be that two points from our class and one point from the other class. This is fine. I don't want to purify anymore. 

Well you can just construct a full tree and then start cutting a little bit of the leaves. Like say okay this is a totally pure class. There's another totally pure. 

Sorry. No, not totally pure node. Let's merge them together because this is kind of super complex. So these are some ideas how we can construct a tree that is not very complex. But how can we systematically because you cannot observe the whole tree if in a difficult problem and start doing this without boiling. And the same thing for the regression. Same thing for classification. We have some very simple models that they solve the problem very, very loosely. Some very complicated models that they solve the problem to the extreme case. Let's say they are becoming kind of overfitted. 

They overfit the training data. Well here it seems to be the sweet spot. Okay we solve the problem and we're not very complex. 

This is very nice. And what I said about the complexity is not something I came up with as the idea more clever people. Much more clever people came with idea in the past. They said okay if we want to design a system we don't have to be super, super complicated. We have to make the system as complicated as it has to be in order to solve the problem. Okay these two clever guys they said that before. 

So if you think here we had a very complex decision tree or a classifier or a regression and here we had a simple one. We have to be somewhere in the middle. So what is the solution? What could be a solution in this problem? Here where I bought we saw the errors but in principle we cannot do that when we have big data sets and big problems. So what we can do we can have many models. We can solve a problem and then test every model and say okay. 

How well you are performing in some new data and how well you are performing in some new data. That seems like a reasonable technique and it's not like rocket science. It's a common sense. And precisely that's what we're about to do. We're going to see a technique, but we're going to train a model and then we're going to try to test, estimate how well this model is going to behave in the future. 

Let's go ahead and start with training. So what we have here we have three models. The first degree polynomial straight lines, second degree polynomial and sixth degree polynomial. Okay and we have this small data set. The black line behind our curve is the true function that we want to learn that generated data set of course with some noise. But that's the given data set and we want to estimate that. 

When you solve the problem, basically you are minimizing this objective function, this cost you derived with your last week. So what do you have here? You have the true labels. Here you have your prediction for some input x, right? And you won't find this problem. There's W that best solve the problem. So it makes sense that this number is going to be optimized. So we're sitting if we check this number because we try to minimize this number. 

So it's not like a fair number to report. Say, okay, I solve the problem on my training data, I have zero error. Okay, well done. 

That's good to do, but let's see what's going to happen in the next steps. So it's a training error. We want to minimize it during the training process. What's for like that we want? We want another data set like this green points over here that we don't know during the training phase, but somebody comes in the future and tells, okay, you have this model. Nice. 

Take this data and tell me what is your prediction for this green points. We know the true label, but we don't use it during training. Okay. So we know what we have to predict, but we don't use it during training as information. And that's the test error. That's something fair that we can report for customers. Okay, I train my model, give me some test data as the result. You do this type of error. There's number of errors. And here we can see why because this model. 

This test set has, okay, some error here at this point, but for other points, it works quite well. One for the linear model. Here we have larger errors. 

And for the very complex model, we have also here larger errors. So this seems like a good idea. Turn the model, take some test data and test the model and the test data. Again, nothing complicated. The only thing that you really have to keep in mind is that we have a training data and we have some new unseen data. 

Okay, you have to separate that in your head. And why? Because this is the topic of final fitting. That's one of the most important terms in machine learning. So whatever fitting means, and that turns into this blue curve in the beginning, is that we turn our model. Here's the model complexity. 

That's the great polynomial, second and eighth degree polynomial. And we say that the training error goes down and down if we increase the complexity. If you remember, we tried to increase the complexity and we even interpolated the data. We had zero error on the training set. But we have another test set and we test our model every time we fit it. We say that something like that happens. 

And that's really what you have to remember at the end of this lecture. But if you train your data, the model of the data, the training data, this goes down, but the test error can make it jump again. So, potentially, when you want to select a model and use it, you know how you want to go here, where your training error is zero. 

But you want to come here. Where your training error is not great, it's not zero, but at least your test error is the lowest. That's the important part to have to remember. Overfitting means that I can solve a problem perfectly on my training set. But if I go to a new test set, then I can screw it up. It's the same thing maybe when you're starting for the exam. You know, you take the previous exams, you solve them perfectly at your place, okay? I'm ready. 

I can get a 12. And then you go to the real exam and what you realize is I memorized how to solve the exercise, but I didn't really learn how to solve new exercises. It's more or less the same topic here, the same concept. If you have zero training error, maybe you memorized very well the training set. But that doesn't mean that you understood the real system that solves the problem. Okay? 

So, very important sentence. Never, ever validate the performance of the model on the training set. This is wrong. Totally wrong. And if you do that, you're going to have problems in the future. 

Okay? Imagine that, you know, you work in a health company or industry or in a hospital and then you fit the model and training set, zero error, and then a patient comes and you say, yes, you are healthy, it's fine. My first model says that you are healthy. 

And this guy dies because you're on prediction. Okay? So, keep that in the back of your head. 

It's a very important thing. It's about the generalization error and this is the true quantity that we want to know. What is the generalization error? We don't have just a test set, but we have all the data in the world. And we test our model on this infinite data. Assume that you train a classifier to recognize cats from dogs. You have, let's say, 1000 images. You train your model. 

Nice. Then you take a test set with 100 images. You test it from this one. The generalization error is if you have all the possible cats and dogs in the world, you know, you have the probability distribution that generates images with labels. Of course, this is in the real life, it's an unrealistic quantity to have because we don't have access to the, you know, to the system or to the physical phenomena. 

Right? You just have a collection from the training set. So we cannot compute this number truly, but we'll try to estimate it with, we'll see ways how we can go close to this number. 

So this is the true performance measure, but we cannot really evaluate that. So I want to make a pause here and discuss about this thing over here. This is the formal definition of the generalization error. What it is. Here, okay, it's just a, you know, an notation is for the model M, the generalization error. 

Okay. And here what we have, we'll have an expectation and here are some subscripts and here we have a quantity of this random variables. When I was a student, especially a young student, I was looking into these things. I was like, okay, what is this? You know, it's looks very complicated. But then if you take a step back and then you try to use just the basic math that you already know, it seems that the problem kind of simplifies. So you say, okay, this is an expectation. Let's say, let's see what we can do if we have an expectation. 

You have an expectation. It means I take the integral of a function over a probability distribution. Here's my random variables. So what I have to do this by function, I have to take an integral. Then this is the space I'm integrating it. And I have here a function. 

Okay. And I have here probability distribution. This is just the definition of an expectation. If you go in Wikipedia and say, what is expectation? It's going to tell you this definition. Okay, nothing, nothing special. And then you take a step back and say, okay, what is this probability distribution? What it means? 

What are the problems? So for our problem, what this means is that X are images and the Y are the labels of these images. So there is this generator, like, you know, some process that generates data. So I have like a probability distribution, generates an image, like a cat, and just a label. Generates a dog and just a label and so on. So I don't have really to know what it means, but I can understand what is the intuition behind it. Okay. And I suppose that you have seen how you can estimate this type of expectations. 

Again, in Wikipedia, I think third line or something, you can see that. This is called Monte Carlo integration. So if you generate infinite data from this distribution, you can estimate this integral. And it means that this one, this complex quantity from the beginning, can be estimated by this infinite sum. 

What are the X and YS come from this probability distribution? Okay. And all this seems to be more simple to understand. What it means, I have some infinite data that I potentially can generate from a distribution and that can evaluate the performance of my model. So this complicated quantity in the beginning, that is, it looks a bit A-fine. You know, you broke it down and it seems to be like an, okay, understandable quantity. I just have to sum up numbers. 

And precisely that's what we have to do. We have to estimate the generalization error. Hopefully we want to generate big data sets, test sets, okay, not data sets, sorry. 

And then we want to estimate the expected or the average performance. Okay. I hope it was clear. Okay, we start from a mathematical quantity that looks terrifying. But then if you break it down, you see that I didn't want to do, I have to use many test sets and then compute the performance of my model and take the average. Okay. So here on the left is what I saw in the previous slide. This is the training error. 

This is the test error. That goes up, down and up again. So hopefully this is what you have to use when you turn a model and look at this one here. And trust me, many people do that even nowadays, even in research papers. So this is not a good tactic to do. And this on the right is the generalization error. 

So let me mix the colors. This is the same curve. This is the true quantity, let's say. Let's say we have access to this probability distribution. 

We can generate infinite data points so we can estimate these numbers here. But the lines in between are not important. It's just to show you the trend. What you're interested in just the dots here. 

Okay. And then we zoom in a little bit here. You see these points. 

And what are these black lines are? This is the performance of my model on individual finite sets. Okay. So here I have one, two, three, four, five, six, seven, eight, nine, probably 10 sets. I compute the performance of my model and then I take the average. And the average would be somewhere close to the true generalization error. 

Maybe not exactly the true generalization error, but hopefully it's going to be close. And the same thing here. You see that for this model, I get these numbers over here. And for the more complex model, I get these numbers over here. 

So if I have, let's say, K sets, I compute this value and then I take an estimate, I average them, and hopefully I'm going to be close to the true number. And that's it. That's, if you remember that, you're in a good place. 

In the next slide, we're going to see how we can implement that systematically. But that's the whole point. You have to understand how these dynamics work. And then how to implement that isn't very complicated. It's just a matter of common sense. That's why we're going to need the cross validation. 

So the cross validation is a technique that helps us to estimate the generalization error. And why? Because we don't have infinite data. We don't have a good friend that gives us all the time new test sets. What we have access to only is a training set. 

So we have just 1000 points. We take that and somebody tells us, I don't know what you're going to do, but I want to solve this problem. Solve it. Okay. 

I don't know why you are fired. Something like this. So we have three ways to estimate this generalization error. And we're going to see the person comes from each of them. 

But let's see first what we do. This is the full data set. Okay. Let's say these are 1000 points. Somebody gives me 1000 points. 

And what's the simplest technique that we can think about? We take this 1000 points, we cut the part out. Let's say 100 points, we load them up. Then we turn our manual with 900 points. And we use the ones we left out for testing. 

I hope you were expecting to hear something more complicated and fancy, but that's, that's it. So you take your previous exam sets that we gave you, you solve, let's say nine of them. And you leave a 10th outside without ever taking that. And then you try to solve it like a real exam. And you see how well you have done. 

You had this exam. Super simple. This is the whole method. That I said you cut it to, you know, not necessarily in the hat. You see, you look at the route, you train on the rest and then you test on the part you left out. 

That technique is called K for cross validation. Here are the things that a little bit more complicated, but not really. So again, we take the full data set and now we consider three different splits. So you have this 1000 points. You consider, let's say, 300 33, 300. And then you turn with this, you test with this part, you turn with this, you test with this one, you turn with this, you test with this one. So now you have three estimates. You have a value and this is your estimated job. 

Is this an error? Okay. To some extent, this looks like I have three different training sets and three different tests sets. 

So I'm training three different models and I'm testing three different models. Okay. Again, I don't think it's super, super complicated. And the extreme case is to consider every point as a test set. Then use all the rest as a training set. 

So eventually you have to train and models every time you live on one point, you use the rest for training, you test on this point and then you do this thing in times. Okay. So this is just cutting a part out. 

This is considering many splits and this considering every point as a different asset. Do you have any questions up to here? Yep. Yep. Okay. 

Yeah. So the question is if we pick, if we have, let's say three different arrows here, we pick only one model. No, here we want to estimate the error of the model class, not of a particular model. 

So I want to know that if my polynomial of degree two performs well or not. I don't, we're going to see some examples in the next slides, but I don't consider only one model. I'm considering a class, a complexity class. 

So I'm saying I have a line, a polynomial of second order and a polynomial of third order. Which one I have to use for my data? Not necessarily which specific model I'm going to use. We're going to see that, but I want to know the model class. 

So at the end, you take three numbers, you average them and you say, okay, this particular model class has the best performance. For example, here, this is like lines. These are polynomials of second degree. 

These are polynomials of degree eight. So you say, okay, I want to use this type of models later on. Yep. Yeah, we're going to see that on one of the differences. 

I'll just introduce them, but we'll make a long discussion later on what, when you use one or when we use the other, more or less. Yeah. Yes, we're going to say that they just, just wait for you anticipate. Nice. Okay. It's good. 

It means that you follow what I'm saying. Hopefully. Great. So we can see here. 

We say the example that I was discussing a minute ago. So I have here the three full cross validation. Okay, we'll split it out. The other set into three faults. 

So this is for one, two, three. If this was the full data set. This is this part of the data for test. So we use this part for test and this part for test in this column. 

Okay. And these three different lines are the different model classes. So in the first day of the first row, we use only straight lines to solve the problem. In the second row, we use. The nine miles of second degree. And here we use. 

We use high degree polynomials. Okay. So we have. A three full cross validation and three models. So I didn't have to train nine models. 

Okay. And if we consider now, there are a family of these one models, which is like one. The errors of this one. Here, there are some data one here. There are some data one. We have a list of errors. And here we come up with a number. And it's going to be the error that we expect a linear model to do in this particular training set. Okay. 

In the future. Same thing here. We have three errors. We have a gym. And we take the expectation. So our estimation. 

How will the model is going to behave in the future? Okay. And the same thing for this case here, we have again some higher or. Here and here. So we're going to average them out. 

I'm going to get a result. What is important to understand here is that because the training set changes, you see that the fitted model is not always the same. You see it's different because we'd change the training set per fold in the test set. 

Of course. That's why you have this variation. And maybe already you can see that if you select the model, the variation on the model is not super bad. Okay, it changes a little bit, but not dramatically. 

It's only here a little bit of difference. That's it for the intuition. Okay, so I mentioned that more or less already. 

Good idea to do with rain. You just cross validation to estimate the generalization of these three models. And then what we do, we can select this one. And we say, okay, this is the model class I want to use for my problem. In this example, I'm going to use probably this polynomial to solve my problem. Okay. 

Nothing fancy. Just see where the test error kind of makes this use a so we'll select this. This number is a K. I'm going to use these models. And I'm going to say both make a whole house happens in this answer to your question, basically. And we do that for more selection. Okay. 

We want to have the. The cross validation technique that estimated the generalization error. Okay. For every model. And now based on these numbers, we will select the model. 

Is it clear? Here we estimate the generalization error of model class. And now we just select what is the best model class. 

Again, a simple example here, we have three models as before and five for cross validation. So we have. I split. Five different splits. So in total, 15 models. And if we have a rates now, all this column, we're going to take the error of this model one. If we have a H all this column, we're going to take the model two. And if we have a H all this column, we're going to take the error of this model three. And for example, the models graph straight lines again, polynomials and 30 degree polynomials, something like this. And this model is what does. It's the. It's a data set considers K number of splits, five splits, for example, then takes the blue point, the blue parts only for training and this red part for testing. 

Then trains three different models, saves the numbers and it relates until at the end, you know, they make these averages. This advice from my side to you, we have the same values. I hate those. These are terrible full of text. So what I'm trying to do, I'm trying to take all this thing and just make a small example. When you have much same thing, something looks complicated. 

I have to draw something. It's much more simple to understand what is happening. And at least in my opinion, this is more how to understand on something like this. 

Okay. So here we have a way to estimate the national error of this three different model classes. And then we select the one that minimizes this number. Let's say is this this thing over here. 

So say, okay. Well, in our mouse of second degree works the best. Now what I can give to my customer. Back to your question. And the question in any opinion, what we can, what we can use because here we have different models. You know, not in the full data set, but in different parts of the training data. So I have five models and a model class. 

So we have two options. One option is to take these five models we have trained. Let's say all the models from this class that we have trained during that way during the cross validation. And then we can use something that we call an ensemble method. You're going to see in a couple or three, four weeks how it works. But basically we're going to take all these five models and we're going to have a prediction. 

Okay, because we said this class has the best models. So I select all of them basically and I use them at the end. I evaluate each of them individually and then I take the average and this guy will be my prediction. And I have an estimation of the other of this model. 

Let's say, let's call it like F star. I give a new is a house price. I give the new house price, a new house size that I'm voting one, two, three, four, five models. I take the average and I'm saying, okay, this is my prediction. Assemble model you're going to see later on that this model is against with what we have estimated for generalization error, because each of them will know how much error makes. And another person could say, well, you train five different models and each of them will know it's not ideal. You left all the results on test set. Why you don't train a new model by using the training data? Okay, and I think it's perfectly good recommendation because yeah, why not? So I can train another model using the full training data now. 

I'm not considering splits. I selected which is the best class. I said this degree polynomial is the best. Now I'm training a new model using the full data set. 

Okay, what is the problem with these approach? Yep. Exactly. For this new model that we're trying to have on the full test set, we don't have a good test set to use, right? We don't have any left out data. Okay, we'll have it. And I'll do that. Yes. Okay. If in all these cases it worked well, it's going to work on the same level, but we don't really test it. 

We have probably been lucky and selected that one. So we don't have a good way to test the final train model. And keep that in mind. 

For the small star, we're going to see in 20 minutes more or less how we can do that. Okay. But this is what I would say is the best thing to do. 

So then you use a full training set and then you estimate the model where you can do that with the samples. Okay. Now we'll take a step back and we're going to see schematically like with captain plots, how this works in practice. 

I'm going to consider the whole method, the simplest one. What we do here, we'll move out some of the training set. So these three points will have them out. 

We don't consider them doing training. And we use only the blue points to train our model. And then we go to the other one. We see that yeah, here we do some errors. 

This seems to work well. Here we do some errors. Here we do again some errors. So probably I'm going to select this one. The best model. This is the simplest scenario. I look at some data. I'm training and I'm testing. Makes sense. 

And this is here. Okay. This one is all okay for then confuse yourself. 

Sorry. And maybe I have to do it before. But this means like the complexity or the degree of the polynomial, right? This is number over here. 

Cool. And we'll see again this overfitting behavior that the training, the training error was always down because here in table of my training set, you see, I'm going really close to my training data, even zero error. But the test error, we see that changes is increasing. Why? Because we have this, this part's over here. Well, for this one, it looks to be fine. 

But the test set will have a CDU in training. It seems to work very well. That's why we select this model class. 

That's, that's what I'm going to use. And let's say how the liberal and up to solidation works. That's the other extreme. In this case, we leave all those one point out and we use the rest for training to continue. And if we do that only for one iteration now, what is the, what is the thing we have this point out? We're trying the model using the blue points. This is the solidifier model and this is the error. This is for model class, model class, model class. And potentially now we will have to do the same step for all the training that I will have. Every time we leave one out. 

So here, this one, this one, this one, this one, this one. We do the same process and we estimate the error at the end. They're all error. 

This kind of the bit of this from this light. I know it's a very, you know, too much information, but if we say that we say that this, they have something in common on these plots. Can you understand what? What do you see to be more as the same in all the cases? Yep. 

Yes, the early, of course, because we use models, but something along those lines. You said that you know the grant one doesn't change much, right? It's always more than the same. We don't have like big jumps on this line. And this is because probably we don't change too much our training set every step. We just leave one point out. So there's not a big difference. So what we're training sets are very similar. 

One with other day. There's strong correlation. So we don't expect with the one out to change very much the model at the reiteration. We're going to say again, why in few slides. And the same thing you will increase the degree of the polynomials. 

I have second degree. And we say again that that behavior does not change much. And even here, more complicated, the normal 50 degree. I will say that, you know, that will behave over the care. Of course, there are some small changes here and there, but it doesn't change crazily. Cool. And then we're going to move on out again. We see that my feeding behavior training error goes down. Test error. 

Makes this you say. And now I'll go back to that question. Okay. But we don't have like a rule. 

Use this or this one. However, it's because every technique has benefits and disadvantages. First of all, I think for the whole method is kind of fabulous. What can go wrong? Do you have any idea what can go wrong with the whole? Let's say I have a classification problem with three classes. 

What can go wrong? Yes. So if I spread my data set randomly, I might be super lucky and I live almost all the class that I love of the game. Let's say if I have here some class and here another class and here another class. And let's say for some reason I'm super lucky and I consider only at least two classes during training. Oops, sorry. Or even, you know, consider some parts of this class, some parts of this class, but some parts of this class, but really bad points. 

Not as a representative. Well, I forget one part of the space totally. So the whole technique is a bit dangerous because we can leave out an important part of the information. 

We don't want it to exist. It exists because if we have a huge model, like you say it's at the PTO or whatever, that they use billions of data points to turn the model, this is not a viable solution. So your test set now, it surges in the training set, it surges that the probability of forgetting a full class totally is very low. So it's after much information that you're living somehow is not going to matter. And also another problem is that training this model maybe takes a year. Right. So I cannot train and models of the size is going to be super bad. So we have super big models or super big datasets. The whole technique seems to be a little bit one. Let's see. But there was a he is not data efficient in the sense that you might forget some part of the data. 

You don't take them into account during training. Okay. Now let's go to the other extreme, the live on across validation. What is the benefit of the live on across validation? Where is the downside? 

The downside, I'm more or less a little bit out. If you have to train any models and the number of projects you have is really big, then you have maybe to train and models. Models. 

The market. If the, if that is a big, it's a problem or even a model is very big is also not a problem. Maybe my dad has it is not very big. The model takes two weeks to run. Again, not ideal. So this one is very cost intensive. It needs a lot of resources to train. On the other hand, what is very good for the one across validation is that it's very accurate. So for a particular training set, it gives me the best estimations that I just know. It gives me the best value I can get. 

This comes with a cost. Usually when you hear in my cinematic and in science that something is very good. Usually it has another problem that, you know, we have to discuss. What is this problem? This problem is that a particular training set. If we use the move on our technique, we saw in the previous slides that the model doesn't change much. So the training sets are well correlated. 

Correlated means they have many things in common and the results of models are very similar. And this is good to understand how the model behave behaves in the particular training set. But the problem is that if I now again another training set, the result might be totally different. 

So when I'm doing the live run now, I'm really focusing on this particular training set. For example, I'm doing a cats and dogs classifier here in Denmark and then I go to US to train another model, another technique. The estimation of the error I'm going to have here and from the training set I'm going to collect in US, maybe it's going to be different. 

That's what we call here the live run error that has load by us in terms of the error. Using the best error doesn't give me an overestimation of the error. It's very accurate the error that it returns. It has high variance. So high variance, if I take another training set, I might get very different numbers. For example, if we put here the live run out and I take a training set, I'm estimating the zanization error and this comes over here. If I take another training set and I do the same thing, the number might come here. 

It's going to be relatively different. It has high variance. And now we go to the careful cross validation and you have seen that the whole world has some benefits. The whole world has some benefits. We can imagine what the careful does. 

The careful tries to balance these two things. The main point doesn't go so extreme like generating n classifiers and different models, but also doesn't consider only one split. So potentially all the points at some level, at some motivation that becoming test points. So we never have the problem of forgetting total data part of the data. So we use all the data. So it's data efficient. We use all the data. It's not so cost intensive because okay, we train here only three models. 

This is fine. A small problem that the K fold has is a problem and good thing at the same time is that the models I'm training now, they flip to it a little bit because the training set changes. So this part, this part and this part always are left out, which means that there is not a strong overlap between the sets. So the models I'm doing, they're going to be different every time. And this is a good thing and the same time. Why this is a bad thing because I'm overestimating is realization error. 

In the sense, correspondence to the liberal post validation, you will come here and put a default. And last time for the same training set, we might come here. Okay, so we have a bit of a higher estimation for the realization error. We give a higher number because we don't use all the possible information at every step to train our model. So we don't expect to have the best model at every iteration. That's what we always or perhaps we cannot know for sure, but we tend to overestimate realization error a little bit. What is a good thing though is that now we change the training set. 

We don't expect to be very far. So this has no variance because it is a model is not the one that use all the training set. Look to it. So if I change my training set, yeah, it's not going to be very different. The model is probably going to agree. 

Okay. So you can imagine that the careful cross validation is the best technique to use in practice like five or 10 full cross validation. And something else that again, however, I did to keep in mind that whenever you're not training set, the best thing to do is to use all the possible training information where you train your model and that makes sense. I suppose if you have the test exams to prepare yourself, you don't say, okay, I'm going to take only one of them and prepare for the real exam. 

I'm going to try to use all of them probably. And the same thing is here. And we say that the K fold because it uses all the possible resources. 

And they live on now, they perform a little bit better than the hold out method. Of course, you have to take into account the number of the number of data points, the size of the models. But I have an idea using the other training data when you're solving the problem is always the best. 

And that's what you see this trend over here. But typically the level because it uses all the training points at every step, you always leave just one point out. That's why it's not accurate because it never forget some information. 

But of course it has this problem that here might overshoot. I know this discussion goes a little bit, you know, and time is a bit entailed. We're going to put that after the break. And I think it's a good time to take a break. Then we come back. We're going to redo this discussion because I'm sure that many of you are about to sleep. So we're going to revisit this discussion. I want you to make some questions. Okay, well guys. 

Wait a minute. So I'm going to make this discussion please. Some questions if you have doing the break come down and tell me how is doing the, you know, the, the, the, the, the, you can make the questions. What's your visit that because it's important. Okay. 

I think it's the presentation. So, uh, 15 minutes break. Let's be back at the time clock. Okay. There's a same thing again. 

Yeah. So let's, uh, recap what we have seen in the first part of the lecture and really, I don't know how much I can stress, but it's a very important topic. So be sure that if you don't see, if you don't understand what we're saying now, try to revisit it afterwards. Okay. 

It's quite important to understand what's happening. So the whole point is that we get a training set with, we have to train a model or we have to estimate the complexity as well. What type of model we want to train and neural network and linear regression model or decision three to solve our problem. 

And you have to find the technique to estimate the test error. How well this model is going to behave in the future. Of course, the correct number is the generalization error is impossible. We cannot compute that. So you have to come up with another technique to do an approximation estimation of this number. And this technique is called the cross validation technique. So what we do with the cross validation. 

First, the first problem, the most simple thing to do. We take a training set and we split it in half. We split it into test set and a training set. So we never see that during training. 

We forget about it. So we train three different models and linear regression and decision three and neural network. Maybe then we test each of them on a test set. 

And then we say, okay, linear regression model behaves the best. We select this one and we train on the full set now. Probably now we have a new model or we might not. 

We might only check the test it out and they didn't train a new model using all the data because some people last in the chat also have a validation set. Can we do this and that? Of course, you can do many combinations. Okay. 

Now I'm trying to motivate what you have to think about when you are using these techniques. Of course, if I have a billion of data points and a super big model and I make a split and I train a model and I test it and this works well. Then I'm not going to return a new model. Okay. I'm going to say, okay, this works well. 

Let's keep it. That's probably what these guys did in OpenAI. But if I have a decent data set, a decent size of a model and then I train something, train it here. I test it and it works kind of well. Maybe I need now to consider only this part on the training phase. But then we said I don't have a good way to estimate the error because now I don't have a new test set. 

That's what we're going to see now in a few slides. But I hope it's clear how we can use the holdout method to estimate the test error for one model or many models. In the live one out, what we do here, we take the full training set. We generate now many test sets. 

Every time we leave out one point, then we use the rest for training and then we test with this point that we left out. Okay. Zoom works well. Nice. So we do that end times now. If we have end points, every time we leave out one point, we train a model, we test on this point, so on and so forth. 

So we average end values at the end and this is our estimation of generalization error for one model. And then we do that. Imagine if you have a super big data set, you cannot do that. End times. This is useful if you have a small data set or very fast models that you can train in fast. 

Okay. And the K-fold cross validation is more or less something in between. You generate some test sets by splitting the training set into two parts. This is for training. Sorry, for testing, for testing, for testing. Here you train, here you test. One time. 

Here you train, here you test, here you train, here you test. You get three numbers in this case. You average them and this is your estimation of the generalization error for a particular model. You do that for the next model and the other model. You find what works the best and you say, okay, I'm selecting this one, the linear regression model. I'm going to use that. 

And how I'm going to use that? In this case, you have to retrain of course, right? Because you have now three models. So now you consider the full training set and you have to train a new model using all the data. 

Makes sense? nd then you have to train three models, different models. To take a final model, you have to train now from scratch a new model by considering all the training set now. 

The full one. Make sense? Okay, now let's see the other discussion, why the bias and the variance, and why this is a little bit important. I zoomed out one of you comes and gives me a training set, and then I'm using that with a little technique to estimate a generalization error. That is going to give me a very good number. 

As better as it can get. So this is the best number I can estimate. It's just an estimation of generalization error. 

Okay. Just an estimator. If I use the K-fold technique, I know because I don't use all the training data all the time, I'm always leaving some, a big portion of them out. I know that I'm going to be a little bit worse. So I'm not going to get a very good number. I'm going to overestimate it a little bit. It's like, you know, this guy's going to make always some mistakes. So the K-fold, we know that always going to make some mistakes. 

But what is the benefit? Now somebody else comes, gives me a new training set, and I have to do the same thing. For the live one now, it might be that the estimation is going to be very different. We're going to see an example in a minute, but for the K-fold, this number is not going to change much. That's what I said bias in variance. 

So we know that the K-fold is going to overshoot a little bit in general, but it might be just to not confuse you that's going to build ways up. It might be that I'm not estimating this number now. So for one training set, the live one now is going to predict a very good number. For another training set, again, it's going to predict a very good number, but might be that these two numbers are very far away for a particular model. While the K-fold for both cases is going to give me a number which is not the best one, but at least they're going to be close together. 

Let's see why. Assume that you have this training set in the background, then we use a straight line to fit this. If we use live one now, we estimate the number, the error, and probably it's going to be bad for this particular example, because we see that the data, they have a little bit of a nonlinear structure. This is the first training set we have. And now somebody brings me a new training set. And for some reason, this training set looks like this one over here. The red points now. Now a straight line makes sense. 

It's not a bad model, the straight line for this new training set for the same problem. That's the problem now of the live one now. The live one now is going to give me a good number. It's going to tell me a straight line is very good. 

They solve the problem very well. That's the bias and the variance of the live one now. For the blue training set, the live one now technique for the straight line is going to tell me it's a bad model. But the live one now for the new training set is going to tell me that the straight line is a very good model. 

So both of them, they're going to estimate well the generalization error, but they're going to give me very different opinions for the same model class. Makes sense? I think, you know, don't take it as a, you know, I have to understand exactly all the details, why and blah, blah, blah. 

Just think about it, absolutely. I have a training set. This training set is, I don't know, house prices. 

And this is in Denmark. The blue points, okay, they have this behavior. The straight lines here, they don't work well. And if I estimate this number, it's going to be a good estimator, but it's going to tell me straight lines. 

They're not good models. But if I do the same technique in Greece, for example, and I have this data and I do the same steps, I'm going to again estimate the number. This number is going to be accurate, but it's going to tell me also that this is a good, you know, a good model of the straight line. That's why it has very low bias. 

They live on out because I don't know where to make the error, but has high variance because if I take another training set might give me totally different result. There is something number. Totally different estimation. And again, here I'm just estimating the number. I'm not saying what is the best thing to do. I'm just estimating. 

And this tells me that estimation can be very different. Next week, we're going to spend more time on that problems. Okay. 

But I hope today at least is a bit clear. Why what happens if we change it, the training set, and we try to estimate generalization. Okay, so training the changing training set might change many things, especially for the live on up for the K-fold. 

What is good is that there is some overlap between the training set. So this is here. They overlap. They have an overlap. So the models will agree a little bit, but not so much overlap as over here. So here is almost the same data set, right? 

But they live on up. Spend some time thinking about it and discuss it with your colleagues. I think this discussion is going to help you. 

So let's make a small quiz. Here say in your network, you forget about it. You forget about it. You can think about linear regression, spend couple of minutes to read what we have to do. Basically say, okay, here we have to apply the holdout technique and what to estimate zanlization, blah, blah, blah. You can go directly here and try to find what is the incorrect answer. Okay, this is more or less understanding. Okay, so spend couple of minutes to read it and then we're going to see together what is the correct one. 

Thanks. So the first of all, this is more or less an important way of just focus directly what happens here is holdout 50% of the data is more computationally efficient and a five-fold cross-validation. Why do you think this is correct or incorrect? This is correct, right? Because we'll have trained just one time and the five-fold cross-validation will have trained five models. So it's more computationally intensive. So this is correct. Let's go to D. Not all observations are used for testing using the holdout method. 

This is correct, right? Because we live one part out, we never use it only for testing. We never use it for training. This is correct. The size of the training set in tenfold is larger than the training set in five-fold. What do you think? Is correct or wrong? Yes, it's correct. And if you want to do that again a bit like graphically, here you split it into five times. Hopefully these are the same size. 

And here you have to split it, you know, nine times. Okay. So this correct. And the live on out cross-validation gives a poor estimate of generalization error as only one observation is part of the test set at the time. This is incorrect. 

Why? Because at every iteration of the live on out, yes, we live out one point, we train on the other points and test on that one. But we do that n times. So we never do it only one time to live on out cross-validation. We did n times and then we have a result. So this is a wrong answer. So that's, we hear what's going to be incorrect. So you have to do this selection. 

We stopped doing that. So don't worry. You're not going to have any of these funky questions in the exam. Now we're going to see a technique where we want to apply the cross-validation. 

Okay. This, this is not like focusing specifically on cross-validation. It's just a technique that we want to apply the cross-validation. 

Okay. And why, why does this technique we have, let's say, 100 features in our data set, 100, you know, balance. And we know that some of them might be unimportant or we know that we don't have so much resources to use all of them. 

Okay. So a good idea to do is like to select what's features are the most important ones. What we can do though, we can select a model class, let's say linear regression. 

And then if we have 100 features, we have to train this number of models and test all of them. Obviously, this is a bit problematic, right? Because if the M is a big number like 100, this is a very, very big number. So this is not realistic to train all possible combinations and test them. You know, for example, here you can see some combinations. I use only the intercept. I use only these features. 

I use only these features. So on so forth. This might be gene expressions, whatever. Now we want to find a technique that's more efficient. And that's what we call forward selection or back post selection. Sequential features selection. So what we do here on top, you can see these are the features of the variables. 

These are not cross validation faults. Okay. So this is let's say X1, X2, X3, X4, I don't know, size of shoe, height, weight and I don't know, BMI, something like that. Some attributes. 

Okay. And what we do this technique, we start first from an empty list. So we don't consider any feature. Our model now is going to be just the intercept. 

Okay. We train a model. So we compute the intercept. We test the result using cross validation and we take an estimation how well this model behaves. Okay. In the next step, what we do, we train now, sorry, we train one, two, three, four models and each of them has only one feature inside. Okay. And each of them I tested with cross validation how well it behaves. 

For example, if we consider the holdout technique, that's always the simplest. I have my training set. I use that for testing. I use that for training. 

So it's one of them. I'm going to train it using that part and I'm going to test it using that part. I'm going to do that four times. I'm going to have now four models and I'm going to select the one that performs the best. Let's say is this one over here. 

Okay. So now I'm saying I consider this feature like I don't know the height of the person, my problem. Now I'm going one step down and I'm doing the same thing again. I have like one, two, three potential new models I can train. So at this point I have selected let's say f of x, w, zero plus w three times x three. I select this point over here in the previous step. 

Speaker 2: And now I consider all the other combinations, which is this one, this one, and this one. You see this is fixed. I have selected this feature. I don't change my decision. Okay. 

Speaker 1: Now I train again these three models using this training set and I'm testing that using each of them using this test set. I'm getting three numbers. I'm selecting the best one. 

Let's say it's this one over here. So now my model becomes f of x, w, zero plus w one times x one plus w three times x three. And we continue until the point that if I include more features does not improve the generalization error. 

Okay. So you can think about it as a sequentially I select models now at every step I have to select a new model. And for every selection, I need to apply cross validation. So the cross validation gives me a tool always that I can use that whatever problem I have in my study, I have the way to estimate how well this thing is going to be here. 

Okay. And that is for the sequential feature selection. So we'll start with zero features. We do this greedy technique. And at the end we're going to have like, I don't know, in this case, two features only. I'm not going to use all four of them. Okay. 

Is there any question? Once more, these are not cross validation faults. These are just the different features, the different attributes we want to consider. And here is the example I tried to draw before. This is the model I selected at this point. This is the model I selected at this point. 

And here I see that they give me worse numbers, worse error. So I select only okay, I say I stop here. This is not. This is a greedy technique. So I'm not, I cannot guarantee I'm going to find the best combination of features, but at least it works. 

And it works quite fast. The backward error selection now, a feature selection, sorry, is the opposite. We start from a full list of features. And we start dropping at every step one. I don't want to go into details because we're going to lose time and I have to say more difficult to understand things later on. 

But the idea is that now I have all of them considered. I get estimates, analyzation error. I come here, I drop some of them. 

One of them, it's time I estimate, analyzation error. And I select what works the best. And somebody could ask, okay, but why to drop information? It might be that we know that some of the features they have noise. We know some of them maybe are not important for my task because somebody collected all the gene expressions. And I know for this disease, I'm interested in this 10 of them. So not all of them are important. You can think about scenarios where this can be of use. Okay, spent like a couple of minutes reading this equation. 

Then again, together we're going to see the solution. The idea is that you want to do feature selection, sequential feature selection. Here you have all the combinations we saw before. Here you have the training and here you have the test error. And you want to apply the forward selection technique, okay, using cross validation. 

Think about it, how you will do it and try to understand only from this table. And then you can see which features you can select at the end of the algorithm. Once more you stop when the next feature, if you include it in your list, it gets worse, the generalization error. Okay. 

Or the estimation generalization error. Okay. So in the interest of time, we have to be a bit short with the quizzes today. And we might get a bit over time. Let's see if we take another break and then we continue again. Let's see. 

So first thing, first thing first. Here we have two columns, training error and test the MSC, test error. So what do we have to focus on on the left or on the right? I think this is the first thing that you have to be very sure we have to focus on the test error. What happens with test errors? What we estimate was the generalization error, okay. And we start by looking which one improves if we include one feature in our list, what improves the error. And we say that this one gives the smallest number across these options. Okay. So we start with this one. We start with the feature C. Now we go in the next level and we see which combination we have. We can try. 

So these are the only combinations that have B, sorry, C inside the already. Sorry. This one. Yeah. 

And this one. These are the only combinations that have C online inside already. And about the error, we see this is 1.8, 1.6 and 2. We select this one. This improves the error. 

Okay. This is a combination that improves the error. Now we go one step up and we see which has B and C already inside is this one and this one. But here we see that we increase the error and here we increase the error. So we stop with this option. Okay. We are good. We're going to keep B and C only. 

Just a side remark. This one has better error. But as I said, this for what selection is a grid technique. 

We don't use the best combination, but we use some good combination with a grid technique. Okay. Now, what is the problem of cross validation? I said that might have some problem in the beginning of the lecture. What we've seen before the break, basically, and we discussed a little bit after is that we have some models, let's say linear regression, this decision tree, the neural network, whatever. And we have the cross validation as a tool to estimate how well the model is going to behave. And we said, yes, by doing that, what we can do, we can select this model class and say, I'm selecting this model for my customer. 

I'm going to give it to my customer. And this comes with some number here, some estimation, jalaization error. Is this a good number or is it a bad number? So if you do the selection and you give this model to your customer, you cannot say, yes, I guarantee that this is going to be your error. Or there might be a small problem here. What do you think? 

If I estimate the error and I select the model at the same time, is it a problem, a sudden problem that might happen? Nobody read the book? Okay. So the problem that might happen is the following. First of all, we have an estimation jalaization error. We don't have the true one. 

Okay. So the true one might be something like this, the true jalaization error. We don't know it. We cannot compute it. This is the number of, if we have any finite data, we have just an estimation. So we have a training set and using this training set, we estimate numbers. Who guarantees that these numbers are above or down of the true number? 

No one, right? So if we select always the best model and use this number, we have the high chance to be below, right? Because if the true error has some shape, we estimate it so we cannot take the correct numbers, but we take numbers around it. So for another training set, for different training sets, we will compute numbers around this line, this curve, not take exactly on the true error, right? We're going to get numbers around them. So who guarantees that I'm going to select some number that's very close to the black line, not very much down of it? 

No one, right? And that's a problem. To some extent, we again overfit because we estimate the jalaization error. We select the best model, but we might be unlucky and we had the combination of a holdout technique 

Speaker 2: that we select this for training, this for testing. And for some unlucky reason, that was a very funky split that helped me, but it wasn't the correct split, the representative of the problem. This might happen. Nobody can guarantee that. 

Speaker 1: So cross validation has this small problem, but whatever we do, we're kind of biased to underestimate the jalaization error. If we estimate the number and select the model. If somebody comes and gives me like a model and say, okay, estimate the 

Speaker 2: error, then I can do cross validation and can say, yes, I can guarantee that this is the error. But if I have many models, I estimate the error in all of them, and then I select the best model. 

Speaker 1: Then I have a little bit of a danger of underestimating the error, being too optimistic. Let's see this example here. This example, the black line is the true jalaization error. It's a problem I constructed so I can estimate the correct number. 

I can generate infinite data and estimate the correct number. Okay. And I have many, many models. And then I have a training set. And for this training set, I estimate, let's say I have the holdout technique. So I do take my training set as clearly as I have. I take this for training. I take this for testing. 

Okay. And by doing that, I get these numbers over here. We see that these numbers are just an estimation of the true error. So they will float to it around it. It's going to be up and down, up and down, up and down, not exactly on the black line. So now if I come and I select this model, because that's what I'm going to select, is the best one. That's the, you know, potentially the correct decision to do. I have a big danger of being below this true number. And that's of course a problem, right? Because I do something I think is correct, but it's not correct. Or I have a danger of being a little bit too optimistic. Because maybe somebody else again comes and gives me another training set. 

These blue points. And now they're going to look at it differently, right? Again, it's going to be up and down, up and down, up and down. But we see here that this model is better than this model, for example. So we are never sure if our selection gives me the exact error. But they have estimated. Okay. 

I might have been unlucky. Make sense? Let the, you know, I'll try it. Usually I upload some annotated slides or what I use here in the lecture. And I'll try maybe to come up with a graphical example where this error appears by the parent. I don't have something on top of my head now. But this is what you have to really put in the back of your head very well. 

This is the true error. And what I can only do, I can take some training set and estimate these numbers. And there is a high chance if I have especially many, many models, some of them to be below. So if I do select the model and I use also this as my best guess for the test error, something is fishy there. Let's see what we can do. Focus first here on this part over here. 

These are the full training set. Okay. Let's say I leave this part out. Don't consider this. 

I just hide this. So I don't know what happens. So I have, let's say this training set. 

Okay. What I can do with this training set. I can do the cross validation technique. Let's say the hold out method with it before I can split it in half. I use this for training. I use this for testing. Okay. 

Speaker 3: I have three models, linearization, neural network and decision tree. And let's say I select the linearization again as the best model. 

Speaker 1: I have selected the model. Nice. So I can take this model. I can train it on the full set here. And now I can use the test that I left all the way out to estimate the error. That's more fair because I estimate the first step, the errors. I selected the model, but now I come in a totally different test set and I train and test my model there. 

This is more fun because I didn't use this when I was selecting my model. So it might be that it's going to give me a very bad number, but at least it's going to be a fair number. Because I didn't optimize for this number. I didn't try to minimize it somehow. Until now, I told you, yes, we can use cross validation to estimate the error. 

Speaker 3: I can use the resulting plot to select the best model, but it seems like I overfit, right? I try to use the test data to make a selection. This is not ideal. 

Speaker 1: While if I use something all the way out, then I select my model. I retrain that using this part over here. And then I test it to this part. It was all the time out. That's more fair. Maybe it's not the best number I can get, but at least it's going to be a fair number. Yes, well, the question. 

Speaker 2: This is that the new system could try out test data that wasn't in our training days. Sorry about that. So the training days that we made a new test that showed you how much the system now costs 500. It wasn't in our training days. 

Speaker 1: So the question is, if not the test set, you know, except in some behavior that we didn't have in the training set, that the problem, the holdout technique we said before, okay? Yes, could be the case, but at least it's a fair process to do. I'm not saying that this is, you know, it's going to give the best model. 

It might be that in this case, you're going to select a wrong model, but at least the prediction of your error or the estimation of your error is going to be more accurate. Okay. But let's see now how we can also solve this problem. So once more crossvalidation is very good for selecting a model and by estimating the generalization error of different models. But because it's a bit biased to always being too optimistic, it might give me a number that is very, you know, says the error is very small. 

So we have to keep another test set all the way out to test our final model. Okay. And now we come to the two layer crossvalidation that tries to do this technique more systematic. 

Okay. This is for sure the most ugly algorithm that you will say this class. But what you have to keep in mind here is that we have two levels of crossvalidation. Here we have to the outer fold, K outer fold and the K of inner. The K2 and K1. Maybe the outer fold we do two or three steps and the inner one we do five or 10. And I'm going to give another example in the next slide to try to help you understand. But the whole point is that in the inner loops, we run the classic crossvalidation to select a model by estimating the generalization error, selecting the model. And then we take this model and we go back, we train it once more on the full training or partially training set. And then test it on something that we haven't left all the time out. There we repeat the same process. Now this becomes maybe the test set and the rest of the training set and we do the same thing in the inner folds and so on and so forth. 

I have prepared this slide where I'm trying to explain that graphically and I hope it's going to help you understand and I'm going to upload that in my annotated slides. Let's say we have two layer crossvalidation and we have K outer folds and three inner folds and we have five models. One, two, three, four outer folds. We see the outer folds, we split as usual four times the error set and here this is test, here this is test, here's the test, here's the test. We're going to do the same thing four times and now let's focus on one of the alter iterations. Let's see what happens. 

I have here my five models. I'm taking this thing for training and this thing stays out, okay, for testing. Now I'm taking this training set and I come here and I split it now. Now I'm running classic crossvalidation internally. 

I have no idea what happens outside here. I don't use this at all. I'm just using classic crossvalidation here for these five models. So this is the inner folds, okay. This is for the testing every time. So I'm training a model here and I'm testing. 

This comes in number. I'm doing this for this model, this model, this model and this model. We do that three times because we have pre-inner crossvalidation. Okay, so finally what I've collected, I have collected here for five models, three test numbers per model, so I have 15 numbers. 

I take the average for every model and maybe this gives me the best number. This is what we have seen before the break. This whole technique is exactly what we saw before the break. But now I select my model, I say the linear regression is the good one. 

Now come back here. I train it with this part and now I test it with this part I left all the way out. If you remember when we presented the average for the crossvalidation, I told you, yes, okay, we select the model. Now I train it in the whole training set, but how I test it? 

I don't have anything to test it with, right? And that's what we try to imitate now. Make sense? Doesn't mean that this technique is going to give me the best number here. 

Maybe now the testing is going to be awful and it's going to be a very bad number, but at least it's a fair number. Okay, now we go to the second outer fold. We're going to do exactly the same thing over here. Classic crossvalidation. The only difference is that now this becomes the test set and this is going to be the training set. 

We do the same technique. Now maybe we select a different model. Maybe now we select the neural network as the best model. We go back, we train it using this for training set and we use it with this part as a test set. 

Make sense? This gives you a number and so on and so forth. So at the end we have collected, let's say, these numbers. 

Like one time we selected the M2, one time we selected the M3 and one time two times we selected the M3 and one time the M4. And we have there estimation of the generalization error. We make the average and this is now my best guess for the generalization error. And this is going to be a fair number. 

It's not going to be the best number, but at least it's going to be a fair number. I know it's tedious. It has a lot of back and forth. Trust me if you don't go through this slide or if you don't try to generate your slide, your graphics with the algorithm, you don't understand it. 

It's not complicated, but it's a bit tedious because you have to make a lot of steps. Okay. About the final model, again, I don't want to say much because again it becomes like an ensemble method. It means that you have now in this case four models, maybe you can average them out if you want to make a prediction. I'm going to add this on the notes if you want to spend some time understanding. 

But at least that's a way the two level cross validation is a fair technique to estimate the generalization error of a model, of many models basically. Okay. I know it's hard. You're going to repeat in the next lecture again some parts of it to be sure that you remember. But spend some time. Okay. Try maybe to describe it to your friends. That's the only way that's going to help you to understand it. 

Okay. But you have seen if you just zoom in one of the other folks is not super complicated. This is classic cross validation. We run over here, but now we selected the model. And instead of using that immediately, we test it once more. 

That's the only difference. That's a question of the two level cross validation and says, okay, how many models I will have at the end of training if I want to do this technique. And okay, because indeed there is a time you can do that afterwards. But let's consider in this example here, how many models I will have to train at the end of this algorithm. Every other fault, how many models I'm going to train 123, 123, 123, 123, 123, sorry, four, 123, five times 15 models. So every time in the inner loop because I have five models. And the inner cross validation is 34, I have to train 15 models. Correct. 

So 15 models. Plus I do this how many times I do this 123, four, four times. Times four. Because I have 123 four times and doing the same thing. Is it correct number? How many models have to train? 

Sorry. Yes, so to some extent here where I was sitting a little bit, because we do this 15 times internally to select the model. But once I select it, I come over here and I train it once more. So every time we have plus one. Okay. 

So this is plus one times four in this case. Okay. And that's what we need to do this exercise. And here is the formula as well. Cool. 

That's it. We'll finish with a hard part of the course so you can take a deep breath, relax. And next week I promise to make a revision, but kind of promise me you're going to spend some time reading that. I know when I present this slides, every slide on its own might seem reasonable. Then if you think about them all together, it might be complicated, but it's not so complicated. 

You have just spent some time understanding every slide differently, you know, independently. I know that you have a cross validation seems imitating, intimidating because you know, a lot of stuff here and there. 

Speaker 2: But it's not you have just run cross validation, you select the model. 

Speaker 1: Then you have just to make another training and not test it. And why? Because it has a small problem. If I do this only one layer, I might fall in this trap that I underestimate my error. 

I come over here. I hope this plot, try to regenerate that on your own. Try to understand what we said and try to regenerate from scratch all the steps and why this is the problem. Not only this course in general, that's a good technique. Cool. 

So now, as I said, take a step, we take a step back, we relax. And we're going to see the K &N methods, which is, in my opinion, the most effective techniques in machine learning. What we have here, we have a classification problem. 

We have two pictures, X1 and X2. And we have some labels for females and males. We have the height and weight. We want to train a classifier. Of course, you can train a linear regression model or logistic regression. But let's see how we can classify this point only using the information we have. What is your best guess? What do we will do? 

Any opinion? Yes. But how many points? 

Okay. So the answer is like, let's take the distance to the one, probably the closest point. Let's say this one here. And let's use this label. 

That makes sense, right? So it's like taking your closest brand and use this information. So indeed, that's a K &N classifier. 

And that's it. So for this point, we select the closest one, then we give this label to this point. That's the one nearest label. 

But is this the best thing to do? Because yes, if you use the Euclidean distance and you select the closest point, that's the label, the blue class. But if you use two 

Speaker 3: points, like the two closest neighbors, now what happens if we have one female and one male? What do you decide? 

Speaker 1: Who knows, right? It's 50-50. You don't know what is the correct one. And if I use three nearest neighbors, now I change class, right? I go to the female class. So what is the correct number of neighbors to use? 

And probably you understand what I'm going to. We're going to use cross validation to select this number, but wait for it. So what the parameters for this nearest neighbor classifier will have the number of neighbors and the distance measure. Why a distance measure? Because with Bjorn, you show the Euclidean distance, the straight line, the length of the straight line. You have seen the L1 distance, the sum of the absolute values, and you have seen also the infinite norm, for example. But there are many, many distances that you can compute. 

Big topic. But let's consider that the Euclidean distance for now is the proper measure. You have to select the number of neighbors as a free parameter. 

Okay, this is what you have to train, more or less. Here is what's going to happen if we select different number of neighbors, one, two, three, four, and five. And the color coded behind what we showed here in the blue area. The males are, every point here has the closest point in male, and here every point is closest to female. If we come here, the same thing, in the red area, females, every point is closest to a female. Here, every point is closest to a male. In the white area, we have the same distance to a male and a female. So our two neighbors, the closest neighbors, is one from one class and one from the other class. We are 50-50, let's say, in this region. 

Okay, so we use K-PAL-3, same thing happens. So in this yellow area, two of our closest neighbors are females and one is male. Again, here, because it's even, we have 50-50 region here. In K-PAL-5, you know what is happening, here is the decision. Here, three of the five closest neighbors are females and so on and so forth. You can already see that in the KNM classifier, for example, it's not a good idea to have even number of neighbors. 

It's good to have an odd number to be able to take a decision. And let's say, schematically, what happens and how we can estimate using live one-on-cross validation, the best number K. So for each observation now, what I'm going to do, I'm going to forget the label. So I'm taking this point, I'm forgetting the label of this point, and then I want to decide or classify this point to some class. 

Okay, so I'm forgetting that this guy is a male and I'm trying to classify this guy to one of the classes. And let's say I'm selecting K equal to some number, let's say three or something. Then what I'm doing, coming here, I'm forgetting the label, then I'm using the three closest neighbors to select the label for this point. And in this case, the label I'm going to select is going to be female, right? Because the closest two points are females. 

So I'm going to give here a female. But I have, I know the true label of this point. And I just check if I made a mistake or if I predicted correctly. 

Okay. And then I do it n times this thing. And I see how many times I predicted correctly. 

So in this way, we use live one-on-cross validation because every time we take a point, we forget the label of this point and we try to classify it using the neighbors. Okay. And we do that for different Ks. And we use the K that works the best. That's the live one-on-cross validation in practice. 

Makes sense? In this case, okay, the predicted accuracy because C equal zero means that, okay, I predicted wrong. I made a mistake. I predicted the wrong class. 

And I want to select the K that has the highest accuracy. But this is, you know, tiny detail. I think the whole concept is that, okay, I forget the label. I try to predict the label using the neighbors. In this case, these three points. 

And I do that for different K and I see which one works the best. We have some time. So this is the reason number four. So what we have to do, we have one, two, three, four, eight points from two different classes. 

Okay. And here we have the per wise distances between them. How much is the distance between the first point and the second point, for example. And what we want to do, we want to apply the K and N classifier with K equal three. To class two, and we use the live one-on-cross validation basically to estimate the generalization error. 

So spend some time like doing maybe a few of these points. Try to see if you will predict it correctly by using the distance to the three closest neighbors. If you forget this label, what is going to happen? Basically, we do this thing in practice. We forget the label of a point and we use the three closest points to estimate the label. And then we see we give the correct label or we'll give a wrong one. Okay. So spend a couple of minutes and then we're going to see together the result. Okay. 

I hope you have time to check at least the first two rows or some of these rows, some of these points and you show what is happening. But let's see together. Just be sure that we see the solution together. Cool. 

So let's apply K and live on up cross validation for K equal three to estimate the generalization error of this classifier. Okay. What we do, we forget the label of this point. 

We forget that the zero point with the diabetic class. And we try to find the closest neighbors. Why are the closest neighbors? 

We just see the distances. This guy's close. This guy's close. 

And this guy's close. Okay. So what happens now we're going to give the blue label to this one. For the same mistake, right? So we make a mistake. Let's put a second one. 

We forget the label and then we try to see which are the clothes guys. This is the closest. This is. This guy. 

So again, even if it was a red class, we're going to predict the blue class. Okay. Again, a mistake. Let's go. Maybe one of the blue points. Let's come to this guy over here. We forget that this guy belongs to the blue class. 

And we find the three closest neighbors. One guy here. One guy over here. 

No one guy over here. Again, what we see is that the red class is closer, right? Once from the red class are closer. So again, we're going to make a mistake. 

And if you continue this process, you're going to see that all of the observations are basically classified with the wrong label. Okay. Not very fancy technique again. Maybe I disappointed you that I didn't, you know, talk about the self driving cars or whatever. But this is a very intuitive technique and it is very much used. 

Okay. And the, the number of neighbors is a, the parameter here that we have to estimate and we have the cross validation as a tool to estimate how well the model is going to behave. And apart from classification, we can do the same thing for regression purposes. 

What is the difference now? Here's a one dimensional problem. So it's the Y, the prediction index. It's the input. 

Okay. So what we can do now for a point here, we can find the five closest neighbors because the case equal five here. And these guys are this one, this one, this one, this one. And this guy over here. And we take their average output, like the labels, the true labels to make an average. And that's why we compute this thing over here. So for all this region, the closest neighbors are always more or less these five guys. That's why the prediction remains constant around this area. 

And if I come here, the same thing happens like, you know, until here, these five guys are the closest neighbors. So for all the points here, I'm predicting the same, the same value. Make sense? 

Okay. Of course, one problem with the regression or classification with KNM is that the function can become very, very funky, you know, can change very fast. Because if you consider all the closest neighbors, you can change your decision all the time. On the same time, if you increase very much the neighborhood size, it might be that you get very sub-upimal solution, right? 

So it's tuning the correct number. It's hard. And also with Gern, I think you said you discussed what happens high dimensions. And in this instance, they become a bit crazy there. So it's not an easy technique to apply high dimensions, but at least you understand more or less how it works. 

And in low dimensions, it's very meaningful. Do you have any questions for the KNM classifier and regressor and how we can use cross-validation to predict the error? Because as you did with classification, you can do the same thing for regression. 

What you can do, I can forget the label of this point. What is the real prediction? And then I can predict the value by using the five closest neighbors. 

This one, this one, this one, this one, and this one, right? And then take the rubberage, and I'm going to predict this value over here for this point. So I'm going to have an error. 

Make sense? So as classification, we've got to do the same technique for regression. And use cross-validation testing me, the generalization method. 

Okay. So as a summary, now, and relax, coming in, open your ears again, because we're going to discuss a little bit the cross-validation again. We start our discussion about the implication of model complexity. So if I have three models, let's say the model one is the linear regression model, model two is a neural network. And the model three is a decision tree. And then I want to see for my training set, which model performs the best. Let's say I have a classification task. What I can do, I can use a training error, bad idea. Don't use it. What I can do, I can use a test error. Okay. 

Good. How you can estimate or how you can compute a test error and why you need a test error. We need a test error as an approximation to the true generalization error. And somebody tells you, okay, what's the generalization error? Then, you know, to pretend that you are the best person, you know, in machine learning, then you write down this integral and you say, I want to estimate this funky integral by having a finite data. 

Yes, but this guy asked you how you do that. Yeah, I cannot do that, but I can estimate it somehow. And how you can estimate that. I can estimate it with cross-validation. So cross-validation is a technique to estimate the generalization, the true error of the model. Okay. 

Of the model. And how this works, we take our training set, we split it in half or not in half, sorry. We split it maybe one third or even in half. 

It doesn't matter. You break it in two parts, then you use this part for training, and then whatever you train, you use this for testing. Then you can do that for your three models, and then you think this is my estimation for the generalization error for these three models. Okay, this guy says, nice. 

Good job. What are you going to select? And then you say, yes. Let's see. What I have here, I can draw it down for you. 

Okay. This is my training error for the three models. This is my tester for the three models. And okay, man, I'm going to select this model for you. 

Okay, very nice. What is the error of this model? Then you say, okay, this is the error of this model. Then she asked you again, are you sure? And then you say, yeah, let me think about it. Maybe not. 

Why? Because the truth generalization error, maybe something like that. So we always select the best model, which means that we have the high chance to be unlucky and be below the truth generalization error. Because there's just an estimator. 

We said it's not the truth generalization error. So you're going to say, yes, wait, I have to rethink my solution and I have to make maybe another technique to estimate better generalization error for you. And what is this better technique is a two level cross validation. Here things become a bit funky because what I'm doing, I'm taking my training set. Then I'm considering risk, which of this training set. 

Let's see. Training here, training here, training here, then training here, and then testing here, here, here. And now I'm considering only this blue part. I'm coming here and I'm doing classic cross validation for these three models and one and two and three. I'm selecting a model from those. I'm coming back. 

Come here. I'm training again using the blue data and I tested on the red data. Let's say I selected the M3. 

So I have some error for this model. I do the same thing over here. So I'm taking this training. 

I'm coming here. I'm doing cross validation for the three models. And let's say in this case, I selected the M2. I come back. 

I train it using the blue points and I test it on the green points, on the red points. And this gives me another estimation for them too. And we'll do the same thing for the last outer fold. 

Okay. I will have an estimation for the generalization error now. But what happens now have three different models, maybe in the last iteration of the outer fold. Again, we select one of the three models and we maybe we select again the M3. And we have an estimation for this. 

Okay. So now we have three different models and we have three different numbers. What we do is to make the error again by averaging these three values. 

Okay. We average these three values and we say, hey man, this is your number, what you're searching for. But if you ask you, okay, or she asked you, what is the actual function you want to use at the end? Here we're going to use the ensemble technique that we're going to see later on in the course. But here, basically you're going to have to say here, when I say F1, F2 and F3, you're going to have the F1 effects plus F2 effects plus F3 effects. You know, take the average. You're going to say this is also the function that you can use in practice. 

This is just a side remark. But the two level cross validation is a more fair thing to do if you want to estimate correct the generalization error. It doesn't mean that you're going to get the best error. 

It might be that it's not going to be the best error that you can get for this particular training set. Okay. But it's very fair, more fair than the other thing. Okay. And somebody can say, okay, why you don't do another layer, another layer, another layer. 

Yes, there are many combinations that you can try to do. Let's not complicate it more. We also discussed a little bit this idea of bias and various. We changed the training set. What happens with the live one out and the K-fold cross validation next week. We're going to focus again on this part. But really spend some time understanding cross validation and take it as a technique. 

A systematic technique, a practical technique that you can use in any model to estimate the error of the model. Okay. How you use it afterwards can be the decision is a bit complicated because you need to think about, okay, I might be. I may be underestimated the error and so on so forth. So I have to level cross validation and so on. But above all, the cross validation is a technique to estimate the test error. And after you finish this course, you have always to think about it when you train a model, because now this muscle learning is almost everywhere. 

So for sure, many of you, you're going to use it in your life later on. Make sure that whatever you do, you use a test set to test your model. Okay. That's very important. You might, you know, make a big mess if you don't do that. 

So try always to test your model. Okay. Cool. And this is for today. I'm going to stick around if you have questions. 

Otherwise, you have to go to your T sessions. And next week we're going to see again, a little bit this topic and go to statistical testing. Be sure that you read the book a little bit. Okay. See you next week. 
Okay, so welcome everybody. 

Speaker 1: This is the week seven of introduction to machine learning and data mining. And as the title says, we're going to continue our discussion with performance evaluation. We're going to show two algorithms at the end, the base and the knife base methods. And you're going to see how we can use simple probability rules to build classifiers, for example. Cool. So this is the feedback group for this day. Once more, these people, they are highly encouraged to come online and give us feedback. Of course everybody is welcome to give feedback. What type of feedback? 

I will laugh if you give us feedback in the following sense. I like this example gave in the class. I didn't like this example gave in the class. Please try to repeat this kind of stuff once more. This type of stuff that are constructive so we can use them in the next time. Okay. Cool. And for today you have to read this subject 11 and 13. 

I hope you did because today's lecture again is a bit. If you haven't had any idea about statistical testing. But I'll try my best to simplify it as much as possible so you get at least intuition. And later on we're going to see how we can connect it with machine learning. We continue our discussion with performance evaluation today. Next week we're going to see a bit more. 

And a little bit more at week nine. But I would say that today and the last week the cross validation today are the two main. Second lectures we're going to see performance evaluation and how we can estimate the realization error. Cool. Once more the project two is online is available. You can work with it. You're going to receive the feedback soon. And after you receive the feedback you are free to change your group assignment you to you learn. Okay. 

You can do it on your own. You can go there find a new group and put yourselves there or go to another group where your friends are. But please wait before. Please wait until you receive the feedback on your current group. I'm sure everybody last week understood very well what the generalization error was cross validation. 

All the problems it has all the benefits and disadvantages. But for some of you that you didn't understand I'm going to go again and we're going to repeat what we did last week. And this is going to be more or less the warm up for today's lecture. So what we have in machine learning most of the time to have a training data set we have a supervised learning problem we have a data set. And we want to solve a problem. For example classify dogs and cats from images. 

Okay. And what we get we get a data set and then we have to build a model to do this job. Let's say we build a model and and we want to estimate the generalization error of this model and what is the generalization error. And then we have a data set with 100 1,000 1 million points with rain our model on that points. And then what we want to do we want to deploy this model in the real world in the wild. So we want to know how well this will perform in the real world. 

Okay. So ideally what we can do we can generate infinite data from the same distribution so phases of cats and dogs that we know their label and then we test our model on those. That's very intuitive. The problem is that we don't have infinite data we have only data that we use for training. So we have to come up with a practical technique with systematic technique to estimate the error the generalization error. And last week we saw that we can do it with cross validation and we saw three versions of this K fold live on out in the holdout method. But all of them the goal they have is to estimate is an additional error. 

How well the model is going to behave in the future on unseen data. That's the idea. And after we finish the discussion with cross validation and we saw the benefits and disadvantages. We discussed about the sample issue it has. We said, is it always fair to use cross validation to estimate the performance of the model and then select the model. 

Is it really fair what we're going to report at the end. And let's see why maybe this is not ideally the case and why we need the two level cross validation. First of all, what what we did, let's say we do the holdout method, the easiest one. We take one part of our data for testing and would keep the rest for training. Using the training data, what we did. We have a model. 

Somebody tells us okay, use this logistic regression model. We train the model using training data. So this part here, we use this part to train our model. And then we use this part to test our model. And once we do this two steps, we'll get like an estimation of the generalization error. 

Okay. But of course we don't know the true one. The true one, we can calculate that if we had infinite data. And if we had infinite data, maybe this is the true generalization error. 

That we don't know what it exists, but we don't have access to that one. What we have access only to is to some estimates. And this estimate depends on this test set we have. 

So if I'm taking another training set and I'm doing the same process, I might get another estimate. Maybe it was here, maybe it was here. Maybe it was exactly the generalization error, but I don't know it. I can't guarantee where I'm going to go above or below the true generalization error. 

Okay. So if I had this one model and I was estimating the generalization error and let's say it was this ball over here. Or even this ball over here. Then I would say my friend, yeah, man, this is the number, take it and use it. This is a bit fair because I don't have, okay, I have a risk to underestimate my generalization error, but I don't do it systematically. It might happen. It might not, but I don't have anything else to do. Okay. That's what I had one model. 

I tested that and I'm saying this is what I'm estimating for the error. Now, what is the cat? What is the problem? The problem is that if I had more models and for each of them, a generalization error exists. Okay, let's say logistic regression, decision tree and then run network. Let's say I have three models and for all of them, these are the true generalization errors. 

The, the, the, the, the, the, this thing's over here. Now, if I estimate for all of them using cross validation, the generalization error, I have a higher risk of getting below the true value. Okay. Let's say this was my test set and I tried that. Now I'm getting these numbers. And now because I estimated for all of them, the generalization errors, I had just an estimator. I have higher risk if I select the best model to select something that is suboptimal. 

Makes sense. If I had only one model, okay, I have the risk to be below. But now if I have 10 or 20 models and I'm selecting always the best one, then I'm increasing my risk of getting below. So I'm increasing my risk. If I'm selecting the best model, the one with the lowest generalization error, I'm increasing my risk to be below the true one. Is it clear? 

Okay. And that is why we need the two layer cross validation because if I'm having many models and I'm using the cross validation to estimate the generalization error, and I select the best one, I'm having higher risk to not be fair to report a number that is unfair. And what we did, we did the two level cross validation and the idea is, I'm having it over here is a version of two level cross validation. So we take our full data set, we split it in two, okay. Then we select this part or using this part only we select a model by doing what we did before. 

So we might have this problem, but now we tested once more on something that we haven't seen during training. That's a bit more fair. Maybe the number that we're going to get at the end is not the best number possible, but at least it's a fair number. So now I'm selecting a model by using the cross validation technique. This is fine. 

But I'm not reporting this value over here as the generalization error, but I'm reporting the number I'm computing on some totaliencing test set I left out. Okay. If you think about it, that's more fair. 

And it's not like mathematical at this level, it's more or less intuition. I'm leaving something out. I'm using my training set to select one of 10 models. But once I selected my model, I train it once more using all this possible data and I tested on something I haven't seen at all. Okay. 

Of course we had a bit more complicated to level cross validation when we had many models and many faults, but that's one version that we can use it and this makes sense. Is it clear? Okay. What is another, let's say problem of cross validation, or this techniques that we try to estimate is generalization error. Let's say this is a true generalization error and we do the 10 fold cross validation. It might be that for all these 10 folds, we have many numbers about generalization error, so it's performing worse. 

That's the best one that we don't know. And one number comes so very, very low for one of the faults. So now if I compute the mean of them as my estimator, this can go somewhere here. Is this a fair number again to report? 

Even if it's okay, I will report this. I will say this is why I estimate it. But we can see that. Yes, there is a large deviation between the different faults. So many of them they perform bad, but because on one fault, my method worked well. Then I'm going to be, yeah, okay, it performs well. So cross validation comes with two things. 

It comes with many estimates and we just give an estimator, like the average of them. Is it fair? Couldn't we use all these 10 estimates maybe to report the variance of the estimator? 

Wouldn't be more fair to do? And that's what we're going to see basically today, that since we do this technique that we have many estimates for generalization error, we don't report only the mean, the average of them, but we'll report more stuff. Like the confidence interval. Or we're going to use all these estimates to argue about two models if they are the same or different. 

So this is the idea of why we're going to need the statistical test that we're going to do today. Okay. Cool. I will upload that in my annotated slides and I hope it's going to be fine. If you have any questions, come during the break. We can discuss it and maybe we can repeat something at the end. Okay. We'll upload this one on the annotated slides. 

Cool. What we're going to see today, we're going to see that we have two setups. Basically, we have the setup one and setup two. We're going to see what this is, but basically is arguing for one data set only or for all the possible data sets in the world. We're going to apply statistical test to compare models, which of the two models is better, more or less. 

We're going to do the confidence interval approach to understand where the estimator is possible to lie with him. And we're going to see the NAV base classifier and the base classifier. But at first place, why wouldn't it's a physical testing? Because we might have a company or we are in a set slab for health or whatever, and we try to make a new model. Does it worth it then to deploy this model in practice? 

So let's say we have this company and we want to put like a different algorithm to put to place ads on the websites. Does it make sense to do that? Would be better for the total number of clicks I'm going to get. 

For example, maybe that what I'm having now takes 100 clicks and maybe the new one takes 110 clicks, but maybe it costs, I don't know, one million more crons to deploy that. Would it make sense? Because this is testing helps us to argue technically about this kind of stuff. We will have like many estimates if it makes sense to do maybe a movement or not. Or we're going to ask questions, answer questions, how many customers probably will gonna click that if I change the algorithm behind it. 

Same thing here. If I have like an old network and I'm trying to replace maybe a doctor or maybe I want to replace another technique I have to do the same job. Would it make sense? Or I'm going to just spend a lot of money and not making any significant difference at the end. I'm having only 1% difference. 

Cool. So, statistical testing, we need it in order to argue from the statistical point of view again if we have two models, if it's better to use one or the other. Like to replace the old model with a new one. And if I do that, what I'm expecting to be the benefit. Like what is going to be the new outcome or less, how many clicks I'm going to get. We're going to do some examples later on and I hope it's going to become more clear. Statistical testing is an objective way to make these conclusions, but this depends a lot on our assumptions and on the data we have collected. So, it's not a silver bullet that we can use all the time. 

What are statistical tests said? We're going to use it and that's it. It comes as a tool that we can use, but it's not like Godlike that we can trust it all the time. And that's what it says basically here that we cannot draw certain conclusions that if I have two models, one is better than the other. 

It's better than the other based on this data set, on the assumptions I made and so on and so forth. We're going to see some examples. This is more or less to prepare you for what we're going to see. And definitely it's not a black box. It's not a silver bullet. So, whatever you do with statistical testing, you have to be careful. Okay, don't trust only the numbers. 

Use it as a way to interpret your results, not necessarily to lead you to the decision. So, we're going to see that we have two setups. Setup one is when I have a particular data set and I want to understand what happens with this particular data set. If two models are different for this particular data set or what is going to happen on the click rate if I do something on this particular data set. The setup two is what is going to happen if I change my data set. Okay, so I have two models and if I had a data set here and a data set in another set, in another country and I'm using the same models, will the result change? 

That's the idea of setup two. If I add all possible data sets, what is going to happen if I train the same models, for example. And the conclusions we want to have is which model is better and what is going to be the performance of the model more or less. We're going to see the hypothesis testing and the confidence intervals and we're going to see it for classification regression, basically. Now, let's consider that we have a particular data set and we have here's data set input output, let's say, image of cats and dogs and their label, and we have two models, model A and model B, decision three, logistic regression, for example. And we train two models, one using the logistic regression and one using the decision tree algorithm. Okay, for the same data set. 

These are just two different classifiers. We saw last week how we can estimate the generalization error in theory. We can take the true distribution once we train the model, generate infinite data and compute this number. So this number maybe is how well the decision tree is going to work in the future on unseen data. And this is the number how well the logistic regression is going to work on the future on unseen data. Okay, it might look complicated and technical, but once more, think about it as a generator that gives you new images and here just compute the loss. We average the loss on infinite data. Okay. And how we can say if one model is better than the other, if we just subtract the errors, if this is larger than zero, it means that this makes more errors than this one. Okay. 

Or the other way around. But the important thing that I highlighted is that we speak about this particular data set. So this holds for this particular data set. 

Okay. And this is set up one. If we train our models on a particular data set, what is happening? Which model is better than the other? Now let's see if we have another data set, a new one. 

If we have many basically new data sets, what we can do, we can estimate for each of them their difference and then average this out. Again, this might look terrifying. We have an expectation with respect to the of some quantity. I will suggest slowly expand all the stuff and understand intuitively what happens. So this thing inside is just a generalization error for one model and one particular training set. 

Okay. And the second integral, what it means based on the discussion we did last week, this just mean I'm sampling a full training set. So here you have two steps. One step is I'm drawing a random data set from the true distribution. Then I'm training my model and I'm testing this model on infinite data. Then I'm sampling a new training set. I'm training a model and then I tested again on finite data and I'm doing this process over and over again. I know maybe this sounds weird and we don't do this in practice, but this is the theoretical quantity. This quantity is going to tell you how much is the generalization error basically of one model if it's trained on infinite training sets. 

Makes sense? To some extent this is the best quantity that you can get for a model because it tells you that this particular model, if I had all the possible training sets on the wall, how well it is going to perform? Then you know everything about the model. In practice we cannot compute that using infinite data, but we have a way to approximate that and I'm going to talk about it later. If we try to write down this as an approximate quantity, what you can have here is you can have like let's say J models, sorry, J training sets that you can sample from the true distribution. So I'm sampling J different data sets, like 100 images of cats and dogs, and then another 100 images of cats and dogs and so on. Then I'm sampling all these models. And then inside here what I'm going to have, I'm going to have like the generalization error of all these, which is J capital, okay. Then I'm having here the generalization error of all these results. 

Where the X, S, Y, S, they come from the true distribution. Okay, so it means that I'm sampling a data set and training my model and then I using finite data to get the behavior of this model. And then I'm getting another training set and I'm doing the same process. 

This is just writing this integral above using a Monte Carlo integration. So this is setup two, and this is the best thing to do, but in practice, but it's very hard and it's very time consuming as we're going to see later. And most of the times, I would say that the setup one is what you're interested in. 

If you have a particular data set list and you want to have some conclusions about this one. And because of course it's simpler, we're going to check setup one today. But setup two is the general quantity and this is better to do in science. What I can tell you maybe for setup two is that what you have to do, you have to run cross validation one time. Then you perturb your data, then you run another cross validation more or less, then you do this thing again and again and again. 

Many, many times. And hopefully you get closer to the true quantity. This is never the case, so you don't reach the true quantity, but you're expecting to get close to the true quantity. Paranthesis, what we are looking into these days is what we know at the moment as the best techniques to estimate these numbers. 

Maybe in 10 years from now we get something better. So we haven't solved this problem of estimating, analyzing, or narrowing. It's a very hard problem. You try to predict the future. But let's focus now on this particular problem, having just one data set and trying to understand if two models perform similarly or different. Or for one model, how well this is going to perform? What is the confidence interval of one model? The first thing is the hypothesis testing. So the hypothesis testing tells you the following. I have model one and model two. 

Will they have the same behavior if I train them on this particular training set or not? That's the hypothesis testing. And the confidence interval just tells you your estimator, so what you predicted as your error, maybe the interval where it is lies into. So if I predict 90% accuracy, I'm going to say this goes from 85% to 95%. 

That's the idea. For the hypothesis testing, we're going to use the p-values. I'm sure that many of you have seen at some point in your life, these p-value things, we're going to discuss it again. And the confidence interval, we're going to see that we set the significance level and we're going to take like this confidence interval. Of course, if you increase or reduce the significance level, these two values can change. So how big your confidence interval could be? If I have like a very low significance, I would say yes. My confidence interval is from 0 to 100% accuracy. This doesn't help a lot. 

But let's try to give some examples which are not necessarily for machine learning. Let's say I'm collecting the heights in this room. Okay. And my hypothesis is that the heights in this room are around three meters on average. 

Okay. Then I have to make some assumptions. Maybe the assumption is that the heights, they follow a Gaussian distribution. So I'm getting my hypothesis here. I'm saying the mean values of the heights. 

They follow a Gaussian distribution and this is centered, let's say, at three meters. Okay. Because I'm kind of, you know, stupid. Then I'm collecting here, we have a random set. You are not the same people who are going to come here next week. 

So you are just a random set. I'm collecting all your heights. I'm getting the average and let's say the average comes around here. What intuitively means this, you know, this plot I made. But my null hypothesis that the heights are around this value. If I estimated this one from the random set I had here, probably is wrong, right? My hypothesis is not correct. If I changed my hypothesis and I make it 170 or something and I do the same steps again, it might be that this comes around here. 

This is not anymore three, but this is 1.75, something like that. This would mean I had the hypothesis, I computed an estimator of the average heights and this goes near the center or near my guess for the heights. And that's the hypothesis testing basically. For the first one, the p-value is going to be very low because on the tails of this distribution that they assume. And for the second one, I'm going to have a high p-value and I'm going to say I cannot reject the null hypothesis. 

We're going to see more details later, but that's the intuition. That's what you have to think about when you see hypothesis testing. Of course, this is for heights and we don't care here for the heights of the humans, but we care about muscle learning point of this. It doesn't make a big difference. You can just change these numbers with accuracy. You can say if I have two models, if they are the same, what is going to be their difference? 

It's going to be around zero. The difference is their accuracy. That's the only difference. You change the way how you manipulate the numbers, but the idea is the same. 

I'm having some quantity. I'm assuming something that is true, like the two models, they behave the same. They have zero difference. 

I'm collecting some random set. I'm testing my models and then based on this estimator, I'm checking if this follows my assumption. And for the confidence interval, it's more or less the same story. Now I'm not computing. I'm not having this type of hypothesis. I'm having a hypothesis about the distribution and by collecting the data set and doing some manipulations as we're going to see later on, we're going to say that the estimator, if I take another training set, probably is going to be around this region. 

Okay, it's a little bit different approach, but the result, I think, is against intuitive. It tells you if I had an assumption about the distribution, I collect the random subset, like the people inside here, where is going to be with high probability inside the true estimator? Now we're going to save one example for a machine learning, hopefully. So we have a binary classification problem and we have 200 samples and we do 10 fold plus validation. Okay, so we have 10 different estimations of the test error. 

Okay, and these are the 10 dots over here. Don't bother about this dimension. It's only to have some thickness here so we can see the data points not be one on top of the other. 

What we care basically is how much they are distributed along this axis. And if you see, here we go from 90% accuracy more or less to, I don't know, nearly 60% accuracy. And this depth line over here is a baseline model. The baseline model is something like whatever comes, I predict it to class A. And because maybe class A has 120 points on the training set and 80 from class B, this means that my test error on average is going to be 60% if I predict everything on this B class. 

If I have 100 points and 60 of them are class A and I'm saying whatever comes is going to be class A, this is going to be roughly my test error. Yeah, accuracy, no, sorry. Here we'll have the accuracy. Anyhow, the estimation, accuracy and the accuracy of a stupid model. Now the question is how different is my training model compared to the stupid model? How different they are? Here what we see is the confidence interval. 

The confidence interval roughly speaking is this area over here that covers my estimation of gelization error for these 10 different faults. Okay. And what we see here is the p-value. The p-value tells us if the p-value is low, it tells us that the two models are different. So in this case, a very low p-value suggests that the two models are different. We cannot reject the null hypothesis that the two models are different. Okay, we don't have enough evidence to reject it. So I'm saying, okay, I cannot say for sure that they are different, but I don't have enough evidence to say something different. With what I see, I think they are different models. That's what the p-value tells us here. 

And that makes a bit of intuition, the 20 different sense, right? I have the model side for the confidence interval and I get the p-value. Now let's say that I'm taking another training set from the same distribution that has 2000 points. And I'm training again my model and now all my estimations are coming over here. Once more, we do 10-fold cross-validation, which means I have a full training set. I'm separating that in 10 different ways. And then I'm having one test error for each one of them, right? I'm having 10 dots again, but here I'm very much concentrated. 

And maybe I do here, you know, 20-fold cross-validation, something like that. But in any case, what we see is that they're concentrated around this point over here. The confidence interval now is very, very small. It's very close to the stupid model, but the p-value still is very small. So what we can say here? Any comment? Yeah. Yeah, okay. 

So if they don't overlap, the comment is that the p-value has to be low and I agree. But is this a good thing? Will you say if this costs one million crores to deploy this model in practice, will you do that? Exactly. 

Maybe this isn't a very good idea. Because, okay, the p-value tells me that the models are different, but here I see that the effect size is very tiny. It's like almost 5%. So maybe it doesn't make sense to spend a lot of effort and money, for example, to deploy this model in practice. Even if they are different, the difference is tiny. So maybe it doesn't make a lot of sense to do that. That's why I'm saying maybe looking only at the p-value is not a full story. 

The best thing to do is to plot also these numbers to get an idea how they look like, maybe plot the confidence intervals and finally using the p-values to conclude about something. So now if I had a boss maybe and I had two different models and I was going to share to report, I would say, look, we have these two different models and I would say this one, let's throw it away because it doesn't matter like our time and effort to deploy this model. It's going to be a waste of time. Maybe let's focus on this thing over here. 

Okay. Another problem of the p-values is the following. If here we see that the difference, all of these values are very much concentrated and above this stupid model. And if I have many, many of these estimates that are just above the average, sorry, average, about the stupid model, they are just above this one. 

But I have many of them, the p-value is becoming more certain. But this is not always good. So this looks, let's put it as follows. I'm taking many times that my assumption holds, but I don't see how big this effect size is. So I'm taking always, for all these test errors, all of them are better from this baseline stupid model, but I don't see how better they are. And I would say that this is not ideal. I would say this is an okay tool, but they're not ideal tool to use in practice. The confidence interval is a bit better. 

Yes. Now, why the p-value again, maybe this is like a small manifest about p-values, but why the p-value is not good. Let's say we have this 200 points as the beginning. We do the same thing cross validation, 10-folds. 

We get these estimates. Now they overlap and the p-value is high. So yes, I cannot, I would say that the two models are different. Sorry, they are the same. Yeah. 

Okay. But now let's say we change a little bit the fault. So we just suffer a little bit there and we do it again cross validation with 10-folds. And we see that now our estimates, they change. 

Okay. Now the confidence tell us they don't, they don't include inside this baseline model. And the p-value is very small. So I'm going to say that the models are different. Okay. 

But is it true? This is just because I suffered a bit my data set and I did the same steps again. I get a totally different p-value. So that's a problem with the p-value that if you make small changes, if you suffer a bit your data, if you consider different splits of your full data set, you might get a different behavior. And that's what they say gives a lot of false positives. 

Even if there is an effect, maybe you can destroy that by using many false, many soft for example, the data set for softening. And that's in some sciences, for example, when they do p-value testing, they can kind of make these tricks, you know, try many, many times splitting differently the data set to get a significant level like the p-value to be very small. And they say, okay, we have a difference. But this is not always the case. It might be because of the particular behavior of this particular split. So don't trust the p-values. Yeah. Okay. If you have a lot of outliers, what is happening? 

It depends. You have to compute the numbers. But let's say if here you have like some points over here and over here, probably your confidence interval is going to be much higher. 

Okay. And then the p-values again depends if they overlap or they don't overlap, they're going to change. All this discussion in the last five minutes was for you to tell you first plot, maybe the estimates you have, compute the confidence interval, compute the p-value and try to make a conclusion using all this information. Not blindly taking maybe one of them and say, okay, that's a low p-value. So I'm rejecting an all hypothesis, something like this. Okay. Don't do that. It's going to be wrong most of the times. 

I'm not wrong, but it's going to tell you the full story. Because if I resuffled my data set and I do a gain-prose validation, I might get a totally different p-value. Now, let's try to see in practice how we can do these steps if we have a particular training set. 

I'm going to consider just the holdout method because it's simpler. So we have the training set. We have the test part. Now we'll keep for testing. This part is for training. Testing training. So here with a test set, I might have a hundred points. 

Okay. So what I have here is like 0.1, 0, 0.1. So Z3 is 1. 800. I don't know, this is, let's see, two extra values, but let's see, you have this kind of numbers, okay? 

Let's say I'm just taking, not that, you're asking per se, but if one model works well or one model doesn't work well. I'm getting just some numbers. I'm testing my model on 100 points on a classification problem, okay? So I'm having 100 points here, I'm training a model and I'm testing this model. And I'm getting here, if I predict correct or wrong one label, one particular data point. Makes sense? And I do that for two models. I take the rubberage and I subtract it and this over here. I'm gonna repeat because I think it was a bit confusing. 

Taking the training set, I'm splitting that, keeping that for train, I'm taking that for testing. Okay, here I have 100 points. And maybe for the first model, for model one, this gonna be my predictions, if it's correct or wrong for this 100 points. And I have, for the model two, I have a similar thing. 

So I'm just taking on this 100 points, how one model behaves and how the other model behaves. And I'm taking these values here and I average them out. And I just subtract them. Okay, that's what we did last week if we had just one model. That's what we did when we had a holdout technique to estimate the realization error. And now we do that for two models and we just subtracted values. 

Makes sense? But now let's take one step back and let's see these numbers, this G1 to G100, as just some random data. Okay, because this was a random split. This was just a random data set basically. This 100 points was just a random test set. So I can consider this as a big data set of 100 points, of n points. This is similar to the heights in the room. But now instead of heights in the room, I'm having, did I predict something correct or if I predict something wrong? Just a random variable. Whenever you hear random variable, you can think about, I'm having 100 numbers and each of them takes a value, let's say zero or one with some probability. Okay. So now I'm making this thing abstract as I did in the previous step and I'm having let's say 100 points or n points and each of them, let's say zero or one with some probability. 

And the probability is if the model performs correct, that you are so the model basically, how many times the model predicts correct. Okay. And now I want to actually draw conclusions for this number over here. 

If I subtract the true generalization errors, but what I have eventually, I have just an estimator of this one. Make sense? Okay. 

Once more. I'm having now my test set only here. The part of the training set I kept out for testing. And here I have 100 points, let's see. And for this kind of points I have two models. This is for model one, I predict one for model two, for model one, this gives one. Like I predicted the label correct or wrong. For model one, here I predicted something wrong and I have the same thing for the model two. Okay. So I have just the predictions for the two models. And I just subtract all these values. 

Okay. Here's gonna be zero, here's gonna be one. There's gonna be minus one. 

This is it. Just taking some numbers. And now why I said this is a random variable because I just took a random test set. If I take another test set, these numbers are gonna change. Again, they're gonna be one, zero or minus one, for example, but they're gonna be different numbers. That's where we want to model them as a random variable. And that's the true quantity we want to estimate. 

But we don't have the true values as we said many times. We'll just estimate that by using the numbers from the test set we took from my training set. Okay. As I said, this is a random variable because it takes random values. If we simplify the problem, let's think about the coin. We have a coin and we toss this coin 100 times. 100 times. If I do it 100 times, I'm gonna get a sequence of hidden tails. If I do it another time, again, 100 times, but at another time in the day, I'm gonna get another sequence of hidden tails. So each of the sequences is a sequence of random variables, the outcome of a random variable. 

Makes sense? And because, for example, with a coin, this is just a zero one problem, we can say, yes, it makes sense to use a Bernoulli distribution to model this. This is just our modeling decision. I have a problem that takes zero or ones with some probability. 

So I'm using the Bernoulli distribution to model this problem. Okay. And that's what we do here. We say, if we have a sequence of random variables, you can think about it as tossing a coin. And let's say one is a head, zero is tails. And I'm doing that and I have a sequence. 

If I do it again, I'm gonna have another sequence of hidden tails. But each of them, it comes with some probability. And this is, let's say, the Bernoulli distribution. 

So if I fix the theta here, and for the coin problem, it's 0.5, it's gonna tell me what is the probability to have head or half tails every time I'm tossing a coin. Okay. And now I'm taking the sequence I have, because I collected that. I did one experiment, I took a coin, and I just start tossing that 100 times. I collected 100 numbers. Then I'm saying, what is the probability of all these things happening at the same time? Okay. That's what I'm asking. 

Okay. I have the sequence of random numbers, but now I have observed them. I did one experiment, I have an observation. 

What is the probability under my modeling decision to use the Bernoulli distribution? And then we have this IID assumption that we expect each of them to be dependent from each other. Every toss could be independent and to follow the same distribution. So now this pool join becomes this product of individual distributions. That's a classic recipe in machine learning. We have a modeling decision about your problem. You collect a data set, that of course makes sense. 

These two things, they have to make sense. And then we have the full join distribution that tells me under my parameters theta, how plausible is the observation cycle? For the coin problem, if I do the steps and I set theta to something, not 0.5, but one or zero, I'm gonna get a very small number here. If I put theta around 0.5, this number is gonna be on the top. 

So I'm gonna have something like that and this is gonna be maximized at 0.5. The probability, the likelihood. Makes sense? You have to understand this, it's a bit abstract. 

You have to abstractify that. When you have in machine learning a problem, you have the random data set that comes. So you model one of these observations with random variable and then you try to find the parameters that explain the random variables you collected, the observations you collected. Okay, and now we do the same thing, but instead of having data, like tosses of a coin, we just have the outcome of an experiment. 

If it's one model better or the other model better. Again, it's a random experiment because we just collected the random test set and we tried it. That's precisely what is happening. Instead of having a tossing a coin with a random experiment, I'm training one model, I'm training another model, I'm collecting 100 points, then I'm testing my models on these 100 points and I'm getting a number for each of these points. If one model performed better or the other, this can be seen as tossing a coin. 

As a, you know, from the scientific viewpoint, as a modeling problem is the same thing. Now what we want to do, we want to use statistics to argue about hypothesis testing in confidence intervals. What are statistics? It's just a fancy name to say, if I have a random variable and I take the average, for example, or I take the standard deviation of this random variable. Okay, so if I have this a random dataset and if I put it in a function, that's the statistic. And we're gonna check the estimators of the statistics. So if I'm giving here a particular sequence that I've collected the observations. Let's see what is a confidence interval first. So the confidence interval, I want to get from my particular training set and I want to get a closed interval that likely contains the true value, theta, the true parameter. 

The true parameter when I'm costing the coin could be like 0.5. Whether two models are performing on some task, this value can go, I don't know, from minus one to one maybe. Which model is better? The first model or the second model? Or if I'm just taking for one model, how well this model behaves, this theta can take a value between zero and one. 

The performance of my model. So the confidence interval is a function of the data I have because if I change the dataset, the confidence interval is gonna change because I'm training my model in a new training set. I'm testing that on something new on a new model. So this value is gonna change the confidence interval. 

So depends on what I've collected as a training set or test set. And what is technically, how technically defined the confidence interval is the probability the true value theta, this is the true performance of the model, to be in the particular confidence interval I've computed for this particular training set. That's what the confidence interval tells you. For a significant level, significant level, what is the probability if I have this confidence interval, the true theta to be inside? Okay, so if I set this one, this determines how big is this confidence interval. 

So now a question for discussion. The probability here is with respect to what? We said for the probability what you have, you have an event and you try 100 times and you see how many times this event was true or false and this gives you the frequentist probability. What thing inside here is stochastic? So what changes? Is the theta or is the confidence interval? Yes, exactly. So the confidence interval changes because we have, if we have a different training set, or test set, we take a training set and we take a split for the test, this is gonna change. 

So the confidence interval tells us, for example, if we had 100 different training sets and for each of them I was considering a test set and I was doing the same process, how many times the true theta would be inside this one? Let's try to draw it. Another advice is whenever something like that happens, try to draw things down. Many times it becomes easier. So let's say this is accuracy and this is the true theta, so the true accuracy of a model. 

I don't know it, but it exists. And let's say I'm sampling a particular training set and I'm doing cross validation, I'm getting 100 points, I'm testing my model on 100 points and these 100 points, they go around here. Okay, now my confidence interval will tell me that it's something like this. 

Okay, if I do the confidence interval estimation. Now I'm taking another training set and I'm doing the same process. I'm collecting another 100 test points. 

Keep in mind that this is random, right? Maybe it's totally different training set, maybe the same training set by just sampling the points. But the whole point is that the test set is gonna be different now. Now I'm getting another confidence interval. I'm doing that another time for another training set. I'm taking some values over here and I'm doing the confidence interval and so on and so forth. If I do it 100 times, what the confidence interval tells me that 95 of this time, if the alpha is 0.05, like 5%, this theta is gonna be inside the confidence interval to 95 of them for the 95 of this training sets and the confidence intervals. 

And for five of them is gonna be outside. That's what this point tells you over here. So in practice, what this tells us that if I take a training set and I compute the confidence interval, 95% of the times I'm gonna have inside the true estimator if the alpha is 0.05. 

Okay, that's what it tells us. It doesn't tell us that all the time is gonna be inside, but depending on the alpha I'm selecting, it tells me how many times I'm expecting the true theta to be inside this interval. It doesn't tell me that for this particular training set that's gonna be inside. It might be like if I had this case, so I had this training set and I was doing this technique, I see that the true theta is gonna be outside, but I don't know it, okay? So you have to keep that in mind. So the confidence interval again makes mistakes. It's not silver bullet again. Let's see now what is the hypothesis testing and after that we're gonna take a break. 

So please be a bit patient. So for the hypothesis testing, we have two hypotheses that two models, for example, are the same or the two models behave differently. You can think again about the problem of the heights. 

My assumption is that the heights in this room, they're around three meters. So I'm collecting all the random points I have inside here, all of you, because you're random. Maybe next week some of you will not come here and some of you, some different people will come. 

So I'm gonna have a random set again, a different set. But now if you are random variables, now for each of you I have an observation. Let's say the seats are random variables. 

Whoever sits there takes a value. So now I'm having some values for all of the random variables in this room. So I can compute an estimator of the heights. And my assumption is that the heights here are three meters or whatever. So I'm saying this is my assumption. 

I'm collecting the random set I have here. I'm getting the estimator and the estimator comes here. So what the hypothesis testing tells you that if the next time I'm gonna do the same thing, most probably I'm gonna get the value again around here. And if this goes to the tails of this distribution of this assumption, probably my assumption is wrong. 

That's the intuition of the hypothesis tester. Okay, and now instead of heights, you can have the performance of two models. You can say if the two models behave similarly and I test them and I take their average blah blah blah, what I compute is gonna be around zero. 

If they perform differently, one of them is gonna win. So instead of having here the three meters maybe, and I have only two models, this is one, this is minus one, and this is zero. So if the models perform the same, I'm expecting my estimator to come around here. So they have one of them 90% accuracy and the other one 91% accuracy. 

If I subtract them, it's gonna be near zero. But if one of them performs always good and the other one always bad, I'm gonna come probably over here. One of them is gonna win. Or here if the other one is winning. 

Makes sense? Try to understand that these are, again, mathematically speaking, the same problem, but you have just different data now. Now a small discussion about the p-values. So the p-value, when we compute the p-value, this is low, like over here. We reject the null hypothesis, but this does not mean that the alternative is true. 

That the heights are not three meters. It might be that the sample set I am having here is ideal. Okay, or it might be that I have a wrong model assumption, of course. 

It might be that my assumption is correct, but I did the mistake somewhere inside, like collected wrongly data set, the estimator is not correct, something, I did some mistake, and that's why I'm not getting a high p-value. But it gives you a hint that under your assumptions, under your test data, something is wrong. Your assumption doesn't hold. The hypothesis doesn't hold, sorry. On the other hand, if you get a high p-value, again, this does not mean that the h0 is true, but we don't reject it, but doesn't mean that it's indeed true. So for some strange reason, let's say maybe you are here very tall people and I'm getting an estimator that's around 2.10, and I'm getting this stupid assumption of three meters, but maybe because here for some reason, 10 people came or all of you, you were really tall, then I'm gonna get a high p-value, but this doesn't mean that my null hypothesis is true. It might be because of this particular test set here, it seems to be true. Okay, but again, it gives me a hint that might be true. How we do that in practice, we have the hypothesis, let's say the two models behave the same, or the heights, whatever it helps you to think about. Then we want to compute an estimator, and we do that by using the test that we have, okay? And these are technicalities, you can read it on the book. 

But the whole point is what I described earlier. You have an assumption, let's say the highest, the Foller-Goussen distribution, and the three meters is the average. Then I'm collecting a test set, I'm computing the estimator, the estimator might come over here, then I'm computing the probability of this one and this one. So the probability of getting an event that is more extreme than what I've observed already. And that gives me the p-value. So the p-value is the sum of these two probabilities, okay? And if this is very small, then I'm rejecting the null hypothesis. 

If the estimator was coming here, so I'm integrating these things over here, this has high probabilities, I cannot reject the null hypothesis. It means that my estimator is not totally off, it's plausible if it comes over here, for example. And that's what you see here, written mathematically. 

But if you draw it, it looks something like that. Okay, so what we see before the break, we started by checking again the cross-validation, how we can use it to estimate the performance of one model. What problem might have, if we have many models that we can underestimate the error, we discussed a bit about the two-level cross-validation, then we check some examples, why it's important to do statistical testing, maybe confidence intervals, because for the cross-validation, we don't have just one number, but we have many numbers behind the number that we report. What happens to the different faults, for example. 

But let's try again now to spend 10 minutes repeating what we saw before the break. Let's say I want to draw conclusions for just for one model. Okay, now I have just one model and I want to estimate the generalization error. This means I have to use infinite or infinite data, okay? 

All the data involved. Of course, this is impossible. So what I want to do instead, I'm going to take the full dataset, I'm going to keep that for training and I'm going to keep that for testing. You heard it many, many times, but trust me, this is very important. Okay, so this for training, this for test. And let's say my test data, I'm having 100 points, okay, for the test. 

I'm just having 100 points for the test data. Think about it as a random set, because if I do again cross-validation by suffering a bit my data, there will be have, you know, a different model, of course, and different test points. So this is just a random test set. Think about it as a random quantity. But this random quantity, I evaluated. 

I can give like a value to this random variables. This is my observations now. The observations for one model, if it performed correctly or wrong on the test set I have. 

Okay. And this what I have over here is the estimator of the generalization error. So the generalization error, later on we're going to call it theta, is the true quantity, what I cannot compute in practice. So what I'm doing, I'm doing the cross-validation technique, let's say with the holdout method to estimate it. And we call it estimated, say estimator, because this is just for a test set. We might have another test set and this is going to have a different value. Okay, but for this particular test set, this is the values, let's say that we compute it. 

One that performed correctly on our data point, and zero it performed wrong on one data point. Make sense? Okay. Now the difference is, and maybe I made one mistake here, the difference is you have two models. We do that for two models and we just subtract their numbers. So for model one, we do the same, these steps, we do the same thing for model two and we subtract their numbers. That's the only difference. Okay. 

And maybe now that was a bit of a misleading maybe what I said. Now the z is not going to be from zero or one because I subtract these numbers is going to be from minus one to one. But that depends, depending if we have just one model or two models, let's consider only the one model for simplicity. So we have this one model, we separate training from tests. We train the model and train, we test on the hundred points I collected for the test. Okay, so I'm having now 100 random variables with some value. 

It's like tossing a coin 100 times. And this is what I collect here. This data set D. Okay, this is a fake data set. It's like the random set that I collected the test set. 

And that's what I said before. So if you see here, we had it as Z Z D, calligraphic Z. Here is theta, here is theta hat is estimated from this quantity. If you are getting problems with the errors and the accuracy and so on, think about tossing a coin 100 times and your friend tossing another coin 100 times. You collect, you know, this number and you subtract how many times you had hit person A and how many times that hits the person B. That's the thing. 

Okay, literally, it's so simple. But now instead of tossing a coin, you have the result of the class fire. And why we need this random variable discussion, and you will see it many times in muscle learning, basically, because this helps me to make modeling assumptions. If I say this random variable that is binary, I can use the Bernoulli distribution to model one outcome. 

Okay, and now I have, oops, I have 100 outcomes here. By using this IID assumption that the data independent and identically distributed, I can break this joint to this product of unimodal distributions, univari distributions. So the whole point, it was collected a test set, a random test set, and tried to see what is the probability of all these events happening at the same time. And now I'm trying to find a theta that makes this thing very probable. And that's going to be my estimator, for example. 

Okay. And that's what we do precisely for the confidence interval. And that's what we exploit for the hypothesis testing. Now let's see in practice how we do that. If we have now, we have real data. 

Okay, real data come. Let's say we use cold out technique again. And then, oops, we have a split train test set. We have here some observations like hundred observations for one model only. Okay, is zero or one, if I predicted correct or wrong. And then I'm doing a modeling decision that each of them follows a Bernoulli distribution. I'm doing my idea assumption. 

So I'm getting now a product here of hundred numbers. And I want to see what's theta will increase this thing, this probability. For the coin example, if I have like some observations from a coin, the theta that's going to make sense is going to be 0.5. Now for the classifier, we say that we give one if the classifier predicted the correct label or zero otherwise. What this means if a classifier performs very well, so predicts very many times the correct label, which theta is going to make sense? A theta that is high. 

That's the infusion. So here we have the number of accurate guesses. And if you remember from lecture four when we had this dog and the coin example and tossing the coin and the dog there, similar, we said that we have this likelihood. So the likelihood was this term over here. We had this likelihood. We're multiplying here a prior. What we thought is the correct theta. 

And maybe this just uniform gives to all the theta the same weight more or less. But eventually we came out with a posterior distribution. And what is the posterior distribution? For this particular problem, the theta is going to be from zero to one. And the posterior distribution is something that for the particular train set I have here, for the particular observations I have here, can be training set, can be test set, can be tossing a coin, whatever for the particular experiment I had. It tells me where this thing is going to pick. And this is going to be the optimal theta. 

This optimal theta that maximizes this likelihood I have. And that's what we use for the confidence interval. So for the confidence interval, we run our classifier on the test data. We see how many times we predict correctly something, the number of accurate guesses. 

We model using this Bernoulli and Berra distributions. This posterior, of course, you don't have to remember all these details. But you have to understand what this means. 

This posterior gives you a shape like that for a particular set of observations you have. And what this would mean? That the theta picks here around this value. So this is going to be the prediction of my classifier, like the average of my predictions on my test set. And this is going to be the confidence interval. 

So we take some observations, the test set, I'm checking how my classifier performs on those. I'm collecting these numbers. Then I'm modeling, sorry, I'm modeling this posterior distribution. I'm taking the expectation of this distribution that comes over here, somewhere over here, where this picks. 

Okay. And for the confidence interval, I'm inverting the CDF. But basically, what I'm saying, if the alpha is 0.05, I want here to have 0.025 percent, 2.5 probability. And here I want to have another probability. 

And here I have 95 percent of the probability inside this interval. So this formulas, I know make no sense if you just look at them, but just try to draw something like this. I have here the parameter, I'm getting some observations using the modeling assumptions I made. I'm getting a posterior, that gives me this function basically. And I want to see where this function is maximized. This is maximized here on the expected value. And then I want to have here some probability and here's some other probability that sums to 0.5. Okay, I have to invert the CDF. These are technicalities, but the whole intuition is that I want to keep here the 95 percent of probability around my expected value. 

Makes sense? This one, of course, if you need to do it in practice in the future, you cannot do that with pen and paper. You need to use a computer or some libraries in Python. But if you understand how this thing works, then the other thing you can design, is not a big problem. Then it's just standard steps. If I change, for some reason, this one to another problem, but I know the correct recipe, then I can do that. 

If I know that I have to collect some data, come up with a posterior distribution, then I can plot it somehow, then I know where the expectation is going, and I know how I can compute the lower and upper bound of my confidence interval. Okay, so try as much as possible to draw down things when you want to understand an algorithm or an approach. Now, if we have two classifiers, it's more or less the same thing. We predict how many times one classifier performs correct, and in which case is the other one predicts correct. 

And here we have the following numbers. We have, if both classifiers perform correct, if both perform wrong, if one performs correct and the other wrong, and if they are one with other performs correct and the first wrong. Just counting, how many times both of you gave the correct answer, how many times both of you gave the wrong answer, and their combinations. And once we have these numbers, again, we do the same steps. Now, we come up with a different posterior. 

Again, it's a better distribution, but it's a bit more funky, I don't want to explain now why, but this is a bit more funky. Again, a posterior. That's maybe what you want to pay attention to. This is the expected value, so it tells you how different the two classifiers perform. And this makes some sense, because it tells you how many times the first classifier performs correct and the other wrong, minus the opposite thing, how many times the second one performs correct and the other one wrong. And you divide by the number of points. Okay, of course, this looks super ugly, but this is not so ugly. And what you're expecting to get here, before we have this one classifier, and you come from zero to one, the theta is between zero and one. 

But if you remember before and after the break, I told you, you have two classifiers. Now, what your optimal parameter will be, what's the interval, can go from zero to one or from somewhere else to somewhere else. What are the numbers that you can play inside? And I guess, exactly, so you can go from minus one to one here, because it can be that this guy is always correct, this minus one, or this guy is always correct, so you get one. So now your posterior, if you understand the recipe we saw a couple of minutes ago, it's telling you that I'm going to get like, depending on the training set, or the test that I'm getting, I'm going to get the posterior. And if it looks something like that, the posterior is going to say that this guy performs better, because the optimal parameter is somewhere around here. 

If the optimal parameter was around here, then I would say that the two classifiers more or less perform the same, if I had something like that, as a posterior. Makes sense? Getting the intuition is more important than understanding all the details here on the page. So if you understood this one, how to do that, then as before, to get the confidence interval, we just have to invert this thing and compute the, let's say for the blue one, this is the things I want to keep out and keep the probability that the true parameter is inside here. It's the same thing as we did for the one model, but now we have two models. It's just the posterior is changing a little bit, but it's the same recipe. And for the p-value, same story we have here, okay, for the p-value, we don't compute like how, where the confidence interval, but we want to see if the two models behave similarly. And if we had this posterior, what you can say, that they perform similarly or not? 

Probably not, right? Because it seems that one classifier performs better. So now if I want to compute the p-value, I want to integrate these things, and probably they're going to have very small value. And so they're going to get like a rejection for the null hypothesis, they perform similarly. If it looks something like that, then because your confidence interval is around the zero, it means that the two classifiers, they'll probably perform similarly or your estimator is going to tell you that. Because maybe your estimator comes over here, so for the p-value, the estimator comes over here. So if I integrate that, maybe it's above the mic's significance level. 

Okay. So for the hypothesis testing, it's like the heights in the room. We say we have people in this room, I'm expecting the height to be 175. 

Okay, I'm collecting the heights and maybe for some reason we are very tall in this class, and it comes here around 2.10, my estimator. So what I'm doing to compute the p-value, I'm computing the probability to be over here or to be over here. So I'm getting the extreme scenarios. And if this is very low, I'm rejecting my null hypothesis. But this doesn't mean that the null hypothesis is indeed wrong. It might be that for some reason today we had all the tall guys in the room inside here. 

Okay. And the same thing for the classifiers. Most important to understand is that having a random variable, you make some modeling decisions, you come up with a posterior, you plot the posterior on a paper or on your brain, more or less, and then you try to see how it behaves. If it favors one model or the other model, or if it's around zero. That's the real intuition you need to keep in mind. And now we can do the same thing for regression. The difference for regression is that we don't have zero one. 

So we cannot use the Bernoulli distribution for our modeling, but we can use maybe Gaussian distribution for our modeling. Okay. Again, it's exactly the same steps. But now, because our values, they're not zero or one, predict correct or wrong, we have to change our modeling decision. And what is our modeling decision? My training set is a product of Gaussian distributions. 

Sorry. My likelihood from the observed random variables is a product of Gaussian distributions. So every error I'm making, this is the prediction and this is the true value of the output. 

Every one of these errors follows a Gaussian distribution. That's my assumption. Okay. And if I do this assumption as before, we come up with a posterior. Now the U is the estimate error I'm going to do for the regression. 

Okay. I'm getting again a posterior. And by using this posterior, I can do the confidence interval or the p-value. 

I know they look very ugly. These things, you need to remember this formula. But you need to remember again the steps. I'm having a problem. I have a regression problem now. I have continuous values. So I cannot use the Bernoulli distribution. I'm using a Gaussian distribution for my errors. So now my errors is not that I predicted correct or wrong, but how far I predicted the, how big is the difference from my prediction from the true value. This is just a continuous value. So I'm saying that here is an estimation for the U and here I'm getting for my different training set, wait for my test set, the errors. 

Can I find the correct U such that this error makes sense? That's the idea. And the steps are the same. So we assume that each of them comes from a Gaussian distribution. We make a product of all these Gaussian distributions. This gives a student T distribution as a posterior because, okay, these are some steps in the book you can find them. 

But these are just tedious. We come up with a posterior distribution. The student T looks something like the Gaussian, something like that with heavier tails. 

But overall, what we, what we do, same story. Here are the estimations, like my error basically, maybe for five fold cross validation, each of them. I'm doing the same steps. I'm coming with a posterior. This is the posterior. And here maybe this one is the true value. I don't know it, but I'm going to compute this expectation. So this is going to be my best guess. 

And this is going to be my confidence interval. How? Okay, here are the formulas, but you don't need to remember by heart the formulas. 

You have to remember how to do that. If you had the plot, how this would make sense. Make the assumption for the likelihood. Then you come up with a posterior, you put it down. Here is going to be for the particular observation you have here is going to be the peak. Okay. And this is going to be my confidence interval. Make sense? And if I now want to do the peak value, like compared to models, of course, comparing the two models, same story, each error now is not going to be like the prediction, the difference from my prediction to the true value, but it's going to be this one subtracted the number from the other model. 

Okay. So I have g, i, sorry, yi maybe y, the model a, the prediction of the model a, square minus okay, I'm just subtracting these two values. And again, I'm expecting that this follow Gaussian distribution is a modeling decision. And if I make a mistake on the modeling decision here, this is going to give me wrong results, but this is something I have to understand. There's not like a golden recipe here. 

I have to try things. So I'm assuming that they follow Gaussian distribution in these errors over here. Okay. And by doing that, then we have this product of Gaussians. We come up with a student T and if this U now is around zero, we say that the models they behave similarly. And if it's not near zero, they have different performance. Same story as the classification problem. But now instead of having zero or one, so you have this better distribution between minus one and one, you have like a Gaussian, you have something continuous. And if I want to draw something here, I would say that here's the zero and here's the collection of these differences, maybe 400 points. Okay. 

I'm just putting them on the line. And then I'm more or less centering here Gaussian distribution or student T in this case. Here's going to be my prediction, the theta hat, like the best case, the expected value for this particular observations I had, I'm going to give some confidence interval. 

And I can also speak about p values by integrating what happens outside. Makes sense. Do you have any questions? Yep. 

Yes, yes. Like let's say the test that I had was 100 points. I'm computing this value for 100 points and this value for another 100 points. And that's attractive. So it's like I'm having here for the model A and model B. And I'm having here these numbers for why now I cannot write it so clearly, but it's going to be something like that here, all these values. And going to be for the model B here in this column. And I'm going to subtract them one by one. 

Okay. These values, they're just going to come here. I'm placing them there. And then it's going to give me, because there are actual numbers, right? I can compute these numbers. It's just numbers. And if I have now these numbers, I put them inside here. So these are the number of points I have, let's say 100 points. 

These are average difference. So this number over here comes here. I evaluated this number, just the number. 

And this thing over here is also number. Here you have the formula. Here you have the formula here. 

Okay. So these are just numbers. I compute the numbers, giving the predictions of my models or from one model in this particular slide is for one model. 

I'm computing the average error of the model, the average, the standard deviation of this model of the errors. I plug them inside here. This is the number of points I have. And this gives me a distribution. 

I can evaluate that. It's like I'm giving you the center and the variance of a Gaussian distribution. But now these values, they're coming from the test set. From the numbers. It's like, assume that you have your heights. 

Okay. And I just take the average and the standard deviation. These are two parameters. And now instead of having the heights, I'm having the errors the model is doing. I'm computing the average of those and the standard deviation. And I plug them in instead of a Gaussian to a student t distribution. But these are the same steps. 

Makes sense? And of course, depending on the results I'm getting, the student t is going to change or the beta distribution before. Depending on the actual results I'm getting from my tests, the shapes of these post-itters they're going to change. And they will determine all the confidence intervals or they will determine the hypothesis testing. For example, let's say I'm doing like the classification. I'm comparing two models, maybe one model. And I'm having a very good model. Here's the theta and here's the post-itter. 

How the post-itter is going to look if my model performs well on my test set. Anyone? Yep. 

Sorry. Yes, but where is going to be centered? From here to here. Here's from zero to one. 

Where is going to be the peak if the model is performing well? Yes, exactly. So the posterior in this case, maybe something like, oh, no, this is not a good drawing, but might be something like that. So this is going to be a good classifier. Now let's say I'm training another classifier. 

I'm testing that on the same points, totally different model, and performs really bad. Where is going to be placed? The posterior. Yeah, it's going to be something like that maybe. It's going to be a bad classifier. For both of them, I can get the peak. So what is going to be the estimator, the expected value of the error? Okay. And I can get the confidence intervals for both of them, but they're going to be different. I have two different classifiers. 

And now I can take these two classifiers and compare them. Okay. And if I do that, my posterior now is going to be from minus one to one, because now we subtract the numbers. 

Okay. And what I'm going to get here in this particular example, what is going to be the peak of this distribution of this posterior? What I'm comparing to classifiers, these two classifiers is model A and model B. Around zero, why? Yes, but around zero, it would mean that they perform the same, right? They have the similar performance. Maybe you want to change your... 

Speaker 2: Okay. No, no. 

Speaker 1: It's like, you know, tell me your opinion. Where should be placed now? Let's say we have these two models and I'm getting the posterior if I subtract the two models. Yeah. Exactly. Now one classifier, let's say here's model A. 

This is going to come around here, or even worse, going to come like... Okay. Because this guy performs better, the model A. So now if I comparing the two models, my posterior is going to go towards model A, for example, right? 

Makes sense. If my classifiers individually, they look something, I don't know, like this one, this one, then yes. If I subtract them, they're going to be placed... 

They're going to be placed around here. Let's say for this scenario, because they perform more or less similarly. Makes sense? So try to understand it by doing these kind of drawings. I know that the details and the posterior and all these parameters there, they're very ugly, but that's the whole point. 

If you understand these three things, like one, two, three, that's it. And okay, about the parameters here, the numbers and so on. If it makes a bit complicated, think about predictions of classifiers and so on and so forth. Think about a random experiment that can be zero or one. Like can be if you have two students, the number of correct answers or wrong answers they give to the exam. 

Okay? If this guy has better performance, this guy has worse performance. And then you get like that each answer comes with some, you know, it's a random variable. And you do this random variable modeling because you want to come up with a recipe. 

I'm getting the Bernoulli, I'm doing the likelihood something, I can get like a formal posterior. It's just a rule we have, or say, a recipe you have in muscle learning to be able to do something. We need to follow some rules, some recipes. Yep. Yes, exactly. 

This could be like, I don't know, here's the 0.5 maybe. This guy performs bad. It's a bad model. It's a bad student. It's a bad model. 

I don't know. It's a bad policy for advertisements, whatever. You did this model, you tried it in 100 cases or 1000 cases, and many of them fail. That's what it shows this plot. Okay, the red one shows that this system, this technique, this student, this model, I tested that 100 times, 1000 times, whatever, and performed many times correct. That's what it shows. Makes sense? 

So this is something like that you have to understand. And the same thing for the regression case. But now for the regression, the only difference is that you don't have better distributions between 0 and 1, but you have something in continuous interval. 

Okay, and these are the errors. So what's type of cross validation? Today we discussed about the setup 1 and setup 2. The setup 1 is what we saw this time, that we have a particular training set, and we want to draw conclusions for our models on this particular training set. 

If one model, the confidence interval of the model, if the two models behave differently, and so on and so forth. The holdout technique we discussed many times is good if you have like a lot of data, so you can make a reasonable splits. And last week also we highlighted that the live one cross validation is very good for the setup 1, because you focus very much on this particular training set. 

You exploit very much this training set. If you remember we said that the live one out generates n test errors, like for every point at some point, every point becomes at every loop test point, right? So you have n test points at the end. So you have many, many test points to play with this statistical techniques. 

That's why it's also very good. So results are going to be good if you have enough data, we can compare many models, we're not going to see in this course. What I would say is that focus on the estimated effect size, so check the confidence interval more, not the p-value, we have seen before the break why, because the p-value sometimes is misleading, you have just a little bit different performance, and still the p-value tells you the models are different, but maybe it's not reasonable to do that one or the other thing. And you use all these techniques transparently, say what you did, say your assumptions, if you do statistical testing, try many iterations and then say something about it. So if you do, if you suffer your data set, compute the p-value, you reject the null hypothesis, and then you suffer the data set again, and then you compute the p-value, and now you accept the null hypothesis, then you have to mention that, so that it's not very clear what is happening. This depends very much on the suffering of the data set, which means that something is fishy there. 

So be transparent when you use statistical tools, because you can very easily lie with statistical tools. The setup too is more general quantity, it's better to use most of the times because it tells you what is going to happen to all the possible data sets in the world, but it's more complicated to analyze of course, and even in practice you need to run K-fold cross-validation many times. In the book you can see how you can do that in practice, but that's what you also have to keep in mind, the setup one is for this particular training set that you have. Now take a breath, we finished with the statistical testing for today, we're going to see the base and native base classifiers, and how we can use them to make predictions and decisions, and this is in my opinion again a very simple technique, because what we have to do only here is a data set, binary data set. We have two classes, let's say males and females, and here we have like the attributes, if somebody is tall or heavy, and we have one or zero in this table. And what we want to to answer these type of questions, what is the probability somebody to be a female for example, if it's tall and not heavy for example. 

Okay that's the type of questions we want to answer with this type of classifiers, if for example is tall and is not heavy, that's it. And how we can do that, here we will apply the base rule, and what the base rule tells us, this one you have to remember all the time, something like that the base rule. Okay and of course you can invert it and say that the b given a is p of a given b, p of b, p of a, why because the p of a comma b is the p of b comma, the same question. 

These are exactly the same things okay. So it's a base rule and now we will exploit that in order to make a classifier using a simple table like that. And you can guess what we're doing, first of all you have to remember the rules, this one is the event a let's say, and how we compute the p of a by marginalizing, these are rules you have to remember them. Okay so you marginalize the b outside and if you remember the marginalization was something like p of a, b, i, p, b, i. Okay in this particular case what is the probability, the prior probability of b a male or female from the data we have? 0.5 okay. So probability b a male is 0.5 and it's a probability b a female. Why because here we have four points, another four points so four over eight is the prior probability. And also by using this matrix we can get the conditional class conditional probabilities. And what is the class conditional? What is the probability of x1 for example b1, x2 equal to 1 given that I'm a male. How we do that? 

Any guess? Exactly we just take the table we come here to the males and we check how many times both of them are true. So this happens three times out of the four. 

Okay so this is three out of four and that's it. That's how you make a base classifier. And in particular this example here we want to answer this question like the probability of being a male while you are short and then you are heavy something like that. There we have to compute these numbers from the table we plug them in the formula that's it and we come up with a number. 

So the probability if you are short and heavy to be male is 0.5. Okay using my data what is the problem of the base classifier? Can you see a problem here? Maybe here. 

Yep. Okay but I wouldn't say that this is a problem. Okay this is just the data. 

It's something that is even a bigger problem even by looking at that one or that one. Yes yes. And so and so yeah. More or less yes. So the answer is about the outliers and we cannot classify all the people. More or less yes but let's formalize it a little bit. 

What it means? I would say these are enough. It's similar yeah the answer but maybe by looking here can you see somewhere that we might not have points in some region? So here we don't have any point right in this region over here. And what is the problem here is that if we want to compute something on this side on this place over here then we might have p0 sorry the probability of being a male and being tall and light is 0 and the probability of being a female and having the same properties is 0. So at the end we're going to have to divide 0 by 0 inside here. So that's the problem. 

Okay. We don't have information so it was more or less what you said and it was more or less what you said but the correct formalism is that we might end up having 0 by 0 at some point. And that's why we need the naive base assumption. This is highlighted here. This is just an assumption. Okay and while this is an assumption that giving the class then we can break this joint into this product. The same thing we did with the IID assumption is an assumption. It's not true. 

Okay. In the particular example this would say that if you are a male or a female you can break these properties down as follows. This doesn't hold in practice. Almost never right. This is just an assumption we do in order to simplify the computations and make things work but it's not true in general that the variables are independent if you condition on something. 

Most of the times. So the naive base classifier is again the same thing exactly the same thing. The only thing we change is apart from this thing over here that we have to see what happens on the same time for all the variables. Now we break it into this product. 

Okay. That's the only thing that changes. So here this changing over here and also in the denominator the same thing changes. That instead of checking if I'm a male I have to be on the same time heavy and tall for example. 

Where's the probability of being a male to be tall and where's the probability of being a male to be heavy. Okay. Precisely in this example over here that's how we factorize these things and let's say we want to compute this one for the base. You can think about it as follows while I'm writing. So what is the probability using the base rule to have this thing over here. So we have something like that on the numerator. 

Okay. And what is this we come to the y equal one we check how many times both of them are correct. So this is one over four. 

So here we're going to have one over four divided by you know what we have in the denominator. Let's see now what happens if we have the naive base assumption. We want to answer the same question. But now the only thing that changes that we sorry here we have the p y equal one. And the numerator. Okay. 

The only thing that changes now is that we have something like this. Female times p of y x 2 is equal to one. The female giving female times b female. So we have something the denominator as well. But now what is the difference is one over four. This number over here is one over four. And the other number is two over four times the prior. It's one over two. You see the number is going to change. So now instead of checking what happens at the same time like you know that we have one over four that is happens at the same time. This thing now we check how many times we are female. You are tall. This is one over four. 

And how many times you are heavy if you are female. Okay. Once more. This is an assumption that we can break factorize this thing is this one over here. It's just an assumption. And if we do that then we come up with this number over here. 

Okay. Now what is again another problem we might have even for the NAV base and all these things. The problem is that even with the NAV base it might be okay now with two attributes it's hard to happen. But if you have 1000 attributes it might again happen that you don't have any observation for a particular class. 

Okay. Let's say now we had only haven't told but let's say now we have 1000 attributes. And for some of them maybe you don't have observations. So you can again end up to a situation where you have zero divided by zero. And for this reason we have this robust estimation processes. 

How to robustify this. And what we simply do we add a small jitter to the number we count. So if we count zero for example here we add something like one and we fix this problem. 

So we always give a prior more or less to an event. That's it. Nothing complicated. And of course we have to divide by two times alpha in order to have like a probability assumption to one. Nothing special just a way to avoid having divisions by zero even with the NAV base. Or you can think about it in a continuous scenario. 

If you have continuous data and all of them that are placed exactly on the same point and you want to compute the variance what is the variance. It's going to be zero. Right. But now I assume that you have this one point. 

What is going to be the variance of one point? Here you have a problem right because it's going to be again division by zero. That's why you have to add something in order to prevent this of happening. So you add something on the variance in order to prevent extreme events like having zero variance maybe or you know having this type of problems. So if all the points that concentrate along the same variable along the same value then the variance is going to be zero or if you have just one point you're going to have again zero variance and you want to prevent this because on the computations this is going to make a mess. So you add something here a small value like say 0.001. 

We saw using this binarized table how we can do the base and the NAV base classification. Now let's see how can do it continuously. This is one slide and another one after that and then we're going to make a summary. 

So be patient. Before we had the binarized table now we have continuous data. Again is the height and the weight of people and then we have males and here we have females. The base rule still holds but now I have continuous attributes. I don't have like if somebody's heavy or short I just have like the continuous value. 

How I can use probabilities to do the same thing again like classify the people. I'm taking the base rule. I'm following the rules. I'm saying here's the prior. Here's the class conditional and here is the marginalization. Okay and now what I'm missing is I'm missing this part over here. 

How I can model this thing. Before it was just counting. Now I cannot count this just continuous values. But what I can do I can just fix that with some distribution that makes sense and here because of continuous attributes I'm selecting the Gaussian. So what I'm saying is that I'm taking just one class so I'm considering only the females maybe and I'm saying put a Gaussian here model them with a Gaussian. 

I can't make it okay. Just take the average and compute the covariance so I place a Gaussian here and I do the same thing for the males. I'm doing a class conditional like a distribution for the males and by doing that I'm having now my class five. Okay again I plug numbers in. Maybe this looks ugly but I'm just computing numbers. I'm computing the mean of the males and the covariance of the males. The same thing for the females and then I just plug these numbers in this formula and I can get something like that. This is a continuous classifier so for every point in the space I can get the probability to be in this case a female by being at every point of the space. So if I'm around here the probability of being a female is very low and if I'm around here the probability of being a female is very high. So you see just by following simple rules, base rule, super simplistic. I'm sure you have seen something like that in school. 

So this is the base rule. Okay then I made a modeling decision how to model these parts over here using my data. I managed to build a classifier that is useful and it's not even useful. It's also non-linear. It has a decision about the radius like that. If you continue in muscle learning you're going to see that this is a principle. It's a recipe and what you can learn to do later on in your career you can take now this Gaussian and instead of a Gaussian you can have something more complicated. 

You will see about the end of the course that you can have a mixture of Gaussian. So you can have something that's like that for the females maybe and something like that for the males. So you can have more complicated components inside here but the general recipe will not change. It's going to be the same. 

Only the components of this recipe can change. So be sure that you can understand the basic principles. Okay and how I can make this naive instead of having the full Gaussian properties. So if you have a Gaussian distribution and you want to factorize it so there's a joint and if I want to factorize it I'm getting these things over here. I'm taking only the variance here and the variance here and then I'm making these two. Okay if I have a Gaussian I just want to compute the in this case the Gaussian was like that the full Gaussian but now I'm taking every component individually and the properties of the Gaussian distribution tells us that this holds. Okay so having a Gaussian you can break it down into product of univariate Gaussians. Of course this is just an approximation again in the sense that if we have if we have this over here and this was a Gaussian and if we make it the naive base assumption this is an assumption again I'm going to write it as follows. Again assumption. Okay why we need to do that because it might be that this covariance is very ugly or is very big maybe it's very high dimensional and this is a problem to work with so I can break this down into product of Gaussians but this again an assumption okay that I can take this model over here like here's a Gaussian for the males here's the Gaussian for the females and I'll break it into the product of Gaussians so if you do that instead of having like this for the females you're going to get something like that in total so the the covariance structure is going to break it's basically putting these things to zero if you want to make a a full Gaussian distribution into a product of Gaussians so you break the correct structure of the distribution. Okay. Be sure you understand this difference because if I have something like that this is a covariance a b and I break this structure and I have this covariance so I put zeros on the diagonal then I'm going to get something like that as a result. Okay so I'm breaking the covariance structure I'm removing all the covariance structure the correlation I'm removing it cool so I'll try to summarize a bit fast what we have seen the last two lectures once more I think repetition helps we have seen the training and the tester and the generalization error why we train a model on the training data and want to see how well this will perform in the future on the on the infinite test data we don't have access to infinite data that's why we use cross validation to estimate the generalization error okay that's what we have seen last week and today what we saw we saw that because cross validation comes not as a number but we have many numbers behind so if I'm having my my training data I'm breaking that the test and train and here I have let's say 100 points and I have like the the result of predicting correct or wrong in each of them so I'm not having just one value not the other only the average but I have many values and I can use them somehow and this is the statistical testing and the hypothesis testing so from these numbers what they can do I can make an assumption for the likelihood so they come maybe from a Bernoulli distribution something like this comes from a Bernoulli distribution over here we came up with the posterior and the posterior gave us shape between zero and one and this gives me a systematic way of saying here's the estimator giving the test data I have and also talk about the confidence interval and the hypothesis testing if I have two models to compare them but the the the whole story is to understand that we want to get the generalization error we cannot get it what we can get is a test error by using some finite test data and these are random so we need some extra steps maybe to have a better argument so if I test on 100 points and I get 0.9 let's see all the values behind how they look like more or less so if I have 100 points and let's say they come with this distribution I want to know it having only this this prediction like their average is good but it's good to know also how they distribute more or less that's what the statistical testing helps us for okay the confidence interval and then the hypothesis testing if you have two models and so on and if you want try to simplify that not taking you know training test data and so on so forth but just human heights a similar problem or tossing a coin if it helps you to simplify things I'm going to upload the annotated slides by the end of the week maybe this helps you some of the notes I wrote today don't understand some of these topics 
Speaker 1: Test Zoom. Hi. Can you hear us? Hi. Hi. Hi. 

Speaker 2: Hi. Now, things good. Okay. So welcome to the week eight with Redux, we're going to talk about how to use data mining. Today we're going to cover the last aspect of generalization error. 

We're going to close this loop. We're going to see this bias and variance trade-off. And we're going to introduce artificial neural networks. 

And I suppose many of you are in this class because of the neural network type the last 10 years maybe. Okay. As usual, the list of people who are highly recommended to come online and give us feedback. 

Of course, everybody's welcome. Indeed, you learn there is a particular place where you can do that if you don't know. For today, you have to read chapter 14 and 15. 

And let's see what this is about. Bias and variance trade-off. Today we're going to see from a theoretical perspective how generalization error can be thought and how we can, before start implementing stuff, what we have to do, what we have to think about. And the neural networks. 

Some practicalities. The project two is already online, as you know, the example date and so on. But the feedback one is already online. 

Some of you, I think they don't receive the message when we upload stuff on DTU Learn because it disabled the notification. You can go online and you can take your feedback, long story short. Keep in mind that if you didn't pass the first report, this will be explicitly mentioned in your feedback. And then you have one week to resubmit by paying attention to the feedback. 

Okay. I'm sure that Bjorn mentioned that many times in the beginning of the class. In the previous lecture, I made a small mistake in slide 36. Later on, we correct that, but just to be sure that you have the information. 

When we define the Bernoulli distribution, if this is the parameter theta. This is the correct definition I had mixed a little bit on the end on this slide. And what this represents. It tells us what is the probability, the problem variable z i to be one probability is going to be theta. And if this is zero, the probability is one minus that okay. Just for completeness, this is the correct one in this slide in the previous lecture. And there's also a midterm test. At the end of this slides and previous pretty can find 10 questions. 

And you can answer them in detail. This is not great. It's just, you know, for us to get an idea how well you perform on the class. Okay. If there are still things we have to pay more attention to. 

It's just a baseline. We don't. You know, care. Care. 

Well, how you perform on that one for the for your final grade right, we don't take that into account. Just for us to understand what is happening. Cool. So, I think it's a good time to make a small recap for the lecture six and seven and try to motivate what we're going to see today. So in lecture six, we start the discussion about the generalization error. And what the generalization error is the performance of our train model. 

So here we have a model with trained on a data set D. And we want to see how well this will perform in the future on infinite data. I hope until now this is very well established in your head by the generalization error is the true error. Of course, we cannot compute that in practice. 

We don't have infinite data. And we have seen a technique a practical technique do that. And let's say we focus on the cross validation with five volts, the five volt cross validation. And what we did there, practically, we broke our data set into five equal pieces. Then we use that for training and that for testing. And we did that five times. This to some extent implies that we have five different training sets and five different tests sets to some extent. 

So we have collected five estimators for the test area. Okay. For each of these faults. And then we average them out and we said, that's what we expect to be the generalization error. It was just an estimator we gave for the generalization error. So we try to estimate this quantity with this technique. That's what we did in practice and why we did that because we know that if we increase the complexity of a model, we expect the training error to become better and better. 

So we can fit our data perfectly. But the problem is that we have the danger of increasing the test error or the generalization error, meaning that if we have very low complexity model, we cannot fit our training data. We cannot solve the problem. We have a very complex model. We can fit very well the training data, but we don't solve the problem. We don't generalize well. So there's always a sweet spot somewhere in the middle that we want the model our models to be in that spectrum. Okay. That's what we have seen in lecture six. 

In lecture seven, we expanded a bit on that one because we said yes, we have an estimator for the error, but we have five numbers here. Can we use them somehow. And indeed we use them using statistical analysis for one model. For example, we can use these five estimates to get a posterior for the parameter theta. In this case, the parameter theta is going to be the generalization error and try to provide the confidence interval and say yes, we estimate this number over here with this number here, but we also give the confidence interval where this number could be inside based on what we have seen. And we have seen also the hypothesis testing so we have two models for example model a and model b. 

Then we can get again under our assumptions distribution about the parameter theta. The other thing is going to be the difference of the two models. And we want to see how extreme is the observation we have like the errors we have collected between the two models that difference. And without information to reject or not to reject in our hypothesis. 

Okay. So, generalization error on practical estimator, some statistical analysis on this practical estimator. That's what we have seen so far. 

Today we're going to close this or finish this puzzle to some extent close this loop by trying to understand theoretically how the generalization error will behave based on the complexity of the model, because these techniques are practical we can estimate the numbers and we can do statistical testing. Today we're going to see what happens behind the scenes and what we have to think about. And let's start our discussion with this very intuitive plot. What this is is the bias and the variance of an estimator of a game or whatever. 

So let's say we want to hit this goal in the center and we throw some balls right and we know them are you know landed in the center. Intuitively speaking, this means we have low bias, sorry, high bias. Sorry, low bias and low variance. Why? Because they are very concentrated so the variance is small and is low bias because we are very close to what we wanted to hit. 

Okay. So low variance, low bias. This seems to be a good solution right we want to hit the goal and we did hit the goal. In this one what happens we throw again our balls and then they fall around the center. If we take their average again their average is going to be very close to the center, which means our bias is small. We don't go away from our goal on average, but we have, you know, a lot of variance is very spread. Our solutions. Okay. Now you can see how this propagates here we have high bias and low variance. 

Why? Because all of the solutions they kind of concentrate in a small area so it's low variance, but we have some bias because all the time we are far from the goal. Okay, so that's the bias more or less we are always far from our goal. In the last one, which is probably the worst, we have high bias. 

So if we average all the solutions we're going to be away from our goal, then we have also high variance. This is the worst. This is the best. If our goal was to hit, you know, the target. And now let's try to connect that to what we know models training data solving problems. We're going to use the linear regression model in the simple form. So what is what the simple form we had some parameters that are we had X, W zero X transpose W. Okay, so when we have the tilde what we did basically we put the W zero inside here so having W zero W index tilde is the one. 

Okay, so when when you see the tilde, you have to think about it that you have included the one into your the vector of features right right things more in the compound for and especially here we have a matrix so this looks like having all the features like that one. X two, blah, blah, blah. Something like this, right. 

It's a matrix now. And we have seen with your new you have seen that I think in the second lecture that we can standardize our data. And many times is a good idea because we can have different scales something can measure years something can measure like, you know, salary. They have totally different variants in scale. She's a good idea to standardize our data. Maybe standardize our data was a practical mean we divide by the standard deviation every column. 

So we can write this matrix X, tilde now as the X at. We have the ones at the end. Okay. Nothing special until now we just. I'm just telling you now how we write down things, nothing special. And that's the model we will use at the moment just a linear regression model just straight lines with one input and one not. 

Okay. This is what we had in the previous slide we had in the compact form with the matrix for but this is basically what we have we have a sum of the prediction error so these are predictions for his input. So we have X i after we have standardized data. This is the prediction we want. 

And that's something new. So this, this thing over here, we call it a regularizer. Why, because if the parameter lambda is positive, and this always positive. Or non zero at least. This is a positive value as well. The norms, especially the media norm is always positive right. So here we have a positive value. And if we can think about this practically what it does, it increases this cost right. 

So intuitively speaking, if this lambda goes to zero. This goes away. So it's the classical integration we have seen so far. 

Nothing special. But if this lambda has some positive values. It means that we pay some cost right we will increase will be some penalty. If this W is high. Because all this term is going to be high. So we have one term here that tries to feed the data. And one term here that tells me, don't make your W is very big. Okay, that's intuitively what this term is doing. And why, why this important because this is a way to control our weights. It's a way to control complexity. This is a very simple model of course it's just a line straight line, but we'll see that this can analyze more complex models. So this in practice is a way to control our complexity by setting this lambda to the correct value some extent. We'll see in the next slides how we can do that. 

So at the moment what I'm only doing and doing taking this sum, the predictions, the prediction errors we have the sum of square differences, and I write in the matrix form. And here still we have this positive value. Okay, this is just a number. Let's try now to find optimal parameters w user and w. First, let's consider the w zero, the bias there. 

This is the same right. For all the parameters for for all the points. So just take a partial derivative of this thing with respect to w zero. And because a convex function I hope we have seen these things in the mathematics but we are a convex function. 

And we want to find the critical points we can take the derivative with respect to the parameters, and we can set up to zero. But what is interesting now we take the derivative we have this thing over here. And if we put the sum inside, because the square now came outside, and then the w zero. Parts of the way this is one. 

We take this inside. And what happens here we have the sum of all the predictions, what we want to predict right, the output value sorry the labels. Here we some n times this parameter w zero. And here, we some old X eyes right w is constant so it can go outside. 

And why this important because we have center eyes, we have standardized our data so this is centralized. So this goes to zero. Okay, this goes away. And as I said, what we we did we said this has to be called zero to find the critical points and if we do that. We get a condition for the w zero at least we say that the w zero. If we have standardized our data. In this simple model. 

Can be just average of the output. Okay. This is very convenient. We can immediately estimate that just by taking the average of the output values. And now what I do I just subtracted subtract this scalar from all the output values I want. Okay. And I have a simplified version of this thing over here. Where I just subtracted the w zero optimal w zero. 

Okay. Now we do the same thing again but now we take the parts of derivative of this thing with respect to w. And we set that to zero. This is vector calculus. But what we end up with is this formula over here in the book you can see the derivations is more or less what we did upstairs here. But now we have vectors not scalars. Of course you can do that with scalars. But we end up with a solution like this one here. 

Just as a note, this box last means that you here you have the inverse of this matrix. Okay. It's the same thing that we written in both ways. So we started with a function that looked like that. A multi dimensional input. Okay. X one next to We standardize our data set. So we say that this has zero mean and one standard deviation the inputs. 

Okay. And we'll find now the optimal parameters given a data set. Firstly, we can estimate w zero with this way here. Second thing, we can find optimal parameters. So the rest w one and w two by solving this formula over here. 

Okay. That's exactly what you have seen when you were doing the linear integration without these are utilization term with your name lecture five. It's exactly the same thing. Here the only difference is that we have this extra term and this extra term appears here. That's the only difference. 

You have seen this formula before at lecture five. If I remove this part. Okay. Note that this w star here is the w one in my example above and w two star. Okay. 

And the w zero star is over here. So I have trained my model I found the three parameters based on my data so that they solve the problem in the best way possible. But under this parameter lambda I have here if I change this lambda, you see that this solution will change right now let's see how this is going to change. This is what I had before at the end as the optimal parameters. And I will write it in this way here but assuming that we have just the one dimensional problem and no bias. So I'm just writing this as the inverse. 

If it was one dimension problem. You know divides this sum over here so this is scalar this is scalar. And let's see how the lambda now behaves. 

If the lambda goes to zero. What happens here, this goes away. It is a classical linear regression you have seen with German. Okay, nothing special. I don't have any regularization. On the other hand, if this lambda goes to infinity so I have a very, very big number. What is going to happen. This part over here is going to be very big. Which means that all this term here is going to come become small. 

Right. Have a very big number of the denominator makes everything small because on top I have constant things. Just my training data and output values. Makes sense. 

So what this lambda tells me is as I said the regularization parameter. If I put it in a very high value. Then I'm telling my weights. I want all of you to be really small values. 

Some controlling my weights in place to be. I don't allow them to get any value I want but I tell them go to zero all of you. If the lambda is really big. If the lambda is really small, I say I don't care. Take whatever value you want. 

Now you see this a game of complexity. I have to set the correct lambda to be a balance between the two extremes. Neither super free, neither super constrained. 

You have to be somewhere in the middle. So lambda controls the complexity of the model. Of course somebody would ask now we did a very simple linear regression. Does it hold for general regression? 

Yes. So I assume now we'll have a more complex model. Now I'm taking one dimensional input and I'm just adding nonlinear transformations. Let's say sine 2 y x. 

You have seen with this way you can make no linear functions to feed your data. Okay. And this is a good example. We have seen that in the first lecture 6. We have seen that by having many, many degrees of polynomial we can get very complex functions like that. But if we put some regularization and we turn some of these weights very small, we reduce this effect. So we don't allow our function to be very weak. That's the point. So we have to set the correct lambda. By setting the correct lambda, we can have no linear functions, but we control more or less the complexity. 

We control how weakly they are. Okay. What is the difference now from the previous solution that we saw before? First of all, this one doesn't make sense to standardize the data because we don't have like real features. These are just features transformations. So it doesn't really make sense to standardize them. So we have to set the correct lambda function. Which means that we cannot have a closed form solution for the W0 most of the times. However, this closed form solution for all the parameters still holds. 

We can take the W0, sorry, the W star. And this is going to be equal to the x tilde, x tilde plus lambda times. So this matrix here has one diagonal and zero at the last point. 

I will explain why minus one. Something like this. So what are the numbers here? These are our outputs. So it's just, you know, a big list of the predictions we want. Just a vector. 

Okay. With n dimensions made. What is the x tilde matrix? 

So now I changed my features. And I have to write down how this matrix looks like. And basically it's a very simple thing. We take just the expansion we have here over here. And we do the same thing for every data point to have x one, x one square, sine two pi x one for my first input. 

And then at the end, I have a tilde here. So I put the one to take into account the bias there. Okay. Is that simple. 

X two, x two square, sine two pi x two. We do that for endpoints. For my training points, basically. Okay, so this is a matrix and times four. This is something I can do beforehand, right when I start the feeling of my model to some extent. And now let's pay a little bit attention to this matrix here. Before we said that this just a identity matrix. Why because I've already trained a w zero. So we had only, you know, one, two or three parameters without w zero. Now the w zero, we want to estimate it with this formula over here. 

And we don't regularize the w zero, we don't want to put any constraints on the w zero on the bias so this one can be free can take whatever value. Okay. That's why we live here is zero in this diagonal matrix. And that's it. So happy you have here a general formula. And you have seen is not very complicated. 

Basically to play. I would suggest to play that with your with a computer with that one generated data set, maybe from a sinusoid or something a function generate some random points, then construct a matrix like that. Take the output values you want. Write down this formula over here and you're going to get some parameters that optimally solve the problem. 

Okay. And by changing this parameter lambda, you will see how your function is going to change. If it's, if the lambda is really big, your function is going to be something like a straight line. Because all of these values are going to go to zero. So it's going to remain only one parameter like the bias there. Okay. 

If you allow the lambda to be very small number, you will see that you can get a very complex complex function depending on the nonlinear features you consider. But I really recommend you to do something like that. To understand how this thing works. Pay attention to this part over here we have a zero because we want in this case the w zero to be inside this parameters w one star w two star. W one star w two star w three star w zero star. Of course, if you want to put w zero as the first element, this has to come here as the first element. Okay. 

But this is how you can fit a nonlinear regression model by having some regularization into the game. Okay. And I really encourage you to do that. This 10 lines of code is not something more than that. Now, the question is how we set the proper lambda because the lambda will take into account only during training after we train the model and we find the correct parameters to just plug them in our function and we use it on the future in the future to predict on new data. 

Okay. But how we determine the best lambda given our data set. Here is one example. 

Let's say we have three different data sets which come from the same distribution or it might be the same data set and I just break it into three faults. Okay. This is the true function the black curve below. 

And these are the training points. Okay. And I have a polynomial of degree eight as far as I remember to have something like that. Plus w one x plus w two x square plus w eight. Something like that. Okay, just a polynomial of degree or something like this. 

Basically, I think it's nine. So, if the lambda is zero, we have seen the previous lectures, we just solve linear regression with a very complex model so we can fit our data perfectly in all the cases, we have a very complex model. And we can fit our data perfectly in all cases. How this looks like here, but what is this a scenario, what scenario is here. Yeah. This one who was for this one. Can you raise your hands. Okay. 

Yes, is this one, the correct scenario. Why. No, actually. No, is this one you trick me is this one over here. It was correct your answer. 

Why is this one over here because I have high variance. So our solutions are really far away. Okay, from the goal. So we're not very close to the black line behind in any of the solutions. So we're going around the goal. 

Okay. But if we take their average. Think about averaging this orange lines or on scarce, then the average will be something along the true, the true function. 

You have to, you know, think about it. But if I just average these three, I'm going to get probably something closer to that one. That's why I have high variance, but low bias. So my average solution is going to be close to the goal. 

But each individual solution is going to be bad. Now think about if we had just one day as it, let's say I had only this day as it or this day as it. Then this means I can fit one model like this one over here. But I'm expecting to be far from the goal. I cannot average many models. I have only one model now. So I'm going to be somewhere over here far from the goal. Because I have high variance. 

So I fit it only one model. I'm expecting to be far from the goal where I want to be. Let's see what happens over here. Okay. In which of these ones we are. Now it's what our friend over here said. So here we have like small variance because all the models are very similar to each other. 

But I have high bias because even if I average them out, I'm far from the goal. Okay. I will say that this one over here comes here. This one over here comes here. 

Okay. So with lambda equal to zero when we solve just simply linear regression and I have very complex model. I have a high variance model by changing a little bit the data set. You see that data sets are very different from here to here. The data points come from the same distribution. But you see that the model is very, very different. And that's not a good thing because it means that I'm very, very prone to overfitting. Okay. In the other extreme case, yeah. 

Speaker 1: When you talk about the variance, you don't talk about the variance on the testing. No, okay. Yes. 

Speaker 2: So the variance, the question is what variance means variance means how much my function is going to change if I change a little bit the data set. Okay. So in this case, I changed a little bit my data set. But my function changes a lot. 

You see one, two, three functions, they don't agree with each other. In this case, you have zero variance in terms of the model at least because it's in your data set. But your model is very, very, the difference of your model is very small. So the variance represents how much your model is going to change if it changes a little bit the data set. And the bias the same thing how far you're going to be from the goal if you have a rate of the models. 

Okay. Is it clear because it's quite an important step here to understand what is the variance and what's the bias. The bias is if we have a rate of our solutions, how far we're going to be from our goal. And in this case, we're going to be quite close to our goal if we have a rate this three. In this case, even if we have a rate, we're going to be far from the goal. Even if we have low variance. Low variance means I change it there. I said my solution is not going to change months. Okay. Yep. Okay. 

Speaker 1: Yes. Yes. Yes. 

Speaker 2: Yeah, that's what you said exactly. Good. You follow what I'm saying. So yeah, the idea is that if we have a very strong lambda, we kill all these values over here, all these parameters, we put them near zero, which means that only this guy here survives. So I'm expecting my output always to be constant. Okay. Something like that. Well, if the lambda is small or near zero, like here, then I allow my which big whatever value more or less, which means that all of them survive. 

That's why I have this weekly behavior. But I would say that something along those lines is the optimal lambda. So a lambda 0.1 seems to be very good. 

Why? Because if I change my data set, my function doesn't change a lot. So my variances small. And if I average these three models out, I'm going to get very close to my goal. Okay. 

So a lambda of this level seems to be a good idea for this particular data set. Makes sense. I know it's just intuition because here I'm very in all the models are very close to the goal. 

And if I ever it's staying close to the goal. Well, in the other cases, we have problems. Any more questions on this slide? Yep. 

Good question. So the idea. Later on, you will see that this is even a model class, you can average models to get predictions. 

But in terms of today, it's just a way to motivate that theoretically. Assuming that, you know, you have a data set. Only then, if you have only one data set, you don't have anything. 

You just see over here what what is happening. So you have to think about if I have just one data set and I put the lambda to zero, I'm probably going to overfit the data. Right. I'm going to be on the on with high, with a high variance model, fitting my data. Well, if the lambda is very big, I'm going to have high bias. So I'm going to underfit my data. 

Okay. The reason why we have here more columns is just to give you the intuition that if I change a little bit my data set. I'm going to have a totally different model. 

So it's just for a motivation for the motivation purpose on the slide. Okay. In principle, if I had three data sets, I'll just combine them to one day. 

I said, right, is the basic thing to do. But that's for motivation here. That's what we have to pay attention to. Now let's try to take that into mathematics and try to see this discussion with it. The intuitive discussion how we can write it down in mathematics. And this is, I believe the only theoretical proof to some extent we're going to see in this course. Maybe some of you will not like maybe some of you, you would love the idea that you can have theory. 

I'm in the second bunch of people I like when we have theory in muscle learning. And I'll try to do my best to make you also enjoy the process. Okay. So what is the idea here? The idea is that we have the generalization error. And now we have the most general generalization error to some extent because we take into account also what happens if we change data set. 

Okay. So now we have the generalization error of one model internally. And then we average that with all possible data sets in the world. We did that before two weeks. And if I want to write this thing down, I will say that the first generalization error. The x y, the loss of y. 

Speaker 1: It's something like that, right. 

Speaker 2: We decompose it a couple of weeks ago. And why this is the case because the few effects from a Y using this rule can be written as follows. Okay. So what this means is that I can train a model using a data set D. Okay, I can train my model and then generate a point x, a particular input and then generate infinite outputs. Do the same thing again. I generate another input, like another size of a house, and then I generate all the possible outputs. And I do that for all the possible X's. Okay. 

Here. That's what this distribution tells me in practice. Whenever you have things like that integrals and, you know, this big complicated expressions, try to think about it them as Monte Carlo sampling for integration, for example, with sampling. 

It helps me a lot. So this one means I sample one X, and then many wise, then I sample another X and many wise, and I keep doing that for infinity. But it's going to give me just a number. How well this model behaves when trained on the data set D. And now what we can do, we can do that for all the possible data sets in the world. 

So the only difference is that we have here another integral, but what this integral just does. I sample a training set, I'm training my model, and I generate infinite data testing. And I do that over and over again. This is what we have seen so far. Now we specify the loss function. If you have seen here, I just left the loss function arbitrary. Now I specify that I'm saying this is just the L2 difference. The square difference between, okay, here, there should be a parenthesis, sorry. This is the square difference between my prediction and the actual output I want to have when I train my model. 

The data set D. Okay. Cool. And now let's do the following first. Let's consider the X fix is what I have inside here. Okay, more or less. But now I will just rewrite it. I'll just put the integrals in a different in a different sequence. Now we'll explain the second why. 

Okay. So now what I'm doing, which sounds very stupid, is only for theory, right? We consider only the thing that we do here in this part. What we do, we take a particular X only, okay, selecting an X. Then I'm taking a training set. I'm training my model and I'm testing only on this X what happens. And then I'm taking another input, another training set. I'm training my model and I tested on a finite outputs. 

Of course, this is not something that you do in practice. It's just for theory, you know, it's just a way to decompose the same thing, we just decompose it in a way like that, because it's going to help us on the computations, but it gives exactly the same number. That's why here we have this conditional, right, because we sample conditionally given the X. What we have to pay attention to is that we keep the X fixed now we speak for a particular X what happens. And what is this value over here. So now we have the X fixed and we sample many Y's and we take their average. This looks like the actual variance or the actual sorry the actual average value of the real problem, because the real problem comes with the variance if I have houses of a particular size of 69 square meters is not that all of them they're going to have the same price right they're going to have different prices. So this they they're actually the true average value the true expectation. 

Make sense. So if I have like house prices. Sorry, size of the house here at price. Then I take a particular X, I fix my X is size 70 square meters and let's say or something. 

And then I take all the possible outputs I collect I go in the market and ask different people the size for the specific size what is the price of the house. Okay, I just collected from the world. And I take the rubber it's over here. And this is this number over here. Okay, is the real variance and expected value of the problem. 

I cannot do something for that one is what the problem tells me. So all the cats in the world they don't have the same face exactly in the same human humans right we don't have the same face. There must be the average face and that's what we have here. Cool. And now we take this, this part, which is this part when we decompose it. 

It's like only this thing over here. I just write it down in a compact form as follows. And what I'm doing now I'm adding and subtracting the value. 

I have noted down here. Why this might might be very stupid because it's just give me zero right they can cancel out. But by adding this thing over here and taking the square. I'm getting this three terms. And for the ones of you who like mathematics this is a classic technique to start doing proofs like you add something that is zero. But it helps you to expand, you know, the, the formulas. Cool. 

Now let's take step by step and see what the steps are. This term over here is something that it has our model inside. Is the function f somewhere inside here. 

No, right. We don't have anywhere the function f. So this is the variance of the problem is the, the mean house price in the world. What we expect to be the house price for seven square meter apartment. And this is the possible outcomes. So this is the variance of the problem is this this thing over here is the true variance of the problem. I cannot control this. This is a term that is fixed. 

Is the problem dependent. Okay. Let's see what happens over here. This one here is also missing. The parentheses. So this is again the same value that I have explained over here. And this is the prediction of my model when trained on the data set D and I do that by sampling new D and training again and taking again the difference. Does it sounds, you know, interesting this number. Okay, let's see in the following slide. 

Actually, this is the interesting number we're going to decompose later. Now we will come over here, which is the simplest thing to do first. We take this one and this appears only here the why so we can take it inside because expectations in linear operator we can put things inside expectation. 

So we can put things over here. And what why this interesting because this is equal to this one afterwards. So this goes to zero. So we can cancel this there. 

Why because this guy comes over here. Then this gives me the way have this becomes zero. That's precisely what we have here. 

Now let's go back to this term. What this term is, we have the true expected value of the problem. And here we have our prediction if we train on data set D and we do that for all the possibilities in the world. 

And I will do the same trick again. But we have now this quantity over here and what this quantity shows. This quantity tells me again for the fixed x fixed my x. I take a training set and training my model. I'm getting a prediction for the x. Then I'm taking another training set. I'm doing training and then I'm, you know, plotting my function maybe something like this. When I do that over and over again. 

For many, many days. Can you understand now where we are going because for this particular x, if I train on all the possible training sets in the world, I'm going to have an expected value of all these models and I'm going to have some variance. So this is what my model is going to predict if I was able to train that in all the possible training sets in the world is what we have seen before with the plots. Now we make it mathematical this thing. Okay. So I'm taking all this formula over here and I added subtracted that. So this is the term we have we had left behind. This was the fixed one is the term we left behind and then we come here and we expand this. We add them to talk this thing. And we expand again by taking the square. 

So we have again three terms. And if we do again the same trick so we take the, this is fixed, this is fixed value is just scalars. We don't know them, but they're scalars. So this is the D comes inside here. Sorry. 

Comes inside here here. This becomes again f of half so it becomes zero. So we cancel the game one term. And now we have only these two terms. We have these two terms that remain. 

And these terms are really, really interesting. Why? Because once more, this is the fixed expected value of the problem is the average house price for the particular X. Okay, I don't have any control on that one. This one is the average prediction of my model. 

If I was able to train my model on infinite training sets. Okay. This can go away because this does not depend. 

We don't have the anywhere here so this can go away. Okay, so this is a fixed value is my expected output for a particular X. If I was training infinite models, of course, we cannot compute this quantity. 

It's just an theoretical quantity, but it tells me what is going to be my average model prediction. Okay. And what this is giving you. Can you recall from the previous slides what we did what is the goal is the goal. And this my average prediction, how we call this term before that I have a goal and I have my average prediction. Exactly this is the bias. 

How far my model is going to be from the actual goal on average, the average, the average values. Okay, so this is the bias there. And now we can probably recognize this there must be another part that was missing from the discussion. So this is the variance. Why, because this is the same value is the actual prediction. I want, but sorry, the prediction I'm going to have if I train my model on infinite data. 

And this is per model the prediction. So this is exactly the definition of the violence. Okay. So if you want to visualize this is exactly what is happening over here. 

This my prediction. This is this term over here is this term over here. Then we have this term that somewhere, somewhere, we don't know where it is but somewhere in the space. And then we have this term over here. 

The whole thing over here that is the variance of the green curves of much the deviate. Okay. And that's what we need. That's how we can finish the proof basically, because now what we managed to do we take. We took the generalization error, the original one. And then we broke it down into three components. 

One is the variance of the problem. We cannot control this. But the other two things we can control is the bias of our model. How far we are on average. Our average prediction from the prediction is the bias there. And this is the variance there. How much my model, they're going to be different from their expected value. 

If I train them on different training sets. Again, this is a theoretical quantity, right now we try to motivate theory. We cannot compute them in practice, but it tells me that my generalization error can be broken down into three terms. And now, okay, to have the full generalization error, we can average what happens for all the possible axis. Okay, what we left out because all the discussion happened for one X only. And now to have the proper generalization error, we have to do that for all the possible axis, but this doesn't change anything. This does the same thing. 

We just average that for the possible axis. So once more. We have our house as we have the price of the house. We go outside the Copenhagen and we search all the possible apartments in the market with seven square meters. And we ask them for the price so they tell us different prices. They come something like that we collect our observations, and we take their average. We take just the value. Okay. This is this term over here. And we can compute the whole variance of this thing basically, which is, you know, the variance of this estimator, the average value and the whole variance, but for the moment let's focus on your distance over here the variance, the sorry, the expected house price for seven square meters apartments. 

Okay. And now we won't train a model to do the same job to predict the house values. So we take a data set with rain our model, and we come up with a function that looks something like this. We take another data set with rain like get another function and we take something that looks like that as a prediction. And we do that many times. 

So we have different functions that we train. Okay. What we can do now we can take the average value of this green models. Maybe this green, the average is here. And this precisely this value over here. And what we see that there is a gap between the true value and the value that we will predict on average. 

And that's the bias. So our model is going to be biased is going to be always far from the true function value. If I train that on many models and I take the average. And also I can compute the variance of these models over here. 

Let's put it with scholar like that. So the variances, how much this really models deviate from their expected value. So the best scenario, what it is, is to have a model. 

That is the true problem. So the best scenario is to get a model. To have a model that if I train many, many models and different training sets. My average predictions is going to be very close to the actual value. And also I'm going to have low variance. So my models they're going to be very similar to each other if I train if I change the training set. And that's a theory. 

We finish the proof and now we managed to decompose the generalization error into three terms. While the first term is something fixed. I don't I don't have control on that. But the other two terms I can control them how if I have a model with high bias and small variance, this can go to zero. If you remember we had this plot before that we had this problem, then we had the straight lines so we can if I train many models, but I have high bias, all of them they're going to be straight lines, for example. And this is going to give me a higher the generalization error in this case going to be high. 

The other extreme case is to have zero bias, but super high violence. So this is going to give me models that you know if you remember they look very, very weekly. Even if the rubberage is going to be nice, the generalization error is going to be high. Okay, so again, this number is going to be higher. So there's a sweet spot, a particular model class that is going to give me a small number here and a small number here. 

If not small, at least, you know, not very high values. That's why we call it a bias variance trade off. I can have a model that has very strong bias and zero variance or high variance and zero bias, and both of these extremes are back. It's going to give me high generalization error. Well, if I have a little bit of bias and a little bit of variance. This is good because I can reduce my generalization error. 

Okay. And that's, I know maybe it was a bit tedious the derivation, but what is important to realize is that we can take this theoretical quantity and theoretically analyze it and end up with a solution that looks like that. That we cannot compute, but it gives us an intuition when we design our models, what we have to pay attention to. So when we design our models, like how big should be the complexity, we have to pay attention to these two terms like. How much bias I want to have in my model or how much variance. 

I want to fine tune my model to have a little bit of variance and a little bit of bias, but not being in one of the two extremes. Makes sense. And this closes more or less the discussion we had with a generalization error. 

And now I hope this plot is the same as before, but I hope now is a bit more clear what happens. Because it might be that we would go in the market we collect the house prices and whatever and we're only with his data set over here. Now that we're experienced machine learning engineers, we know that we don't want a solution here, we don't want a solution over here, but we want something in between. 

Okay. And how we can achieve that. We cannot see that by having a very high complexity model. 

Initially. But by adding regularization is lambda we said the beginning of the lecture, we can control the complexity of the model. So the lambda zero, very high complexity model. If the lambda very big, very low complexity model, not a good idea either. 

So let's try to find the lambda that does the best work. That's also the problem the best. And just to come back to the previous question in principle you can have only one day as the purpose of having here multiple data sets is to show you the big problem. And the big problem is the behavior that if I have a very high variance model by slightly changing the data set I collected, I can have a very different solution. And this is not a good idea. Neither this is a good idea like to have a very stable model. They're very high bias model. 

Okay. This is a schematic figure worth shows that what happens if I increase my lambda so the lambda here is very small. And the lambda here is very big. And here I have multiple weights I have seven weights, I have a w zero that I don't regularize it and I have the six weights of my model. So for example my model could be something like w zero plus w one times x plus w two times x two plus w three x three plus w six. Okay, I have a six degree polynomial. And I want to include regularization to penalize my weights. 

Okay. And we see here that if we increase the lambda, what happens, the w zero. So, when I'm optimizing, I don't care so much about this term, because this term can be small, with respect to this term, because this term is already 100 times something else. So, you see that I see, you know, regularize a lot my problem and don't care what happens with a fitting term. 

Okay. That's why all the weights come here to zero. Keep in mind that as I said in the beginning, the bias we don't regularize it, that's why it remains constant all the time. 

Especially in this problem, we have standardized our dataset, so this is just the average of this value, so the predictions. Okay, this has to be without the hat. Just to make a small recap from what I've seen before the break. So, the idea is that we have a model to solve a problem. 

Okay. And we don't know beforehand what complexity is good for the model. We have seen some techniques how we can do that practically, how we can find the correct complexity with cross validation, for example. But, you know, what means good complexity is we have like different degrees polynomial and then we try each of them and we see which one performs the best in terms of generalization error. 

Okay. So, we have a model to solve a problem, but it's going to be nice if we have a theoretical understanding of the problem and being able more or less to argue beforehand and say, I want a model that behaves in a certain way. Like, and what is the certain way we want them, we want the model to be with low bias and low variance, relatively low bias and relatively low variance. And why this interesting from a modeling perspective as well, because I can define a very, very complex model. And then I have here many degrees polynomial or 100 degrees polynomial a super big model. And instead of trying every different combinations, maybe or many, many models, what I can do I can just include the regularization was it earlier. 

One training to penalize the ways. So I can get I can go very extreme in terms of when I'm defining my model. Then instead of, you know, proposing new models, I'm trying different regularization values to see which one performs the best. I hope you see the difference by defining always different models by defining just a very, very big complexity, big, very, you know, model with a very high complexity. But instead of doing this sub models with just a regularization parameter, and we try to find which regularization parameter performs the best. Which is going to give me the best weights under the same complex model. Makes sense. 

In particular here what we can do and what we did. We defined a very high complexity model, a eight degree polynomial. Okay, and I would just change the regularization parameter. Find the correct. 

The correct one which performs the best in the previous lectures what we did we had the eight degree polynomial, six degree polynomial, four degree polynomial and zero degree polynomial. This is a different thing. I'm defining different model every time. And while here we have a very big model from the beginning and we just fine tune the weights by using regularization. Okay, makes sense. 

The difference. And then what we did in the second part of the first part of the lecture. We started from this idea and we decomposed the generalization error for a particular loss like the L to loss, we decomposed it to the following three terms. And why we did that because we can see that there's always someone certain in the problem that we cannot solve. 

There's always a problem in the problem so there is always some variance the problem that we cannot see. Okay, if I tell you, I don't know. Classify all the possible males and females in the world. There might be that some people. 

It's hard to recognize, right if they are males or females, because they were glasses maybe or they have like hats and things like that. This is a variance in the problem. Or if you want the house prices if you want to predict the house price is not natural that all not all the the the apartments with the same size have the same price. Okay, because, you know, there's the wall. So there's something we cannot control the other two terms we can control. So by setting the lambda, we can have a particular bias and a particular variance in our model. And what this means variance, variance means that if we change a little bit of data set, we're going to get very different solution. This is the variance in the model. By us on the other hand is if we have a rate this values, how far we're going to be from the true function. So to speak. 

Okay. In this case, we have high bias and low variance. In this case, we have high variance and low bias. 

And this is the street spot. So now we have a big model class. So we have a very complex model. And by just changing the lambda, we can have a different combination and we try to find the lambda. 

But that's the best job trade off trade off between the two. In this case is this one over here. So the lambda 0.1. 

That's the best job. Okay, in this one you asked to implement that in your project and basically what you can do there you can fit a very complex model for different values or lambda, and by using cross validation you want to see which one performs the best. Now when you have a different model you don't have to define a new model from scratch by using a different lambda you get a different model. Okay. And what is different model implies. 

In practice I define something like that a very high complexity model with high degree polynomial even higher. And then by setting the lambda in different values. I get a different set of weights. So if the lambda is here, I get the setting for my weights. If the lambda is here I'm getting this setting of my weights. 

And it's setting in just a particular model. So if the lambda if we are in this regime over here, this set of weights is going to give me a function that goes like that. If I select this weights here, it's going to give me a function that maybe does something like this. If I select this function, these parameters here, maybe I'm going to get a function that looks something like that. And if I select the values over here for my optimal parameters, this is going to give me a function that is very stable. 

It's like a straight line. So it's setting now lambda. It gives me a different set for weights, different set of weights that I can use to evaluate my model. And each of them is going to be a different function basically. Makes sense. 

I got a question during the break. So when we speak about variance, we don't speak about the variance on the weights, how different their values are. There might be a correlation, but we don't care for that. 

We mean the variance that the final model is going to have for a particular setting of the weights. Okay. Any questions? I know the proof was a bit nasty, but I think you can redo that when you're starting because the steps are not very complicated. We add and subtract zeros most of the times. Add and subtract the same value to get zero. And then we take the square and that's it more or less. And then you have to understand how you can use the expectations, but I think it's a good exercise to understand how these things work. And every time when you have to deal with mathematics, it's good to try to draw next to it a small, you know, sketch about what is happening. And in this case, you can easily, as machine learners now, you can easily probably draw these sketches. What it means if I have a different D and I train a model, what it means if I have different for the same makes different wise what it means. 

I think it's a good exercise for you to practice to understand how these things change and how you can draw them down. Okay. First quiz for today. So the question here is that we're having a model. And then we have, you know, for options as usual, you have to select what is the correct statement. So take five minutes to read it and try to come up with the answer and then together we're going to see what's the correct answer. This quiz is about discussing with you before the break and this more recap we did now. 

Okay. Okay, so first of all what we have, we have here a loss function that we want to minimize find optimal parameters that solve the problem right that's the classic recipe machine learning we define a model. And then we try to minimize an error function. 

Okay, with the parameters. And now we show that you can include regularization. If you want, you know, to be more clever and, you know, to have a high complexity class but then find the correct lambda source of the problem. 

And here in this plot is something they have to do for the project. You have multiple lambdas. And then you try for every, every lambda. 

You compute a training error and you estimate a test error using cross validation. Okay. And this is going to be the function that you select like the one that is, you know, the lambdas this value so it's around. You know, here maybe because the log scale. Anyways, so this is the function that we find at the end after training. 

Because some people that get confused, we don't add anything more here so we don't add the lambda w when we do prediction. Right this is just a part of the training process. Once we're done with the training, we don't care about this term anymore. We just care for the optimal ways we found. Maybe this is very clear to some of you. Some others is not so when we train a function, we just give the function by nothing more. Okay. 

This thing is wrong. Don't add it here when you're deploying the model. Cool. So this is the function. Let's start from the answer be you will see why. So as we increase lambda, the two norm of the weight vector will also increase. This is what correct or wrong. Yeah, this is wrong, right, because this tells me that if the lambda is very big. 

Basically, this has to be very small, because want to minimize this value over here. Right. So the lambda is infinity. We saw that the w goes to zero basically. Okay. There's an optimal way to choose lambda. 

Because, you know, reduce the variance but increase the bias. Yes. Yes. Indeed, the lambda. If we increase the lambda, this increase the bias and reduce the variance. But there is an optimal way to select lambda. Okay, we do cross validation we cross validate every possible lambda. And we see that there is a lambda that performs the best. According to the model for this particular lambda, if we increase this x one, it will increase average points per game. This is just a linear regression question, but the idea is if we increase this one, what is going to happen on the output. Of course, if we increase this value over here, it's has a minus in front so it's going to go down. 

So this is wrong again. And this is the correct one obviously, but why because it says that the blue care corresponds to training error and the black one to the test error. We have seen many times this plot like this you plot. 

So when you see something like that usually the thing that goes up and down again is a test error. But also you can think about it by taking how the how it behaves with respect to the lambda. So when the lambda is very small, this is the best that I can get. 

And what this means that they have zero lambda. And this is very small, which means that I fit very well my data with probably is the training error right. While if I increase my lambda, I make my model more rigid, like more biased. So this increasing increases the error. This looks like a training error right so I don't fit my model, my data, so my model is bad so I don't fit even my data. 

And the test error is this classic you say it goes down, we select this lambda, and then it goes up again. You have to do something like that for the report. Okay. Now we have done all this cross validation generalization. 

Yeah, yeah, yeah. If you don't implement these things in small toy examples, you will not you will not understand them by just hearing me talk. So what I recommend you to do generate a small data set by using a sinusoid or something to generate some points. 

And then play with these things. Generate like some feature matrices by including high complexity models like high degree polynomials, find optimal parameters by changing the lambda right and try to see how it behaves. Even try to implement cross validation by doing that. The simplest thing to do is to implement the holdout cross validation. Generate a big data set and keep something out for training and something out for testing. 

Of course, if you have a very small problem like that one, the sinusoid, don't generate 1000 points because this already solves the problem. You have enough data points, but try to practice a little bit with a code otherwise you will not understand it just by hearing me talking. I'm going to be very happy with this the case, but trust me is like, you know, mathematics, physics, chemistry, we hear the theory. If you don't do something in practice, it's very hard to understand all the aspects. 

Okay. It's a very good idea whenever you hear something in the class to try to generate with code, very small examples like with PCA with linear integration. These are things that you can do easily. Okay. And you can always ask your case if you try implementing a small exercise that you design, you can ask your case for help. 

Cool. Now, we close the chapter of cross validation statistical testing and theoretical analyzing the generalization error, and we go to the topic of neural networks. And why I don't write neural networks here on the top is because we try to motivate them from a more general perspective as generalized learning models. And we have seen that at the end of lecture five. So what we have here, we have some training data. 

We have a model. This is just a linear regression. We will forget what is outside, but we can put a link function. For example, we can have this sigmoid if we want to predict values between zero and one. If this is just the probability to be in class one, for example, right. If we do a classifier, for example, this is the logistic sigmoid, sorry. We define a cost function. In the regression case, this can be just the square difference between the predictions and the actual value that we want to predict, or the cross entropy from the Bernoulli, we have the logistic regression problem. And then, as I said, we want to find the optimal parameters W by solving an optimization problem. 

That's what machine learning is doing. That's the four or five steps. Okay. 

But now let's see how we can get a bit more clever in that part over here. So how can define new functions, parameterize new functions. And why we call it generalized linear model is because this one can be drawn as follows. 

So we have some inputs like height, weight, and a size and age of a person and want to predict something like the income or whatever. Then what we do here, we multiply everything with the weight. We sum everything up here. So with this inner product, we sum everything up and we give the output potentially by having here a link function. And that's a very basic neural network. That's a definition more or less of a neural network, of a component of a neural network. Because the only difference now, if we go to the neural network regime, is that we take the same thing we did before, and we do it many, many times. Then we take each one of these outputs, we multiply them with some other weights, we sum them up and we come up with output. So the neural network is a more complicated architecture of the previous linearized model. 

Why, let's see what happens step by step. First of all, I would say that we can include here a bias there. And I'm doing that, you know, to remind you that there's always a bias that we cannot into the game. So what happens, we have an input, we have the bias, okay, we have some weights. Let's take this note for example, every line that connects this one to this one, it's just the input multiplied with a weight. 

Okay. So we do that for the inputs, then we sum them up. We'll have a value, we have a scalar, just the value. Then we apply the so called nonlinearity or activation function with the neural network case. Here is some example so nonlinearity activation functions. These are predefined is the hyperbolic tangent, the logistic we have already seen, this is sigmoid, or the relu. It's called relu rectified linear unit. So come here, we do this linear combination, we come with a number. 

In this number, we just apply one of these functions, meaning that when we do this w transpose x plus b, the bias, we come somewhere on this line, and then we just add to this value. Okay. And we do that for each one of these hidden notes. 

Okay. So what we have here now we have this four dimensional problem. We have now three values for the input we would get. And now we do the same thing over again, we multiply each of these outputs with a weight, we sum them up, and we give an activation function here, and we give the output. Now, if we have a regression problem here, we don't apply an activation, we just give the output. And then we do this again, immediately, if we want to have a classification problem, we can have like the sigmoid function to give an output that's between zero and one the probability to be in the class or not. But that's the definition of a neural network. 

And that's why deep learning is famous nowadays and that's why you know there's this big hype of AI is because of that architects over here. Maybe it was a bit, you know, like, you know, it's not very exciting, but that's it. Maybe here we can add the bias if it goes. 

Okay. But that's the whole game. What we have the input, we multiply the input with some weights, we sum them up, we apply an activation function, we do that many times. We go in this layer. We do the same thing again. We apply another activation or not, and we give that to and that's what we, we said called multi layer percept, multi multi layer perceptron. Why because we have many of these nodes. 

Okay. And this is a fit forward network. Why because all the information goes in this direction. 

We don't have any loops that we will go by. Of course, you will see in the future that they are more complicated architectures. But this is the basic building block. Theoretically, you can have many, many of these nodes. And the, based on theory, you can prove that whatever function you have, you can have enough of these nodes. To fit exactly the problem. So the neural networks are very, very complex models that if you increase their complexity by adding more nodes basically here. 

You can fit any function perfectly. Which means these are very, these models are very complex. So it's very easy to overfit with these models. 

Okay. So be careful because neural networks, they work well during training. They can fit your data training data perfectly. But because they're very complex models, they are very easy to overfit as well. Okay. That is something that you have to keep always in mind. And not only for this course, even if you go in the future and work on some area, don't take just the neural network and use it for your data. And say, okay, I solved the problem. 

I don't care. There's a high probability that you overfit with non-node course. And now why AI is so high because this basic architecture, you can play as much as you want. And it's very flexible and you can do many, many crazy things. So we can change, of course, the activation functions. We can change the architecture. 

We're going to see later in the, your studies how you can do that. You can change the cost function. We can change the number of neurons. We can change the training techniques. We can change a lot of stuff when training the networks. Yep. 

Okay, we're going to, we're going to see the big later. So the question was what happens if we have many layers? This is the deep learning, basically in deep learning. 

What you have, you have this thing and you do the same thing over and over again in couple of slides. We can see that the question was if we change activation functions, typically not. So you set that activation function, but this is an open problem, right? It might be that the research comes in a few years and says, yes, you can change activation functions. 

So this is the best thing to do. This is an open problem. We study that and we're not yet sure how they behave. So it's really hard to tell you all the details. 

Okay. But we can play a lot about the architecture and other three parameters of this models. What we have to do to train those is to find optimal ways. And you can already see here, only with these three hidden nodes, we have a bunch of weights. We have one, two, three, four, and another, now 15 more or less here. So we have 20 weights, more or less, only with these three nodes here. 

You can imagine what happens if you increase those and if you have more, you know, nodes in other layers. This is a nonlinear optimization problem. So you would not have a unique minimum as the linear regression. In the linear regression, you remember, we have a formula. We can apply the formula and we get the optimal weights. Here we don't have a closed-room solution. 

So we have to apply an optimization technique, a numerical optimization technique to find the optimal parameters w. We're going to say some things later on. Yep. Okay. So if we just take this one over here. Okay. 

How the bias comes into the game. You have this one is the wx plus w0. Okay. And then I apply this activation function. 

That's the same thing over here. So for every one of these nodes, I have my inputs. I multiply them with one weight and I have the bias that I multiply it with its own weight for this particular node. I sum them up and I put the activation function. 

Okay. It's just a linear regression model. So think about having a different linear regression model for each one of those. Okay. Something like that. That's why we call it generalized linear model because a linear model that is generalized. It's not our terminology. 

It exists. So let's see more into the details like what happens. The problem with neural networks that I realized from the students is that you have a lot of indices, a lot of nasty equations and stuff like that inside here. 

But if you understand how the information flows, then all these things are just a matter of writing this thing that you understand from the picture here how you can write this mathematical formula. Okay. So when we see this X hat, what is the X hat? We have seen that this is the X one X two X three X four and we put the one at the end. Okay. Which means that if we see X hat, typically there is a bias there. We're going to go over here. So this thing inside is just w transpose X hat. 

Sorry. And this is w j one times X one plus w j two X two plus plus and the bias of the zero. So it means that each one of these notes has the particular set of weights. 

Okay. So in this case, we have three different sets of weights for the first layer. And that that's why we have the here the w with the J. Each of them represents the weights of the different neural. 

Okay. We do the sooner products and we apply the activation function. Let's say we applied it hyperbolic tangent. So if I give my input, like the height weight, suicide and the age of a person, I add the one at the end and multiply them with some weights. I get the number and maybe this number comes over here. I put it on my activation function. 

So what is going to get out is going to be this value. I do that for my second note, maybe now for the same input, I have no different weights. So it might be that I come over here. So this fits out this value and I do that for my last net note might be that comes here. So it's pitch out this value. So I just have the same input and I have now three different output values. 

Okay. Now I have the weights of the second layer. So the weights of this layer and we see here that we have something else, which is the bias is well over here. I just write down the bias graphically because I want you to remember that there's always a bias. If I had it written, probably you wouldn't mind. But you have to be sure that I include my bias or not. 

It might be that you don't want to include it, but then you have to change also your formula over here. Okay. So now this, this values over here is the values that the previous guys gave us. We just multiply them with this weights, we sum them up. We add the bias and we apply the last layer activation. In this case, the last layer activation is just a linear function. So we don't put any activation basically whatever comes here. We just take a linear combination of those we add the bias. And then we give it out. And this is a regression function. So we can have this cost. This is going to be what my neural network will predict. It's going to be the prediction that I want to get. 

And I just want to bring this values close to each other. Makes sense. So if you have the linear regression model, you can forget all of those. That's a linear regression model. Like I take my input and I multiply it with some weights, I sum them up and I give the, give the output. 

Now this output is going to pass through an activation function. I do that for some other guys as well. I get some values over here. And I do here in linear regression step again. 

This looks a bit nasty. So if you don't sit down to write your on your own what is happening and to understand. It's very hard to understand only from a talking about it for two minutes. Okay. 

But again, it's nothing complicated. Like an inner product here with input and the weights of the first layer for each different note. You apply the activation function of the first layer, get the values. You linearly combine them with some other weights. You have the bias and you have the final activation. 

Maybe this is a bit ugly because we have here the days we have a lot of W's here a lot of the V is and stuff like that. And a good idea always in muscle learning is if we have a lot of parameters that they look similar, we can try to batch them together to a matrix. So what this light tells you that if we have a different. For every note here we have these weights that correspond to this guy over here we have also the bias once more. Then we can have the weights for the other guy over here. And the bias term over here and so on so forth. So what we can do we can put them together in a matrix how can take the matrix of the first layer. In the first row, we can put the weights of the, or the first guy over here and the bias term. We can do the same thing for the second guy. 

The same thing for the third guy. I didn't do something crazy I just took this, this guy's over here and I put them as rows in this matrix. So now if the input comes, I can just put it here like X1, X2, X3, X4 and one. And if I make this matrix vector product, I'm going to have like each individual product is this one over here. Okay, it's just a different way of writing this thing down. And typically this is the way to express neural networks when you write them on paper because this is much simpler than having this sum in this thing over here. 

Why because this one I can write as follows. So I have the matrix, I have my input and make the multiplication. So now I have a vector with three outputs, one per node. I apply my activation function point wise. So for every one of these elements, and this gives me the output over here three dimensional output. 

Okay, especially for the ones that are going to continue in machine learning, you have to understand how to take vector products and make them matrix vector products. Okay, it's very, you know, useful basically to express something like that with something like this, because now we can do the same thing for the second layer. So this is the output. So take this one, we write it as follows. And this one comes over here. You see that you have that hat, which means that I take this one. I add one. And this becomes my Z hat. 

I bring it over here. I have my matrix w that follows the same idea as over here. So now the matrix w two, in this case, is going to be the weights from this layer. It's going to be v 11 v 12 v 13 because just one node here. I had just one right. 

This is going to be a matrix with one row basically. And of course I add my bias. And that's it. So here's come the bias. 

Sorry, this is the V one zero. The bias comes inside here as well. And just a different way of writing things down. Trust me, if you sit down for, you know, 10, 15 minutes and try to take this thing over here to interpret at first with this function, try to understand how it goes inside here and how then this one can be written in a matrix for everything is going to be much more clear. Don't expect to understand that only by me explain that I'm just telling you what is the recipe, but to make the food you have to cook on your own. Okay. Okay. Cool. When I'm going to put down to the slides and I'll try to write this notes more clearly. Again, okay. So, just for an exercise now, in the second quiz, you have to implement a forward pass of this neural network. 

Okay, so what we did here. You can select either this formulation either the matrix formulation like this one over here. So these two are equivalent. 

This is just a compact form. Try to take this input over here and put it through the network using the function over here to get the output. Okay. 

You can avoid making the computations just try to understand how this matrix vector products should work. Okay. And as I said, maybe 20 minutes ago, it's a good idea to sketch something down. Maybe for some of you is going to help you take this one and try to make a sketch over here. How this would look like. Okay. So take five minutes, and then we're going to sit together the solution. Yeah. So, I just put this formula over here, and I just tried to draw it with a sketch right like draw things down. And what we have here we have our inputs, our inputs are coming over here right. 

1234 values 1234 values. I add my bias there is I have it here. Okay. And now what I'm doing I'm having here. This five values, and I'm taking the inner product between this one and this one. If I put one in front because this how it's coming over here right. And just taking an inner product. So this one multiplied by one this one multiplied by this one, this one by this one, this one, this one, this one, this one, this one. 

I make the multiplications I sum them up, and I get a value inside here. Okay. And I do the same thing now for the weights of the right guy. Yeah, okay. Again the same thing. This guy is multiplied with one. This guy with this guy, this guy with this guy. 

Okay, and then I take another ball over here. Now I have to apply my activation function and activation function in this case is this real activation. And what this tells me that this takes an input. And it takes if this input is below zero, it just gives output zero. 

If this value is above zero, it just gives this value up as an output. Okay. It's that simple. So I have here a value and I check it's above or below zero. And if it's above zero, I just put it here. Otherwise, I just put zero. 

That's it. So now I'm having now two values over here and I have my bias there. My bias there my peers here. 

Okay. So now I take these three values and multiply this guy with this, this guy with this, and this guy is my bias. I sum them up again. 

And this comes as the output. Okay. So you see it's not that hard. It's just a matter of making some multiplications in the summation. Then applying the activation function. 

This can be different. This can be like the ton activation. This can be sigmoid. 

There might be 20 activation functions already in the literature that you can find. But let's say this is one of the most common. Okay. 

And this is a forward pass. And once I do that, I get an output. And let's say my output I want it to be, I don't know, 10. And this gives me output minus two. 

Whatever. Then what I'm doing, taking the 10 minus the minus two, I square it. And then this gives me the arrow. How far I'm from my goal. Okay. That's it. 

And eventually I want to find the correct weights such that to take the correct output. I don't know. Do you have any questions here? Do you think something is unclear? So the output in this case is the 1145. And you see that while you have to be careful is that if you make this multiplications, this inner products, you have to see that one of them is going to give you a negative number and one is going to give you a positive number. So this guy over here, the activation function will allow this number to go out. And in this case, it's going to output zero. So it's going to become zero. 

Because my activation function is going to take, okay, the guy who comes inside is negative, so I'm going to give zero outside. That's it. In the uploaded slides, the solution exists so you can take it on your own. Now, what we can do more? We have discussed what happens if we have just one output, but we can have here more nodes with just different number of outputs. For example, we can have the same guy like high weight, eights in suicide and predict and blood pressure, polystyrene level and heart rate, something like that. So giving one input, now we can get multiple outputs. This can be just values we want to regress or can be just classification problems. So we can give an output which is in the form of zero, one, zero, for example, that tells me in which class the guy belongs to if I have three classes, but if I have 10 classes or 100 classes. So here I can give the number of classes I want, for example, as output. Then whatever comes inside here, okay, we're going to see later how we can try to fix that to give a value that sums to one, like here. 

But the point is that by selecting the number of output nodes, I can, you know, change that with respect to what output I want to get. If we have a regression problem, this can be the heart rate, polystyrene level and blood pressure. This can be the predictions I have. And I just take the square difference, I sum them up and this gives me the error. Okay, makes sense. Now deep learning. This is what we have seen so far by stacking more of this on top of each other, we can have more complicated functions. And that's the whole point again, why AI is hype because by doing that things over here, a lot of hidden stuff happening and this helps a lot in the complexity. 

I don't want to go into details because there are many, many courses that they do that later on, the deep learning course in the advanced muscle learning course to discuss more about this kind of stuff. But that's the whole point, we cannot add more layers and that helps training complex models. And what I mean complex models in the course we have seen simple functions in one day, you know, we can fit functions in thousands of dimensions. Okay, that's what it means, complex models like to GPD, for example, yes. 

What's the time to operate more people? Yes, okay, I don't want to go into this discussion because we can stay more than 10 minutes for sure. But the whole point is that if you have only one layer like this one over here, there is says that if I add enough weights, enough notes, I can fit my function perfectly. Why? Because if I have the RELO for example, if I have four nodes and I have the RELO activation, I can construct this kind of step functions. So I can construct a step function for my each of my training points having enough of these nodes, and I can certainly send the steps for my training points, which means I can fit my data perfectly, only by having number of points times four hidden nodes. Okay. Now, if you have more layers, you can do that in a more complicated manner. Just as a hill, you can have even neural networks within finite layers, so having infinite number of hidden nodes. So there are many, many stories I don't want to go into this discussion. 

I can do that after the course of the lecture if you want. But what do you have to keep in mind? We can increase the complexity by stacking more layers. And they're more complicated architectures. Now the training, and that's super fascinating. I'm working with that kind of stuff and I really love them. So what is the idea? 

If we have a problem in muscle learning and want to optimize it, the best scenario is to have a cross-form solution. But this is very rare the case. It's pollination and aggression and things like that. In practice, we have a cost function. We can compute the gradient of this cost function. The gradient tells us the direction where the cost will increase. 

So it's going to tell me this is the gradient direction and we move to the opposite direction. That is called the gradient send algorithm. This algorithm helps us to fit the parameters of a model by iterating this process. So we are at the position, we compute the gradient, we go to the opposite direction and we do that iteratively until we reach a point where the cost is very low. 

That's a numerical optimization scheme. And this is the best case scenario. So we have a convex function. So we have a unique minimum. 

We can go like that and find it. And precisely that's what is happening with linear integration, for example. So you can solve linear integration with gradient send as well. And this is the one-dimensional problem. You see that I can take small steps until I reach my optimal position. Okay, that's a gradient send algorithm. Here I have my cost. I compute the gradient with respect to my parameters and I just move in the opposite direction. And I can take my normal position by doing a small step. 

Okay. The story in neural networks is not that nice in the sense that the cost function, the error doesn't look so beautiful as this one over here, like having a unique minimum. But it looks like that. And even more, it looks something like that maybe. And this problem is in thousands of hundreds of dimensions. For example, you can have easily one million parameters to train. So you have a cost function. You have one million dimensions. 

Then you want to find a point in this one million dimensions that solves well the problem. So this sounds a bit weird, right? Apparently it works with neural networks. And that's why, again, they are so useful. But all this problem can be boiled down into these small sketch. So what it means to have multiple local minimized that I have one solution here, but I have one solution over here as well. And I don't know where I'm going to end when I'm optimizing my model. Best case scenario is to come here because I'm solving my problem well. I minimize my error. But I might end up over here that I don't solve my problem well. So optimizing neural networks is not so easy. 

Okay, that's a take home message. We have to apply numerical optimization and the cost function has a lot of local minima. It is not so easy to optimize this process. What we can do is I said a couple of sides earlier, we can fix the last layer to have for example three outputs, and we can play with this out of one out of King coding to take like something like classification problem and make it as follows. So we have the three different options to classify and we make this three dimensional vectors as our outputs. So we want our neural network, basically for this guy over here to give a prediction that looks like that gives out three values. 

001 for another guy, it gives these values and so on so forth. So now we train our neural network to spit out three values. And these three values, they have to agree with the training data. 

The labels of the training data. Okay. Let's see. First, what happens in the game. I'm gonna do this. 

Do not get distracted. So in the logistic in the simple logistic regression, what we're doing. We had our parameters that are per point so per point, we wanted to know the probability to be in one class or the other. Okay, and depending on our data, we wanted to make this data as close to one or as close to zero as possible. Based on our data points and the output we want, for example, for this particular data point, maybe the label is zero. So we want to put the theta to be close to zero. 

Exactly. So here, if the P of Y equals zero given theta, what is going to give me is going to give me one minus theta and how this is maximized is maximized if there is zero. So we were doing logistic regression. We're trying to find the parameters W. Oops, we're trying to find them. We tried to find the parameters W. We wanted to find the W such that per point, the theta per point to agree with the label we want to give to the point. 

Okay, that was the training of the logistic regression. We're trying to find the optimal parameters W so that if we're doing this process over here per point to get the theta that makes sense. If we wanted output to be one, we wanted for this particular point, the theta to be close to one. If the real output was zero, we wanted to find the W such that this could be near zero for the particular training point. And we're doing that for all the training points at the same time. That's why we're getting the same W and we're using that for all the points. Okay, keep in mind that this sigma was the logistic. So whatever we put over here is was giving something between zero and one as the output. 

Okay. And that was one more training a logistic regression model. With a binary output. Now if we have multiple outputs, so multiple classes, not the cat or dog, but cat, dog, fish, elephant, whatever, what we can only do. We can take this theta and instead of having just one theta that tells me one or zero, I can have multiple theorists, more or less. And each of them is going to give me the probability to be in one class. 

Make sense. So instead of having only zero one probability. Now I have maybe four guys like theta one, theta two, theta three, theta four. So have the probability to be in one of these four classes. What I can immediately see that I want per point this thing to sum up to one because these are probabilities like the probability to belong in one of these classes. And we want, if I'm, you know, a human, I want this one. If it represents the humans to be as high as possible with respect to the others. That's when we are modeling the problem when we do the modeling. 

Okay. In the logistic regression, we take this one, what was the binary problem and we just do it as follows. Instead of having only one output. Now we have three outputs. The only thing we have to do is instead of having just one weight vector as we had before, now we have as many weight vectors as the number of classes we want. 

Okay. So we want per class to increase the value, the probability to be in this class. The only problem is that if we do that naively, this thing will not sum to one. 

That's why we do this trick over here. So we take our predictions. Per class. We take the exponent of this one to make it positive. So we have a positive value. And then we divide by the sum of all these values. And why we do that? 

Because we want this to be values between zero and one and to sum up to one. Okay. And now the whole training process is to find me the weights. Now I have, if I have a K classes, I want to find the set of weights, K different sets of weights. 

So that my training data agree with their labels. Okay. Makes sense. 

It is the same thing, but now we have just more classes. And you can probably see already from here how we can do that with a neural network. So instead of having only one output in the neural network, we can have multiple outputs. Exactly as we did already with the logistic regression. Take their exponent and sum them up. 

So we have seen that over here. So we take three outputs. This can be just continuous values, right? I take their exponent. I divide by their sum to be sure that this sums up to one. So it gives you probabilities. I transform the output to probabilities. And then I want to write down a cost function that helps me to optimize these parameters. And the cost function looks as this one over here. This probably looks very ugly. 

Say, okay, it has locks, data, I see, blah, blah, blah, whatever. What is the thing only is doing? It takes per point. So it takes for one unique point. 

Takes every possible class. And this is the true labels. So this thing for a particular point is going to be zero, zero, one, zero. So what I'm going to do for a specific guy is going to give me the log I and the true class where this guy belongs. So all of these things, they're going to just give me a sum of n values. 

This is just a compact form. So let's say we have only two data points and one guy or three and one guy belongs. To class one. The second guy belongs to class three. 

And the third guy belongs to class two. Okay, so I have now three, three values over here and I have to optimize them. And these are just values I take as output from my neural network. 

So you see that this ugly formula, if you just write things down a little bit, they just give you some outputs. And for this one guy is going to be this value for this W one. And that's it. So it's going to be this, this, this value for this guy, this value for this guy, sorry, this value for this guy and this, this value for this guy. And then what's a top to my son, this is going to be just a scalar that has hidden inside the parameters of the network. And the same thing for the logistic regression. So when you say something like that, don't get terrified, but try to understand what it represents. 

So this theta represents the probability of a guy to be in a class or not. And then we have just to write them in a compact form. That's why we write it as follows. But by doing this time for every person and for every class, all of them, they're going to be zero except one, one. Okay, and this is going to give me this guy over here. 

If I had only three people in three classes. Makes sense. I know this slide in the previous are a bit tedious because, you know, I cannot simplify, simplify them more. 

It is what it is. But if you sit down and think about it step by step, it's not very complicated. I'm just trying to spit out the value per person, but this value now is not just zero one. It's just a bunch of values could be something like 0.1, 0.1, 0.8. So it's going to be the output of my logistic regression model. And what this tells me is that this guy with high probability is in class three. For another guy, it's going to give me 0.1, 0.3, 0.6. So it's going to tell me with high probability this guy is again in class six in class three. Makes sense. And that's what I would train if my true labels here was zero, zero, one. And if it was the same for this guy over here. 

So I'm taking my training data, training labels, and I want to learn a model to spit out probabilities that agree with the training labels. This is the third quiz for today. Of course, we are over time, so I'm going to very fast explain how we do this, how you can solve this type of exercises. Probably Bjorn told you that when you have something like that, a good idea is to evaluate the model. And what we want to do here, we want to see what is the correct set of weights here in order to get this type of class file. So I'm going to give you a hint what you can do. You can try to take this point over here, evaluate this function and see which set of weights gives you the higher value for the class one is what we discussed before. 

So the guys that are coming over here, the W one has to give the highest value here because it's going to tell me that this guy is with high probability in class one. Makes sense. And the same thing over here. So this is the class three. 

So the guys that are inside here. For this set of weights for the correct set of weights is going to have the highest value. The probability to be in this class is the highest. 

Makes sense. Do you have any question I know this was a bit sloppy but that's the idea. I'm going to upload the slides at the end of this week so you have some time to practice with this exercise. 

Okay. And the last comment about neural networks. Why is the problem with them is that they are very complex model is very hard to interpret them. So here's the neural network. Maybe the green represent the positive weights and if they are big or small represent how important these weights are for the network. 

This is if we have a little bit more notes, you can imagine what happens if I have more notes. It's very hard to understand what is happening for the linear regression problem is much easier. You have a bunch of weights is it to interpret neural networks very hard to interpret. You can see later on that there is a huge direction of muscle learning nowadays that they focus on explainability of neural networks. 

It's super nice, but it goes beyond the context of this course. Some resources and as I said at the end you can see the midterm questions. Go to your T.A. 

session now. Trust me if you practice with these things with small code like for the first part we saw the linear regression feeding the models cross validation. During the lambda you learn the same what is happening the same with the neural networks. Sorry guys, wait one minute for the neural network. You can very easily put the forward neural network on your own. 

This thing we did for the exercise. You can very easily do it in your computer in two minutes or something like that. To code this takes less than a minute or maybe five minutes. Okay, try to do that. Okay, see you next year. 
Yes. 

Speaker 1: Hello, everyone. People in the back, can you hear me up there? It's okay. We have the mic. Yeah. Good. So we have a few visitors today who will say a few words before I start rambling. So let's just see the disworks. Just test it for us. Can everyone hear me? I think it's okay. Yeah. Okay. Yeah. Okay. 

Speaker 2: So, hello, everyone. My name is Martha. And I'm Maria. And we're here today just to say a couple of words about introduction week and how you can become our next circle buddy. So what is an introduction week? It's a welcome event for all new master and exchange students that are coming here to DTU. This year is happening from 25th to 29th of August. And it's actually a week that is going to be filled with exciting, fun and informative events to help all of the new students to settle in. We have some events like campus tour, Danish culture event, some innovation challenges and a little bit of sport with the intro Olympics. 

Speaker 3: Last August, we welcomed more than 1800 students and we are estimating that this time around, they're going to be even more. And this is why it's important for our group to keep on growing. Who you are, we are more than 70 different volunteers. We are from all around the world. And we are all studying at DTU Masters. Some of us are also exchange if of course if your exchange is more than one year. 

Speaker 2: What is actually a buddy? A buddy is a pain volunteer position and you will get your own group that it's consisted of 15 to 20 students that are starting either their exchange or master. And your task is actually to be a mentor for the students of your study line and others as well. And also to make long lasting friendships and increase your network here at DTU not only with your buddy group but also along your coworkers, buddies as well. 

Speaker 3: But what is our role? We aim to help both Danish and international students feel like feel at home at DTU. We are basically the first people that they meet when they join uni. And we are helping them build social connections among new students but also older students. It's about creating a safe space and making sure that everyone feels like they're part of DTU and they don't feel alone especially at the beginning. 

Speaker 2: And also if you're interested you can apply at this big QR code and you can also follow us on our social media to get a little bit of more information what is going on and how we prepare for intro week and how is it like to be a buddy. Thank you so much for your attention. Any questions about this? 

Speaker 1: No. Then you stuck with me. Oh you don't need that one. Do you need that one? Yeah. Bye bye. Bye bye. All right. So welcome to week nine of introduction to machine learning and data mining. Hopefully you have enjoyed George's, hopefully George's has entertained you for three weeks on cross validation and neural networks and all of that. And today we are going to build on all of that a little bit but in the final week sort of present sort of a meta method called ensemble methods for maybe combining some of the models we've seen so far. 

So we'll build that up. Before we do that I think George is mispresenting the midterm or something like that so we are now stuck actually looking at the midterm quizzes. So the results are going to be at the end of this presentation or not the results sorry there are solutions. So there you'll get to see sort of the usual very brief model like solution for the questions. So this results seemed I compared it just offline with some previous semesters and it looks surprisingly similar. So I think you're doing as good or as bad or whatever you want to how you want to interpret it as previous semester. 

So nothing new there. There are a couple of questions that I just want to briefly highlight how you can maybe solve because these are the ones that seem to somehow confuse a bunch of you. So again the midterm stuff it doesn't enter in your final grade it's just OK a few more questions for you to practice and see that you've actually kept up with the curriculum. 

Good. So one of the questions that actually somehow causes some confusion is this one. So you're given let's say a classification problem so there are three classes in this one. 

You're given the green blue and red and I hope you're not colorblind because then that might cause some problems but in general green blue and red classes and then you're given suggestions for classification three here so three sorry and you're not actually given if it's yes or no this way but you don't need that in this case. So now sometimes you do need to know if this is the yes branch or this is the yes branch or your vice versa. Good. So we are now given some information here. 

You are told that blah blah blah. We have the problem here and you're giving a decision tree. We remember those from way back and you have two notes where you need to you're basically asking a question at this point. So A and B two questions and then just a little bit of notation about how we actually let's say write up the coordinates for these points and then something a little bit interesting here namely that basically we have an observation in this point here. 

So we subtract from the observation minus point two that means essentially it's centered around point five point five which is something we'll use in a bit. Thanks. Okay. So in the end here the correct answer is a as far as I believe and I'll show you a way to actually solve this but before we get to that point let's just remind ourselves of a few things. So you're given a question here that's written in oops. 

That was not particularly straight or even or anything like that. So we are given a question here and the question is whether or not this thing here when you evaluate it using some norm in this case that's what these double lines or some vertical lines mean and then the question is whether or not you're below point two five. So the question if we are to actually form like this is the norm of this of the argument to the norm is it less than using the infinity norm is it less than point two five. So how could we go about actually figuring that out and if we remind ourselves what the norm is written here. So essentially what you're doing is a norm of the difference so we have x it could be in here. Let's just say it's point five. Now you're subtracting some mu it's a vector so in this case it's just point five point five. Okay. 

So if we are at this location here it's going to be zero and that's definitely less than point two five and then the infinity norm we need to remember we need to take the maximum of the two coordinates. Okay. So okay you could go check all of these points and see for which one is this actually the case that a is correct and then you end up over here and of course it only needs to be or it should only be correct for the stuff that's sort of within the blue box here. Okay so you could do that so you could take all points manually essentially or strategically choose some points that's one strategy. Another strategy is actually to draw what this means sort of visually and I prefer to do that because I get confused when I have to let's say jog all of these different points at the same time and rules. 

So one way to do this so now I've just made it such that it actually has equal axis which is probably what you need here and then I've drawn a few things here. So in this plot here I've drawn what this rule means. It means that the norm to the argument that means to any of the points here needs to be less than 0.25 and if you remember the infinity norm here that's what it means when we put this infinity thing here as a subscript it basically creates a box. So if I go here and then check whether or not I'm within the boundary then I need to evaluate for 0.5, 0.5 I put that in as a practice so 0 again as we talked before it's below 0.25. So I've indicated all the points that are within this boundary here. I've indicated that with this yellowish thing here and that seems to fit. Okay that means now I have a question I can ask if you're within this yellow boundary defined by the a question then you should be in class A and that matches basically what we have up here. Now what about the next one so if you're not in the yellow box here defined by that norm or that condition then what? Well for the option here sorry did I put... Let me just see... So if I look at option D then the other question B here is sort of a simple question to some extent. It says okay the norm of the vector basically the point so we could take 0.5, 0.5 again let's do that over here and ask is that below let's say 1 and if you remember correctly that and then the one norm basically creates a triangle here so everything on this boundary here so I'm now saying it in a slightly different way everything on this boundary here defined by this turquoise-blueish thing here is exactly a distance of 1 from the origin 0 from this point here so there's a distance of 1 to this point here there's a distance of 1 up to this point there's a distance of 1 up here and so on so forth so it defines this diamond shape okay so that's a way to basically write up this question so when we know in A when we know we're not in the yellow one if you then in the blue one then you should then be class 2 and that seems to fit whatever we're given here so it's all about checking these conditions one by one essentially and once you get the hang of it you can sort of easily draw it and think about how they will look so what are the rules from the question and I think that's maybe what will be of interest to you namely what are the all of these norms because I said one norm and infinity norm we've looked at those so we also had some other conditions that are it turns out over all that these are wrong but what would this condition here look like for example like this what's it called this two norm here okay that's when we raise the individual coordinates to two and then sum it all together and take the square root that's the Euclidean norm what would that look like that would look exactly like this here right so we look at the okay what's the threshold that's the diameter basically from from where we have centered our shape in this case we need to center it at 0.5 0.5 and then we take a radius of 0.25 from that and then we draw a circle and in this case that gives you the green one and we see that that rule it doesn't really correspond to that condition we had that option of A being yes then you are not we're not capturing essentially all the blue points here then what's the other one here we had this infinity norm here and it needs to be less than than than what's called one okay so it needs this radius of one here and of course like before the difference is that the the radius here has changed and it's still centered at 0.0 instead okay so this can be a little bit tedious in some way but but it's to some extent forces you to think about these shapes and how they look like I think for me that's the easiest way to do it another way would be to check a few points for all the different options you have available and see which one which one actually fits the correct or gives you the correct class the problem is you have to check a lot of them so it becomes a little bit a little bit easier to try to do it the other way around where you first check condition A and see okay which one of these would fit that condition here in this case it would need to give something that has this square blue shape okay so hope that makes sense so make sure you can can do this there are like tons of previous exam questions dealing with this norm or these types of of questions because for us it's easy okay checks that you know about the decision trees it also checks that you actually know something about the norms what's the difference between one to an infinity and it could also be more general okay then there's one question here which always battles us how why do you actually get that wrong not all of you know but 70 I don't know 20 25% or something so this is about SVD so remember the SVD thing we send her our data matrix sometimes you also standardize it we decompose it into into some interesting matrices using the SVD so we had something like this where here we had the U matrix we didn't really talk a lot about but then we then had the sigma matrix where we had the singular values essentially corresponding to eigenvalues corresponding to the variance in the end then we had the V matrix so the V matrix not transposed in the columns you will have the principal component directions aka the eigenvalue vector sorry okay so what do we need to do here so remember the sigma matrix in that one let me just so sigma here would be a matrix of size in times M and in this case it seems like we have four features or something yep let's assume that and then we have the singular values here one two three four in this diagonal over diagonal everything below here is going to be zero this is going to be zero and this is going to be zero okay but the important thing is that these values indicate how much variance is explained by that principal component okay so what do we need to do and it's the relative variance right so if we want to actually look at this question here we are given some singular values and I think we are if you're looking at the correct answer here the answer you need to give is not logically correct but here it says that the second principal component accounts for more than 20% of the variation in the data okay so of course once we projected it onto that does it actually capture more than 20% okay what we have to do we would need to figure out how much does principal component two actually capture and we know that from the sigma matrix so we take the singular value corresponding to the second principal component square it and divide it by the sum of all the squares singular values okay that gives 19% that means that this statement is sort of logically wrong but maybe what confuses you up here is that we are actually asking for the question that's wrong okay I think that's why we see the results we do okay as it turns out that's also the case in the next one maybe that also explains a lot of the let's say less than ideal performance on that question in that okay simply negating the question here makes people confused including myself so just be very careful I think that's the bottom line here okay so again we are given a classification problem and it appeared simpler right in the sense that you only have two classes and it's separate like this and the question here relates to various types of classifiers so we just consider the remember we need the one that's wrong so if you just consider the first one the two classes can be perfectly separated by a logistic regression model using x1 and x2 as features what do we know about a basic logistic regression model it can do linear decision boundaries so yes I can certainly train and remember the logistic regression model is something that assigns a probability to let's say class 2 given our weights given our input something like squeezing it through the sigmoid thing here whoops an offset that's a weight and then minus v1 times x1 minus v2 x2 so all I have to do is find some weights where these v1 sorry v0 v1 and v2 that can actually produce a decision boundary here meaning that if you want to put a class 2 on this side up here that means the probability of being class 2 is greater than 0.5 well then your class 2 if you're down here the probability of being class 2 is less than 0.5 so you must be class 1 good so yes that's definitely possible so that's not the answer you're supposed to give here let's wait for this one because b is the answer you're supposed to give okay let's just do c so a logistic regression model can perfectly separate the two classes using only the feature set given by the sum of the two so the sum of the two in this case actually defines this line and if you want to make a very simple decision let's say classify you could simply say well if set is greater than in this case 1 then you're green if you're below then you're blue okay that's definitely also possible now I've just basically given you d because I wrote it up down up here a logistic regression the probability that each observation belongs each observation belongs to the two of belong to two class can be derived from the logistic function yes I just wrote out the logistic function here for logistic regression related to that question this is definitely also correct so now we are back with this one so it says something interesting says a decision tree with less than five nodes so we're given some conditions now all of the usual axis aligned let's say form that means we're putting threshold where you can either be above or below some some threshold on the x1 axis or above a certain threshold on the y axis for example that could be one question okay yes using only x1 and x2 as features so the question here can you actually come up with a decision tree where we have axial aligned splits that means you're considering for example a split at point four the question in the decision tree are you below or above that one then we go down that branch or not okay if I'm only allowed five of these how how advanced can I actually make this so maybe I'll do something like this whoops I think that was five nodes already so it's definitely not perfect but if I give you an infinite number of question you can probably do it right so it's a little bit of a tricky one in that you need to think about what can what can a decision tree classification tree with five nodes here that means five questions what can it actually do when we have axi-aligned axis align splits so this is the wrong answer so that's the one you need to give yeah so I think and again I'm I think the confusion here relating to the two latter questions this one I guess and was it this one I think it a lot of it comes down to the negation of the question I can't promise you or guarantee anything but we generally try to avoid this but now you've seen it so be warned any questions for this so far okay so that's okay the solutions at the back of the in the slides and I'll upload the presentation ones immediately after lecture I think they're also in the preview preview one that was old stuff now onto something new and today we're going to talk about as I said already I think ensemble methods and then a new way to evaluate stuff called receiver operating characteristics or and the related area under the curve this is described in these chat chapters I think these are the updated ones so that's should be okay and of course as always give a little bit of feedback and and I think you've been very good at that so far so so by all means continue that good we're at the end of the supervised block I think did I cover this one and then George's covered three weeks and now we're here so I'll try to sum up a little bit just to also motivate why we need not need but might be interesting to look at the methods we're going to talk about today then next week we're going to talk it's not these the holiday next week right it's the week after yes okay so next week I'll talk about k-means and hierarchical clustering and well you're stuck with me for the rest of the semester basically so I'll cover the rest but we'll have a blog and on supervised learning one thing is that the projects to is due next week at five o'clock on Thursday so do remember that it's five o'clock it's not midnight okay so I just made a few scribbles here in terms of where what you have covered in the last few weeks in the supervised block and I think you've seen a lot of methods and that's maybe confusing in some sense of the boat a lot of methods or models as we would probably call this you've seen decision trees regression trees you've seen linear models in the context of linear regression and logistic regression you see nearest neighbor methods for both classification and regression you've seen the e-base classifiers you've seen base classifiers or Bayesian classifiers you've seen neural networks from last time and I promised George is just to highlight a few things about the neural networks namely that to some extent they are just combinations of linear regression models or logistic regression models where you then introduce this activation functions what did I do or it's the could you close the door up there so the neural networks here to some extent a combination of smaller simpler things namely the neurons and basically linear projections between the layers and then this nonlinear activation functions that you have to choose and in the end the neural networks are sort of the most advanced model we are going to look at because it's a nonlinear model and it can do nonlinear functions so we had a linear regression model that's a linear model so it's linear in the weights but it could do nonlinear functions by doing a feature transformation like doing this squaring all the features like that so just remember that so there's a bunch of methods here and they all have this thing that we've been trying to emphasize namely that there are some parameters that somehow control the complexity and we really really really need to worry about that because the let's say goal of machine learning is to build on train models not build well build train select models that do well on on scene data that means this let's say focusing on the generalization error that George has defined for you in pain for pain for full detail I would imagine so we need to control complexity to avoid overfitting but still being able to do well so essentially addressing this bias variance trade off that George has talked about so basically come or at the end of the day comes down to controlling complexity of your models and what we've been doing so far is basically okay we can make the models more and more complex and we just have to choose than a simpler version of it somehow by for example for trees control controlling the depth or the leaf purity that we aiming for in the decision trees for example we have the linear models where complexity was sort of made up can we select a subset of features or can we actually do a feature transformation so do polynomial regression or even like taking the sinusoid to one of our features if that makes sense okay so we need to select which from information to make because if we add more transformation it comes a more complex model to get more parameters as sort of a proxy for complexity here. You need to choose case the number of neighbors you're actually considering in the base that not so many but at least you could choose between the base and the and the base version of it and maybe some of these parameters related to the robustness estimates of the conditional probability distributions. Okay now the neural networks is sort of the the worst one in this family so to speak in that there are both number of layers there's the activation functions you can have different ones at different layers you can have different number of units in each layer so here you could have two or you could have a million if you wanted to okay so it's annoying in that sense it just turns out it works really well okay but you also need yeah as I said think about the activation functions and all of that okay. 

Now there was sort of a thing where we talked about how cross validation could be used to do this so we keep out at some of our data and then evaluate and select the models for selecting one of the parameters explicitly. Your George has also talked about another method which is an implicit method or way of controlling complexity that is where we take the cost function so he has shown for an arbitrary function. Let's say a neural network has an input Xi some weights a lot of weights presumably and then the true label or measurement and that's the cost function that's the it's not the mean square error is just a squared error here and then you're adding a penalty here and this is where you look at the two norm of the weights so it could be better. 

Basically any norm depending on the norm you get different properties but somehow you penalize the model by favoring weights that are smaller that tends to avoid these very weakly functions. But in the end of the day you need to choose the regularization strength this lambda thing that that that George is talking about and that we also tend to choose using cross validation. Unless you have very good knowledge about your problem which we never do so what do we end up with well we end up with this trade off here where you have complexity increasing on this axis and then you have the error here. 

And then on some held out data set you see some curve like this and we are interested in this point here this is an idealized curve obviously it's never going to look like that. But we're going to choose the one that has the lowest error on the validation set and then we know and hopefully George is really emphasized this then we know that if you choose this is estimate here. Just by one data set here on one held out data set we know that we need to do this outer loop in the two layer cross validation because this stuff here is going to be optimistic and optimistic estimate of the error here. So if someone comes and ask you about what is the error of your methods on some new data that I haven't seen before well you need to report the stuff you get in the outer loop. 

So hopefully he's sort of emphasized that now. What does all of this has to do with what we need to talk about today well if you think about this what we're doing here is sort of starting with models that are very complex or can be very complex. And then we are somehow trying to you know keep them in check by regularizing them or choosing parameters in your model such that they don't overfit. Could we go the other way and say OK I'm now going to start out with the simplest model I can think of almost and then actually combine multiple ones of them maybe trained on different data something that's exactly what we are going to do today so in some sense where neural network starts up here and then we are bringing it down somehow. Now we actually going to pick a very simple model and the model we are focusing on mostly are going to be logistic regression in the lecture here and then you can extend this. And then from so called simple or let's say we classify as a model we're going to build a powerful one. 

And that's the whole name of the game today. So we're going to talk about these ensemble methods or some just a collection of things right. So exactly we're going to combine multiple weak ones into a strong one and there are different ways of doing that and we're going to look at to bagging and boosting. So if you just look at a simple illustration down here what we have is some data set and you just consider one data set for now. And then we're going to build a number of classifiers and then we're going to combine it at the end. 

And you can just think about majority voting. So if you're looking at how many of the classifiers here actually voting for class zero or one how many are voting for class two and then you take the one that has most occurrences. Okay but how do we actually build these different classifiers. Well we can sample different data sets. You can use different input attributes for each model or just randomly select a subset of the attributes and train your model. 

Okay you can do that. And then we can manipulate class labels or it could even be different models that you have here. So sort of the most crazy ensemble we can think of is taking OK five neural networks, ten logistic regressions and a couple of decision trees. We tend not to do that because then we get confused as well. So it tends to be sort of the same model family either let's say linear models like logistic regression or decision trees. We're going to see an example of that. 

Right good. So we're going to look at that namely ensemble methods in the context of boosting and bagging. And then we're going to in the second half you're going to look at class imbalance and what people tend to do and that's not always a good idea resampling on the over sampling. Okay and we're going to maybe just say okay we don't want to focus too much on that but you can do it if you want. And then we're going to say okay but can we at least quantify can come up with a measure a relation measure performance measure that's invariant to the class imbalance for in the binary case. 

And that's basically the RC curve and the area under the curve under that curve. Good. Back to the ensemble methods. 

Oops. So what we're going to do here is present this in the context where the way we compute or construct these different classifiers here. That's going to be by sampling different versions of your data set with replacement. So if you give me 10 data points I can sample that in many different ways if I'm allowed to use replacement. And then I'm simply going to train T classifiers based on these different or these T different data sets. And then I'm going to combine them. And the way I'm going to combine them is using majority voting. 

So we're going to see how that pans out and what what why that might be a good idea. So in general these types of ensemble methods that tend to provide slightly better accuracy. So in some context that's what we are after but it also relates to what we've been talking about in terms of let's say reduced variance and stability. And you can maybe the iteration here is that you can think about a data set where there's a subset or a few data points that really screw up the whole thing. And if you subsample your data set that going to be classifiers that do not contain these crazy or annoying data points and they're going to be good. 

Whereas a few of them where these data point actually is included they're going to be bad. This is one sort of iteration about this there are many more and the technicalities. It's not really something we expect you to know but there's a few references for some very technical stuff about why this works. But the simplest intuition about this that that we're going to provide you is that if you actually look at so we're going to look at the performance of this setup. So if we imagine in an ideal world we can actually create 10 independent classifiers. 

That's a huge oversimplification we can never do that but just play along here. And then we are going to do that in the context of binary classification and they're all going to have magically an accuracy of P. And I think we're going to use point seven in a bit. Then we can actually talk about what's the probability of this classifier using this majority scheme. We can try to write that up and that turns out we said if you have 10 or let's say actually say five classifiers. If the majority is going to be correct then we need three out of five to actually be correct. So we can write out the probability of that and the way we do that is simply some basically half the classifiers and the probability that that each of these is correct. 

So in our case with five it would need to be three four and five are correct. So if you look at that probability that is the probability of this majority voting scheme being correct. And that is sort of the accuracy of the overall ensemble method. 

So what happens if we actually plug in a number. So I have a number of classifiers down here. I have a certain accuracy for each of the classifiers and I then with this really really crude assumption I'm now going to look at what happens. So up here you have the chance of the probability of the ensemble method being correct. On the x-axis here you have the number of classifiers that I choose to include. 

If I choose only to include one classifier I'm obviously going to get the let's say accuracy probability of being correct being the point seven that we defined ourselves. But what happens if we actually increase this. So what you see is that I only by including 15 here I actually get an accuracy of point five nine five sorry. 

This is in the ideal world it never pans out like this. So remember here I assume something very specific they all have the same accuracy they are more critically they're all independent which is never the case. But you do see tend to see something like this where you get in when you fiddle around with all your data and trained a lot of models if you then do the ensemble afterwards. Of maybe one of the models you tend to actually get a few let's say percent points in terms of accuracy out of your out of your system. And if you know cackle and all of these things so anything to do on tabular data like the data you most of you are working on then ensemble methods tend to be the winning solution in in like I don't know 80% of the cases or something at least a lot. 

So ensemble methods is generally a good thing so let's see what happens in a concrete case. So remember logistic regression and I think we are already written it up in the sense that what we are modeling is the probability of him going to do class two given some input X given some weights. And that could be a vector and it is a vector in this case X is 2D so we have X1 X2 I have some data points and I want to build a classifier. So I'm training my new also not my logistic regression model here to get a probability for the class being two and I'm getting of course I can evaluate that probability anywhere in that space. I can of course evaluate it for all my data points so if I evaluate it down here where the true label that means the yellow one here the filled yellow data point. Well what's the probability of that one being to which it's supposed to be so the probability of that data point being to is probably given my input X I and my weights is probably out of that's just a close to one. So if you look at another data point it's supposed to be blue what's the probability of this point being blue. Sorry being yellow sorry so class two still. 

What would that be that would be something almost zero basically so let's just approximately zero here and of course that means then we are going to assign a class not to that one but class one. And that's what's indicated by the colors here. Now you can see this gradient of the colors changing here so if you actually you know you start out here in space and you actually drew a line across here what do you see. Well when you see when you come across here you actually see the SIGMAR function so you're going to see the slope going up to one so it goes from zero here. 

Slow slow slow and it goes to point five here and it goes to one here. So the decision boundary we've chosen to show here is the probability of class two by the way also class one being point five. So if that's I've decided that now that I want a decision boundary a threshold at point five then what happens if I do this hard classification which is shown on the right. So what I've done here so of course over here the colors indicate the probability of being class two over here they simply indicate the class. So if you're on this side that means you have probability of being class two higher than point five you become yellow the end no question asked. Conversely the other way you if you're over here you become red. 

Oh sorry blue blue of course there's no red here. Okay so that's the basic setup we have and in general the way you probably did it in when you looked at logistic regression models maybe new projects on the exercises. Yes you're going to put a threshold of point five but why do we do that we're going to come back to that later. Now why have I chosen logistic regression here because in at least in my mind it's a relatively simple classification method. It's just I need to do the inner product between some X measurements and some weights. So it's very easy to actually make this prediction and maybe evaluate let's say the probability through the logistic sigmoid but it's a simple. It's a simple method where you can actually do these linear decision boundaries. Okay so it's not a crazy neural network with millions of parameters it's a relatively simple model here so in this case I have three parameters. W naught W1 W2 so three parameters I can actually do a decision. 

Okay so it's a simple model some would call it a weak model just for the sake of argument. So how can we combine multiple ones of these into an ensemble method. Well as I alluded to one way to do this is simply to retrain on different data sets that we have sampled with replacement from the original data set. So I've given you an original data set here so 10 data points these are basically just indices into your data set with 10 points. I'm now going to sample with replacement so if you look at the first sample data set well I've had three here I have a three there so that's definitely replacement. 

And well maybe there's an occlusion of a data point or mission sorry yeah 10 is not in that one right but it's in one of the other ones. So if I now train my simple model my my logistic regression models on this data set I'm going to get different classifiers. We'll see if that's a good idea. But anyway so that's a we'll sample the different data sets that we just seen we're now actually then going to train the models and then we're going to combine them and do the majority voting. So how does that actually look like so this is just to remember maybe focus on these three points so of course along the decision boundary and see if it actually makes a change where we then do this combination of weak classifiers. 

Okay so I think this one is it's going to be classified as yellow and so is this one if you zoom in this one is going to be classified as blue obviously these are a little bit difficult to see but let's see what happens. Okay so first thing to notice if you focus on this top left one here the points where there's no sort of solid fill here the hollow ones they're not being used to train the model here. So that means this decision boundary is going to be different from the one we saw before where we used all the data here we only use the ones that that's a solid color here it's a filled circle. 

Okay so so this point doesn't influence the decision boundary only the ones that are filled and remember that can actually be multiple. Let's say repetition of individual data points that's not shown here that could happen. So what do we get with these eight different data sets well we get different classifiers right and that's what we're interested in so in this case they're not that different to be honest that well a lot of them seem to have this kind of trend or at least something like that. But what do we then need to do to actually make a prediction for let's say a certain data point here. Well what you have to do is say basically query every single classifier and ask them do you think this is the yellow class yes or no. And then we're going to pick the one that the majority thinks about this this case again just recall these three data points. 

Good so what happens. Maybe not a lot if you think about your original classifier something like this but if you include eight of them then you see that this yellow one definitely is classified as a yellow one now if you're on this side of the of the line. And the line here indicates of the majority one so this would mean that for this would the line here would indicate exactly for of the classifiers think that it's yellow. Then it must mean that the other four indicates that it's blue. So this would correspond to our 50% data sorry 50% classification boundary as we had before or point five in probability. Okay so this point has definitely changed if you were to classify it now. I think this one has become even worse in the sense that it's way. There's a long distance from this point to the decision boundary. Okay so something that happened not a lot and maybe it's good maybe it's bad. 

What about over here we can combine a hundred of them. We do see that it to some extent seem to follow the decision boundary but also it follows the data but still not really right that there's a little bit of change in here but otherwise it's very similar to what we saw here. Maybe even worse to some extent so there's an aspect of actually choosing your the number of models in your ensemble as well that becomes a hyper parameter. So the question is why should we do this if you have a hundred let's say I'm going to paraphrase your question. So if you have a hundred classifiers then we still need to train I don't know 300 parameters or something like that. That's correct but it's very quick to train this one and it's linear. In the weights each of the models is linear so it's a very simple optimization problem for each of them. That's one thing then you said the alternative here would be to take a neural network with 300 parameters. 

That's not going to do a lot of things here on this problem I would think it can solve this problem right but this is always a trade off. Do you want one very complicated model and try to control it somehow with realization or do you want to take some really really simple models combine them and achieve the ensemble. There's no free lunch here you basically have to try it I think but you can I usually tend to think about this optimization problem you have to solve because solving a neural network optimization problem. If you look at the landscape there that's it's not convex like it is for the logistic regression so it becomes very difficult overall but again it's a trade off and everything in machine learning is a trade off right. 

You need to choose a model in the first place and you need to define let's say the laws functions you need to define are going to recognize it and so on. So this is just another option for you guys to actually consider that sometimes ensembles tend to be a good idea. I'll show a much better way of doing this ensemble than what we have here because here it's not clear that there's going to be a very big difference. 

And the reason is that we're using a very simple model and when you're combining it in this way sampling in a way we did here then you probably need a slightly more advanced model and I'll get to what we call random forests which relate to trees in a bit. Good but anyway the the bagging thing for this problem it didn't revolutionize things a lot right you'll probably still want maybe a classifier that can do something like this decision boundary like this. So are there other methods of of of let's say doing that yes there's so how do we actually do something better than this. And one way and all well it's just another way of doing ensemble methods is by sampling the data set differently from what we've done before. And then basically thinking about the fact that some of these data points might actually be more important than the others and now we are going to weigh the data points somehow. So I still have my data my data set here and now I have and by the way this is called boosting the whole principle behind this there are many ways of doing boosting but we'll get to a specific one right anyway we have our data set. We start out actually assigning a weight to the data point this is not a weight in a in a linear model or something like this this is sort of the importance of that data point for a particular classifier. So it's like it's well it's different you should think about it differently than from a weight in a in a model. Then what we do is we sample a data set using those weights so it's basically uniform probability of seeing each each data point when you sample it with replacement. But we get a data set we train a classifier. 

Okay, we now have one classifier. Then what we are going to do here, which is actually not shown on this slide, which is a problem, then I'll have to do it manually. Then I'm actually going to look at which of the data points did I do correctly. Did I classify correctly and the ones that have a slide red one here are the ones where I misclassified them. So I'm going to indicate data I can of course check that and then on the other ones it must be correctly classified. 

Okay, what have we learned from that. Well, maybe intuitively now you can think about giving the points on which you do well less weight because apparently you can solve that problem and then give the data points on which you are not doing well. You misclassified it give that a higher weight somehow giving it a higher probability of being sampled. Okay, that was based on this classifier. 

So what if we now do that. So I'm going to update the weights now and the way I'm going to do it is I'm going to increase like I just said, sorry, decrease the weight on the data points on which I did correctly, increase it on the ones where I misclassified. So that means that now this point here is more probable of being resampled in the next round. 

Okay, let's see what happens. So I now resampled a data set with those weights, meaning that the ones with point 17 are more likely to appear than the others. Okay, now train another classifier. So now I have two classifiers. 

I repeat this game. So in every round of boosting, I changed the weights. But of course I obtain a classifier as well. So this is a different way of sampling things, right? I'm not just sampling naively now from my data set. I'm sampling based on what my previous or what my current classifier thinks are the most critical data points to consider at this point. This leads us to a semi-famous and well-studied algorithm or boosting algorithm called the Adaboost, adaptive boosting essentially. So it's doing exactly what I just said. It's just putting a little bit of math on it and a specific way of doing the weighting. So let me just go through it quickly, but I think maybe logically you have to actually step through it yourself. But I think there's a small quiz on that in a few slides. 

Anyway, good. So we start out our algorithm here. So for each of the data points in round one of boosting, then I assign a weight to each of the data points uniformly. So they're all equally likely to now appear in my resampled data set. Okay, I now run for a number of iterations here. But in the first iteration, I'm going to create a data set and I'm going to assemble it with those weights that I've just assigned. 

So in the first round, they're going to be uniform. Every data point has equal probability of occurring in dt here. Now I'm going to train a classifier. That can be a simple one. Let's imagine it's a logistic regression one that we'll see in a minute. So I'll train my classifier, know how to do it, and it's quick because it's a simple one. I now compute and forget a little bit about the math for now, but I'm simply going to compute the error rate on my data. 

I'm just going to weigh it by the weight of the particular data points. That's all that's being done here. But let me maybe write up who's confident about what this delta function does. Let me just write it up maybe so we all agree. 

So here's a delta function. Oops, let me pick another color. And scroll up a little bit. So this delta function here is a function that takes a classifier f of t for a certain input x i. And then it takes the true label. And then it outputs one if you classified correctly. 

So one if and only if f of t, that means the output of your classifier in that round equals y i. Good. You can guess what happens now. Zero if that's not the case. 

Good. So all it's doing is saying, well, did you classify correctly or not? And then it's going to subtract that from one. So instead of getting the, let's say, instead of getting the accuracy, you're going to get the error rate now. And it's going to be weighed by the weight for that particular data point. So it's just a weighted error rate. 

Okay, good. So now we have sort of a performance of that classifier trained in that round. I'm not going to compute sort of a, well, maybe a little bit of a weird number here, but it's essentially just telling me how much to actually trust my current classifier. So it's just mapping that error rate into a, let's say, through the logistic function here. And it's going to be one minus epsilon t. That means the weighted error rate divided by epsilon t. So if you think about a random classifier, what would happen? Well, you'll get one minus a half divided by a half. 

You'll get one. So that means alpha would be zero in that case. I'll show you a couple of plots in a minute. But for a random classifier, the alpha, that means our trust in the classifier is going to be zero. So we're not going to trust that at all. Anyway, that becomes important in a minute. Anyway, we computed the error rate and some transform, let's say confidence in our classifier. Now we need to update the weights based on what we have just computed. 

And we're going to start over here. So it means that the updated weight for the ith data point for the next round, for the next, let's say, iteration in this for loop, is going to be the current one. And then if you're correct, and alpha is greater than zero, then we're actually going to down weight at that weight for that data point a little bit. 

Conversely, if you are incorrect, we're going to increase it. So it's a little bit complicated to intuitively get what's the role of eG and alpha t here, but I'll show you a few plots in a minute that may help you a little bit. But anyway, so now we have updated the weights to make sure the axis sums to one and we can interpret them as probabilities. We simply normalize them. But the important thing here is that we now have defined a specific way of updating the weights. 

That's all we've done. It's just a recipe. We run that for t. So let's say a hundred different boosting iterations so you get a hundred different classifiers. They're simple. They're logistic regression models, for example. How do we then combine them given the things we just computed? And we're going to combine them in this slightly odd way of writing a majority vote. But it's essentially just using the delta function again. So if, and you need to optimize over something here, so let's just imagine we try for y equals one here, so class one. Now we need to see for each of the classifiers on a specific data point, which class, sorry, which class are you going to actually pick? It's going to be one or two. 

We're going to start with one and see what kind of sum do we get in here. So if your classifier in the specific round t outputs one and the true label was one, we're going to get a one, otherwise zero. But we're actually going to weigh that with the, let's say, confidence in our in our current, sorry, in our specific classifier in that round. 

This is where alpha comes in because we have an idea about whether that classifier in that round was good or not. And now we're going to weigh the, again, the majority vote by that. So in the end, it's sort of a weighted majority vote where it's weighted by our own confidence in the classifier. Okay, so this is just majority voting at the end of the day. 

Good. So to understand how, let's say, epsilon t and alpha t placed together or in tags with each other here, I've just plotted a few things here also to keep my own sanity. So here we've got epsilon t and you see that the blue one is alpha and you can then go see what will alpha be given a certain weighted error rate. 

So have a look at this and just make sure you sort of understand what happens. The tricky part here also in previous exam sets is when you have a binary classifier with an error rate, let's say all point eight, that's a very bad classifier, right? But the point is that to get a good classifier, you can actually just strip, swap the label that is accounted for in the setup because if you have an error rate greater than, greater than, let's say, point five, you actually get an alpha that's negative. 

So that means it's counting against itself in the majority vote. That's this last couple of sentences. That's a technicality, but just remember, maybe some of the old exam sets actually place a little bit of a trick on you there. 

Anyway, go through this manually at some point and I'll just show you before the break and quiz. I'll show you how it actually turns out if we do boosting, error boost, using logistic regression on the same data set as we did before. So we have a data set sampled with replacement. We have a certain weight on it. So here you can assume that it comes from a uniformly weighted, let's say, the weights are uniformly or uniform. So that means that we are going to get a data set that's sort of just randomly sampled from the original one and we get a decision line here. Now in contrast to bagging, to get the next classifier, we need to actually look at which ones of the data instances are misclassified. 

So the ones that are misclassified are highlighted with this red bold, let's say, outline here. And that means in the next round of boosting where we train our second classifier, these weights, sorry, these data points are more likely to be sampled. So that's exactly what we are doing now. 

We are updating weights and then we are resampling and getting a, to some extent, totally different classifier simply because the desire now is to now be better on the data points on which we were bad before. And of course we continue this round, this game throughout until we've achieved G rounds of boosting. So what do we get? 

So as you can see now, before we had eight, now we have 10 here. And of course, just be aware that the colors here sort of indicates how many of the classifiers think it should be yellow, right? So if 10 out of 10 classifiers think that this should be yellow and we need to wave our hands a little bit with the alpha weighting, then it should be very bright yellow. So you can think about that type of probability. Okay, so we see something, maybe with 10, it's not a major difference here. But if we actually go very extreme here and include 500 different simple models, then we get a very nonlinear decision boundary here. 

So from combining 500 simple classifiers or weak classifiers, we can actually do something very complicated in terms of the decision boundary. Good. So what was my point here? Anyway, it's just a different way of obtaining this complicated classifier from simple ones. And of course, the other boost is one way to do that, there are others out there. 

Good. So now we talked about boosting and we also had bagging before and I said bagging wasn't so great if you have these very simple models. So you probably, or some of you would have heard about random forests. So if you take a lot of trees, decision trees, put them together sort of in close proximity, you tend to get a forest. And that's exactly what we're doing here in the sense that we're using the bagging principle with a bunch of trees, decision trees, and we're combining those. And there are different ways of combining or coming up with random forest models. Either you can do like we did for the basic bagging model with the logistic regression, we simply sampled with replacement that bunch of data sets. But you can also sample the input, sorry, the attributes, which attributes should you use in each tree at each level. And you can do various things here in terms of your dataset to get different random, sorry, different classification trees. You now combine them in the simplest way possible. 

You bag them together and then you do a majority vote. So that's called random forest. And depending on the implementation, you'll see in Python or whatever, there's going to be quite a lot of parameters that you need to tune here. Because if you remember from way back, you had to think about the depth of the tree and the purity gain or the purity measure, how pure do you allow the nodes to become and so on and so forth. 

So make sure you read the documentation for your random forest implementation. Now, onto a break and we'll make that 15 minutes. Then you'll get a chance to do whatever you want to do and then come back a quarter past. But also make sure you just have a look at this one. I'll say one thing, just think about which and how many of the data points actually misclassified. Good. All right. Let's move on with this adaboost thing. Let me just point out something that's, what happened now? There we go. 

Okay. So in this course, in general, the logarithm without any subscript indicating the base, that would be the natural logarithm. Sometimes we use it when we talk about entropy. We use the two base for bits and stuff like that. But generally, and also I would argue in general science beyond your time here at TCU, when you just write log of X, it's the natural logarithm we are referring to. That means exponential to the logarithm of an argument would just be the argument. Okay. 

We may want to use that a little bit here. So the correct answer here is A. And let me explain this one a little bit because here's mostly about the strategy, not actually putting in the numbers. Because you're not actually given, well, you're given, what is it? 

You're given some indication about what happens after the first round of boosting. Here you get this line and you get a few points here. And you have to realize, okay, everything over here is classified as a red cross. Everything over here is a circle, basically. So we observed two things. 

There are two misclassified examples. But overall, the classifier is actually doing well. Good. That means we can now actually say, okay, on the examples where you are doing correctly, we are going to lower the weight. We know that intuitively before it's doing anything. And on the two where we are misclassifying things, we need to increase the weight. So two where we need to increase, four where we need to decrease. So if we just look at the options already here, then we have the first one. The first weight is decreased. 

This is increased. Well, maybe we should just remember, what is actually the weight in the first round? It's one over six. 

All right, so the weight of the first round of boosting for all the examples of one over six. So now we can then see what has increased, what hasn't. So in this case, we have a decrease here, increase there, decrease, decrease, decrease, increase. 

Decrease, increase, decrease, decrease, increase, increase, increase, increase, so on, so forth. But then you get to this one. So these are actually still plausible. But then you look at this one here. 

Here you see that. As far as I remember, you see four numbers being increased there higher. And then this one, these two are being decreased. So we can actually already rule out C just from our intuition about what should happen for this problem. Of course, you don't actually need that to solve the problem. 

So let's try actually to do what's recommended down here. Let's compute epsilon t. I'll just write down here. So epsilon t is this weighted error rate. So the epsilon t is the sum over all data points and then weighted by the Ith, wait for a data point. 

In round one, in this case, one minus that delta function f of tx, y. Essentially, because the weights are the same, they're one over six, we get, and then we observe that we had two misclassified examples. We get that this sums to one or to two. And this one is just constant for the two. So we get two times one over six, where one over six was the weight in the first boosting round. Two was the number of misclassified examples. That gives one over three. 

OK. Then what about alpha? Well, there you had that it's half log, natural log, to one minus epsilon divided by epsilon t. And if you plug this in, you're going to get that, you're going to get one over half to log two. You can try to plug it in to log two. 

And if you remember a little bit about log and doing stuff, then this will be log two square root of two. OK. Now what do we know about the weights here? Because we have now identified that, well, this one is a correctly classified one, so that the weight must go down. 

This one is a misclassified one, so the weight must go up for the next round of boosting. And that means we can actually, if you remember this algorithm, where do we go? Yeah. So if you actually now, now we have alpha, we have epsilon. 

Now we need to write up this one. But we know that even though we don't know the index of all the data points, we still know what should happen to each element in that vector. So the unnormalized vector here for the next round, for two, round two, is going to be a vector where we have an element where we are going to decrease the weight for the first one. Because essentially that's given by the options you have available here. You can't really tell it otherwise. That makes it a little bit of an ask the question. So here you would need to decrease it by this factor. So I'm just going to write this in a slightly more convenient way. I kind of write this in a convenient way. 

Okay. I'm going to do the inner product between what we had for the weights in the first round. So this is a vector. Now I'm going to do the inner product with a vector. I'm going to write up here. So maybe I need to do something like this. I can't remember. 

But then in this vector, we're going to have all these epsilon raised to alpha t or epsilon raised to minus alpha t, depending on whether or not should go up or go down. So I put in here, the first one needs to go down. We can see these are the only options we have available from up here. 

From up here. So that must be an epsilon alpha t epsilon minus alpha t. Sorry. And then epsilon alpha there, epsilon alpha t. And how many do we have? Oh, sorry. That was wrong. It should be, it's going to go down. Then it should go up again for one, two, three. Go down again. 

So this is going well. Wait, give me a second. I think I got that wrong. So let's just double check. We need to do the following. 

So I'm trying to work out in my mind here as we go along what element should I put in here. So it's a wait time, some factor here and that for instances where it's correct and the way to go down, we need to put in this epsilon raised to minus alpha t for the other ones. It needs to be raised. It's on raised to alpha t. So the first one, we have already decided that it should go down. So that should be an alpha t. Epsilon raised to minus alpha t. So alpha t and then the next one, this one should go up. 

So that's alpha t. Now we should go down again, down again, down again and up. So here we have this one. And then if you actually plug in the number you'll see in the solution that there's a fancy way of doing this without actually using a calculator. 

You just need to remember a few, few rules. So now we have the unnormalized weights, but of course you have all the numerical values for everything that goes in here. So you could in principle put it in the calculator or you can manipulate things a little bit and do it by hand here. Just remembering those two small rules are wrote up here. Then you can then you can do it easily. So this is the correct answer. 

A. Try to work through this because it's actually a little bit difficult in the sense that you might actually confuse yourself from the beginning because you don't have the ordering of the data points. But you can deduce which ones need to go down and which one needs to go up. Okay, good. Work this out. 

Then another, we're switching gears entirely now. So we have ensemble methods that can be done in some cases if you want to combine simple classifiers and even get, let's say, fairly complicated one or powerful ones. A lot of you would have totally different topic. A lot of you would have actually seen that your data sets are unbalanced. In balance. 

Sorry. So this is a general problem and some of you might have already encountered this problem and an example could be, for example, that if you go out into the world and you want to build a classifier for detecting an issue that occurs only in one out of a million cases, then your classifier or you report to your boss. I can build a very good classifier with a high accuracy. 

Simply by always detecting the normal operation mode. That'll give you an accuracy of 99.999. So accuracy here is not a very good measurement. Let's say a very good number to report here because it's not invariant to the class imbalance problem. 

And we are going to fix that by suggesting another way to measure performance. However, before we get to that, I just want to highlight a few things that you'll typically see and probably do just from intuition is namely resampling solutions. So if you have a very imbalance data set, what you could do is sample from the minority class. You can simply sample more examples from that one with replacement. 

Then you, okay, suddenly, magically, your data set is now imbalanced. Oh, sorry. Now balance. 

Sorry. You can do that. However, there are some issues with that and I'll mention that as we go along. But you can also be not really going to do that, but you can modify existing classification algorithms to account for this. 

For example, you can change the laws and certain things you could do here. Or as we will argue here, it's probably best at least or first and foremost to actually measure the performance using a measure that's invariant to the class imbalance. Okay. But what do people do when they actually resample that data set? And it's quite intuitive. There's nothing magic going on here in the sense that if you have this very imbalance data set. And you want a balanced one. Well, you can, you can over sample the majority minority class. Sorry. 

To suddenly get equally many examples from the, in this case, red class as the blue one. Okay. What's the problem here? Some of your algorithms are actually very sensitive to these repetition of data points and will not do well here. And the problem is if your test set actually has this correct distribution of, of, let's say, classes, maybe you don't want to actually. Over sample in order to be able to learn something. So there's not a, let's say, free lunch here, as I also talked about before, in the sense that you really need to think carefully if you're actually going into this resampling business. 

Because you need to make sure probably that the distribution in your training set and test sets are the same. Okay. Anyway, you can do it. 

Just be careful. You can also under sample. That means suddenly, well, from the majority class, the blue ones, you can remove data to make sure you have a data. 

Or sorry, a balanced training set. Now you're removing data points. That's certainly an issue if there's some information there. But of course, if you have a massive data set like Google or whatever, then maybe you can do something like that. Or you can do something in between, of course. 

And in, I think my point with this slide is basically just be careful if you start doing something like this. Because as I said, some algorithms like nearest neighbor method, what will they actually do when you have, let's say, many, many repetitions of the same data point? That could be an issue. But of course, there are also hacky ways of getting around this by simply adding a little bit of noise to each of these data points, the new ones that you have sampled. So my plea, so to speak, is that just be very careful and think very carefully in the domain or the specific problem where you're actually doing this. Now, we're not going to give you a magic method for actually solving this problem because it is a hard problem. What we're going to give you is a way to actually measure it and quantify it. 

So let's focus on that instead of working with dot g resampling methods. Good. So you've seen the confusion matrix before. You have, we have focused mainly, so let me just be careful here. Here the actual ones, we're talking about binary classification, so we have positive cases, negative cases here. 

We have the predicted ones, that's the one that comes from your algorithm. We have the positive, we have the negative ones here. So if your example is actually positive and your method classifies it as positive, then you get an increase in increment here. So you get one more true positive. If it's actually negative and you classify as negative, you get an increment here, so you increase your true negatives. And based on these two numbers, we have previously talked about accuracy and error rate and all of that. However, today and for the next half an hour, so you're going to talk about the false negatives and false positives. 

Because they are really important in certain cases. And just to highlight that, I'm going to now define on our way to a slightly more subtle metric. I'm just going to define a few things here, a few metrics. One called position, that's the fraction of the true positives among the objects you have predicted. Your classifier has predicted as positive. So if we look at the confusion matrix here, it's the true positives. 

So let me actually color code this. I think that might be helpful also for you to remember without having to do it every time. So it's the true positives divided by the true positives plus the false positives. 

So let me pick this one maybe. So it's basically that column. The ones you have predicted as positives, which fraction of these are actually, which fraction of these were actually truly positives. So that would be this column you need to focus on. 

That's the way I remember this. OK, precision relates to this column. That's one measure and you can see it takes into account the false positive ones. 

However, if we are sensitive to false negatives, and I'll give you some information about situations where we might be that, where we might be sensitive to false negatives, namely these guys over here. So the actual one is positive. You said it's negative. That's the false negative. You're falsely classified as negative. 

OK, what do we need to do over here? Well, it's a measure that takes into account again true positives. So the green ones, still this one. But now it actually accounts for the false negatives. So now we need to look at that row. So in the end, we're looking for something that's in both cases one. We want high precision, high recall. If you have a classification method with precision of one, recall of one, it's perfect. 

Right? You don't, well, in the sense that you're not making these type of mistakes, at least. So that means actually you're going to have an accuracy. All the elements are going to be in this diagonal, summing them together, dividing by everything in the matrix. It's going to give you one. So that's a perfect classifier. 

Unfortunately, the world doesn't work like that. So likely you're going to have errors made here and over here. So what are situations where false positives are important? So let's think of a good example. So if you think about the legal system, you probably don't want to convict people for something they haven't done. So if you think about an actual case where some person, maybe not you, but some person goes to court for murdering someone, it turns out that they are actually, well, there's some doubt about whether it's positive or negative. 

But let's just imagine, for sake of it, that it's actually negative. The person is not guilty, but of course, the jury doesn't know that. Are you, if you are building an automated ML, AI agent for actually automatically convicting people, you probably don't want to make a lot of false positive mistakes. Now you're sending people to jail or even worse in some countries. You're sending people to jail for life potentially, even though they're not guilty. So in that case, false positives matter. Of course, there's some aspect of also false negatives in that case, in that we don't want to potentially guilty people to actually walk free. 

But there's a tendency that we certainly don't want to put people to jail if they haven't done it. Okay, so the question is, is it not the same for the medical case? So I'll have something else in mind, but let me do the medical one. So if you're screening people for a certain disease, then which one of these are the most important ones? Do you want to make, if you're ill, and basically, truly you're not ill, but the test is saying that actually you are ill? Is that a good thing or a bad thing? I think that depends a little bit on the disease. 

But what's even worse here? If you do a false negative, so actually you're ill and the test, let's say a screening test, shows you that you're not ill. That means we are missing stuff. And of course, there is some trade-off here, and that's the important thing, that it is a trade-off. Do you want to go for the false negatives or false positives? So try to think about cases where one would be more important than the other. So basically, to make this trade-off, you need to go into decision theory. But you can sort of think about what's the cost of making a false positive. 

That cost is actually, well, you actually treat people potentially for something they don't have, or here you're treating people for something, well, you're not treating people for something they do have. That trade-off comes down to utility theory and all of that, if you want to do it algorithmically. Decision theory, game theory as well. We are doing it slightly simpler here, but I can think about these cases, because they can build some intuition about why are we interested in trading off, to some extent, false negatives and positives, that means trading off recall versus precision. 

It's important for people who want to work in this, and many other fields that you actually have an intuition about this. Now, I'll only put up two measures. Could we not just combine them? And yes, there are many measures for this. There's something called the F1 measure, which basically is a weighted sum of the two, so they take into account both F1, sorry, false negatives and false positives. So there are many, many metrics here, and a very good way to get an overview is to go to the Wikipedia page for, I think, precision and recall. You're going to see something similar to this, and then a thousand different measures out here. And it'll show you, maybe for your field, that we're looking for something with a specific name. It could be sensitivity, and other measures, or F1, or whatever it is. 

So maybe have a look at that, because your field of study, or whatever it is, might use different words than we do here. But it all comes down to this small thing here, table, two by two table. Okay, so let's just do a very quick, quick question here. So I'll give you four or five minutes to actually do this. So try to work out this, and the hard part here is not put in in the numbers. It's finding these numbers. 

So at least try to do that for Classifier 1. Try to put in the numbers you can find here. I'll give you a few minutes for this, at least to think about it. Okay. Put in your answer into DTU, learn if you want your vote to count. 

Thanks. Let's move on in the interest of time. I think this is one of these problems. We're actually doing the computations are not that hard. 

It's about actually interpreting what's going on here. So let me just, the correct answer is D, by the way. I'll let you work out and actually do the computation. I think these numbers are put in are correct. So if they're correct, and hopefully you'll also be able to do the computation. 

So what we're told here is that we had two classifiers. Okay. I choose two colors basically, red and blue in my notation here. It detects 54. That means it's making a prediction where 54 of the inputs are now classified as being positive. So predicted, detected, that's the same. So I just write down 54 because I know this sum must be 54 now. 

Okay. So that's one way of doing it. And then I'm told that 18 are actually positive. That means out of the 54, 18 of those are actually positive. 

Those must be true positives. I do exactly the same over here. Oh, sorry. That's one more detail in that I know there are only 20 actual positive examples in, in my sort of sample here. And that means I know that this row here must sum to 20. So I exploit that to say, okay, I know 18 of them are true positives out of those 20 that are actually true positives only my algorithm only gave me 18 of those detected 18 of those. That means the other two must be classified as policy as negative. So this is a little bit about interpreting what's going on here. So, and I do exactly the same for, for, for the classifier number two. 

And then if you plug in the numbers, you're going to get, get classified. Oh, sorry, option D here. Does that make sense? I see some nodding, not a lot of confusion, but try to do this. And then if you're in doubt, then, then yeah, make sure to ask your TA or me. Now, I had a small rant about us always choosing the decision boundaries in our classification methods at point five. So if I now repeat what we did before, we had the point five here. There's the probability of being the yellow class is point five. If you're above that, then you're, our algorithm tells us that you're classified as point, as yellow. Sorry, class two. If you're below that, you're classified as blue or class one. 

That was in the logistic regression case where we sort of had a score for, you know, we, we could put a threshold on the probability to make a hard classification. Okay. But why do we always choose point five? Is that always a good idea? So let's see what happens when we, when we actually don't do that. So let's see. I wrote out an example here. 

So if we, let's see if I can remember what I'm supposed to do here. So if I have a histogram here, or I'm about to create one, where on this axis down on this axis here, I have the probability of belonging to the yellow one. That's class two, given a certain input X and given my weights. So I have trained the model. 

We're not training the model anymore. And then, then I actually do a histogram of the probabilities related to my, oops, related to my data points up here. Let's try to do that. So let's just see if I count some data points in a certain interval. 

Let's see. This is point five. This is where we normally put our threshold. Let's say point five. But I have here one and then maybe, I don't know, a couple of bins here going all the way down to points, or put to zero exactly. 

Right. So all I've done here is I'm going to make a histogram containing the number of times. I see a data point in that, having that particular frequency, or sorry, having that particular probability of belonging to class two. So if I look at things in this bin here, so it must be like, I don't know, point nine, two point or two one or something like that. So these two points might fall into that. So let's, that corresponds to two roughly here. 

It's going to be very rough this thing here. What about in between? Maybe, maybe let's do that point. So there was one point in this interval here. So let's do that one. And now I'm just going to make up something. And then we have a couple of points here, maybe, and one here and something like that. 

And then maybe a few over here, something like that. So this is just a sketch. It's not actively reflecting what you see in the image up there. It's just for things. But in principle, we could do the same now for the other data points. So these are only for the yellow ones. 

That's important. I only looked at the yellow ones, and then I sort of counted how many times to see that, see a yellow point with that probability in my dataset from my classifier. If I now do the blue one, that means I'm now looking at all the, ah, come on. If I'm not looking at all the blue points or the points that are supposed to be classified as blue, what is the probability of being class two for those points? Well, since they are mostly, I guess, at least this one, this one, this one, this one, and maybe one of these, they're going to be classified as blue. So most of them must be below 0.5 in probability of belonging to the yellow class. So if I now do that, and I'm going just to make up a histogram here. So most of them are going to be below 0.5, which was here. 

So maybe a few here, one here, and a couple here, maybe, and then one here. But importantly, a few of them are going to be above 0.5. These are the ones we are classifying incorrectly here and here, if we set a threshold at 0.5. But who decides or who tells us that the threshold here should be 0.5? 

You can choose afterwards. Once you train your modeling, just say, okay, I'm not going to put it at 0.5. So how would that actually influence the confusion matrix? And that's what we're going to look at now. So if you think about how the confusion matrix is going to look at for a threshold of 0.5, you're going to get certain numbers and you can try to count. 

Okay. But what happens if I now move the threshold? Then I'm going to change the confusion matrix, essentially all the numbers in it. And we're going to do that in a small exercise in a bit. So what would be the benefit of then changing the threshold? 

Well, this is one way of actually trading off the false negatives and false positives. Okay. So by doing this, maybe let's do an extreme one. 

I'm not going to count up here. But if you put the threshold here, what would happen? Remember that everything down here is classified as, let's call them negative. 

Everything up here is classified as positive. So now we're not, I'm just going to put in some numbers here, random numbers. They're going to sum to eight if you look at the actual numbers up in the examples. 

So let's do, I don't know, five and three and four and four or something. I think that's roughly correct. If we put a threshold of 0.5, that means this line here. If we now put a threshold up here of, let's say just one, that means my decision boundary is here. So everything here is going to be classified as class one or the negative class. Everything up here, of which there's nothing, is going to be classified as positive. Okay. 

So what would this one look like here? What, so nothing is going to be classified as positive. So we're going to get zero here, zero here. And then I think that's 16 points here. So we're going to get eight that are classified incorrectly as negative. We're going to get eight that are correctly classified as correctly, because all the blue points here, of course, are still going to be classified correctly. But all the yellow ones are not. 

They're going to be incorrectly classified as false positive, or false negative, sorry. Good. I could play this game. I could put a threshold here. 

What would happen? If I put a threshold of 0.0, that means, or almost 0.0, that means I get a decision boundary down here. Now I'm basically doing the opposite of what I just did. So I can change this one. 

So the one I had over here, you can imagine what happens now. Everything is classified as positive, nothing as negative, i.e. I get 8, 8, 0, 0 here. Now you can compute, you can take the precision recall measures and compute those. But this idea of changing the threshold, you should see that as a way of changing, yeah, precision recall, and then some, but specifically the false positives and false negatives, because that's what we are sort of interested in here. 

However, you can imagine doing this, changing the threshold and evaluating for well, infinite many thresholds and actually computing precision recall, or something else we're going to see in a minute, namely right now. So if you think about this histogram here, this very rough histogram, I must admit that I sort of construct it here. It shows the distributions over probabilities and then I've color coded it per class. 

That means that if you have a threshold of 0.5, then we're going to wrongly classify the blue ones that are above the threshold, and we're also wrongly going to classify the yellow ones that are below the threshold. So if we do a slightly more pretty picture here, that's essentially what's shown here. So compared to the previous slide, and now you can imagine just have infinite many points. 

So this histogram becomes a proper distribution. So I have a, let's say, a score of 1 here and a score of 0 here. I put in a specific threshold here, similar to what I just did, namely corresponding to changing where this line is, and then basically I now need to count something. So I'm going to define a few more measures now. This one is called a true positive rate. 

You've seen that before. It's called the recall. It essentially takes into account these two measures, right? Exactly the recall. 

This one divided by the sum of these. Then we're going to define a new one called a false positive rate. So it means out of the ones, out of the actual negative ones, we're going to divide the number of false positives by all the amount of negative ones, right? The total number of negatives. 

So that gives you a false positive rate. So if you think about what will happen in terms of these two things, these two metrics, when we change the threshold? Well, essentially what we are doing here, if you think about it, if we start up here, where we put the threshold at one, then we are getting 0, 0 over here. Since all the measures, that means we have, if we start over here, then we draw this, then we get 0, 0 and whatever number we have in positives and in negatives. 

And we got what did we get over here? We got 0, 0 and in positives and in negatives. So if we start from this one here and try to actually compute these numbers, the recall we know that one is this one divided by these, the sum is going to be 0. 

What's the other one going to be? It's going to be the false positives divided by the sum of these two numbers. It's also going to be 0. So if I put the threshold way up here, everything's going to be classified as negative, I'm going to get a point down here. Conversely, if I put my threshold here and classify everything on the right as positive, I'm going to get this point up here. 

This one up here, typically indicated as 1, 1, 0, 0. Now, what you can see here, or maybe at least imagine, is that if I change the threshold here, this one in the middle, well, let me just go back a second. Obviously, it's stupid to do this in a real classification system, but it's a way of characterizing what's going on here. So if we put the threshold here, we get a certain number of false negatives and a certain number of false positives and of course true positives, true negatives. But you can imagine in an interactive sort of graph here, maybe we should do one of these, but if you actually move this threshold, you're going to change the numbers in the confusion matrix, i.e. also the true positive rate and false positive rate. 

And you're going to get a curve like this. This is called the receiver operating characteristic invented by the British during Second World War to avoid having to deploy the Royal Navy or Royal Air Force every time there was a bleep on the radar. So it was invented for radar use. But the thing here is that by moving the threshold, we can characterize what will the classifier do at certain different thresholds. It'll allow you to actually compute this graph, this curve. And if we actually do the area under the curve, that's called, yeah, well, the area under the curve. 

If you integrate this curve here, you get a certain area under it. And then the question is, so this is one way of characterizing what's going on. Then the question is, what would a good curve and a bad curve look like? So as it turns out, if you think about it a little bit, if you compute these curves for different classifiers, in this case, three different ones, and you, and you look at what a random classifier would do, it would be a straight line here. And the area under this curve would be 0.5. 

If you do a perfect classifier, the green one, so Fp and Fn are zero. You're going to get, for all, basically, for all thresholds, then you're going to get this curve here. So it's going to go through 0, 0, 1.1. 

They're all going to do that. But the perfect one is also going to go through 0, 1. Now, realistic classifiers are going to be in between somewhere. And the measure for a good measure that's in part invariant to the class distribution, because we're changing the threshold here, it's going to be the area under these curves, as I've mentioned. 

Okay. So the area is going to be between zero and zero and one, but a good classifier that's not random, so to speak, would have an area under the curve. Oops. Greater than 0.0. So between 0.0 and 1. All right. Final thing, and I'll give you, I don't think you should do this, because this is tedious and takes some time. 

But I'll go through, I'll give you a few minutes just to read this. And then I'll upload a fairly detailed solution, which I think you should check, because doing this manually, instead of listening to me repeating the numbers, is not the way to actually get your intuition about this. So just read through this problem for a few minutes, and then I'll show you the solution. So you have to put a threshold somewhere. You can put a threshold many places. That'll give you the false positive rate, the one on the x-axis, and the true positive rate. So if you change the threshold, you're going to get multiple points here. On this line, from that, you should be able to detect which one is the relevant one. The threshold here is not the probability, but it's still a score. So you can still put a threshold and say everything above or below. 

It's going to be either a positive or negative. If you can do this without writing on a piece of paper, I'd be very impressed. But anyway. Okay, I had to do it on a piece of paper. So I just did it for one threshold, but I cheated. 

So I've written it up here. Now I'm not saying that there aren't all the strategies for getting the right answer to this question, but the safe way is to do what I've sort of indicated down here, that for all the threshold you can think of, you actually write down the confusion matrix. You can do that quickly once you get the hang of it, but you need to do it manually. And then you basically need to fill out these numbers from what you have up here. So let me zoom out a little bit and then I'll do it for the suggested threshold here. Ah, come on. 

Give me a sec. Good. So maybe take a step back here and say, we need to put in a threshold. Okay, we've suggested 4.5. You need one more to actually solve the problem, but let's do 4.5. So everything above 4.5 is classified as positive. Everything below is classified as negative. And then we have this convention here, which is suggested in the text down here. Okay, so if I'm to fill out the confusion matrix now, well, what do I do? 

Naively, I'm very careful when doing this. So I look at the true positives. These are the ones that are actually positive and classified as positive. 

So if I look up here, well, the ones that are actually positive and the ones that are classified as positive, it's going to be three. And I'm always double checking that I'm not saying something is wrong here. Good. So I get three here. 

Then I have some other ones that are positive, but they're classified as negative. Right. I have 12 of those. Yes. 

12 of those. Then I need to focus on the true negatives first here. So the ones that are actually classified as negatives and are, they are actually negative and actually classified as negative. That means these guys here, so that'll be 15. That's this number here. And then what remains, well, we have 17 in all, or we can just look at this number here. 

These are the false positives because they are actually negative, but classified as positive. Okay, so I have two here. Now I need to put, plug it into my true positive metric up here and false positive here. And maybe, I mean, we can try to do that. Let's see. 

So let's do the false positive first. That's what you have on the X-axis. It's this one here. So we look at the false positive. That's this number down here. So remember, false positive rate, we need to look at this column here. 

Oh, sorry, this row here. So that would be two over the sum of these two over 17. And then the true positive rate would be whatever is in the top row, three over 15. Okay, so we divide, this one's easy. One fifth divided by three. So 0.2 or something like that. And then this one over here, I don't know, divide by two, then we get one over 80. 

So I don't know, 0.12, something like that. And now I've saved you some time, but I really encourage you to actually do this. I'll save you some time. 

And it takes too long to do here in the lecture. But what we now get, we get four, we've tried out four different thresholds. We get four different confusion matrices. We get four different sets of false positive rate and true positive rate. We can now go check which one of these options does actually go through these points. 

As it turns out, that's going to be this one down here. So I need something that goes through 0.12, 0.2. That's roughly there. Then I need something goes through 0.24 here and 0.9 roughly. 

Yeah, that happens to be the case as well. 0, 0 and 1.1 are given. So this turns out to be the only one out of these curves that actually goes through these guys. That is how you, in a safe way, solve this problem. 

There are other ways. You can sort of say it's a sensible classifier, so it's not this one. It's probably not this one. It's better, so on and so forth. But the safe way here is actually write up this stuff here. And you can do that relatively quickly once you try it. All right, go through this because it's... For the exam, these are some of the questions where there's not... It's relatively mechanistic, what you have to do. Right, just a few more words. 

It's not going to be very long. Covered sample methods as a major algorithm for combining weak classifiers. Sort of in contrast to some of the more complex ones that we've seen, where we actually sort of increase complexity and try to control it. Here we take some simple classifiers, combine them in either naively in a bag and shake it about and do the majority voting. Or we do the boosting where we actually focus on the points that we misclassify from round to round. And then the evaluation part here where we arrived at the AUC score and the ROC. So those are important concepts, so make sure you get that. So see you next time for the unsupervised part and then go do the exercises. 
All 

Speaker 1: right. Let's get started so we can get out and enjoy the weather for once. So today we're going to sort of shift topic a little bit from the supervised machine learning to unsupervised machine learning. And specifically we're going to talk about K-means clustering and hierarchical clustering. As usual, feedback groups and read the book. Today I'm probably not going to include as many sort of mathematical details as we've done in the past. 

So make sure you actually read the book and sort of look at the details. So it's going to be many more pictures today than usual. So as I said, we're in the unsupervised block. So there's going to be three sort of, let's say, topics here, weeks on that. And then of course the final week where we discussed the exam and just summarized the course. So today, yeah, as usual, some learning objectives related to specific algorithms here. Some of you would probably have heard about K-means before and we're going to then sort of go a step further and talk about hierarchical clustering and how we can use that maybe a sort of slight more general approach. 

Okay, then sort of in the spirit of what we've been doing so far, namely a lot of evaluation of these methods on various data and so on, we're going to talk about ways we can actually compare different clustering, so different partitioning of our dataset, how to do that in a sensible way and how we can perhaps somehow use class labels, at least inform whether our clustering solution or grouping solution is correct. So I've heard a few people mumble about this in the lead up to the lecture here. So just remember the project deadline is approaching and I hope you have all started working on that. 

Otherwise, it's I wouldn't say it's too late because it's doable, but really make sure you submit this on time. Also, do remember that we're not looking for you to find the best model to get the overall globally best performance on this stuff. The important stuff is that you know what's going on, not whether you get plus minus 1% better results or not. 

So you have to show, of course, that you understand that whatever learning methods we're doing needs to be better than the baseline and so on, but it's not so important that you get, obviously, it would be difficult on real data to get 100% accuracy, for example. So just be aware that we're not looking for you to spend infinite time trying to optimize your algorithms or your variation of your algorithm here. So use common sense here. 

We're not expecting you to run, let's say, very deep neural network models for 10 days on a GPU to pass this report or even get a very good score on this report. So use common sense here. Good. So we've talked about supervised learning. We've talked about unsupervised learning a little bit in the very beginning. 

And of course, in the supervised learning part that we spent five or six weeks or something on this up to now, well, what we had? Well, we have, let's say, a measurement of the world with some values collected in a vector here, typically. And for each of these x vectors, we also had a label, for example, or a measurement. And that was sort of the setting that we wanted to map from x to y using some function. And we saw last time, I summarized it last time, we've seen a bunch of different functions. 

Right, we can use trees and we can use neural networks, linear models, and even k-nearest neighbor methods. Okay. So what happens if I take away this label now, the y thing, the label that actually tells you what you want to be, so to speak, or what is the ground truth for that particular observation? Can we do something sensible here? And why would this even be useful? 

Okay. So this is sort of the regime we're in today. And you can sort of think about it in the same way that we've measured something, in this case, two things about the world. And you can think maybe about this flower example that we've used a bunch of times, in that you can measure the, you can bring a ruler, even though you're not a botanist, you can bring a ruler out into the world. You can measure the geometry of a leaf on this particular flower. And you can put that into your favorite vector space here. 

Maybe the only type we've talked about, maybe a real one. Okay. So you can do that and you can plot all the data points. And then, well, as a human, you can eyeball, okay, maybe in this case, we see two groups, naturally. So we can eyeball this and say, okay, well, these guys are closer to each other than there are to anything over here. So they must be sort of special in some sense. 

They must be closer to each other than there are to anything else. So maybe that's a group. Maybe that's a particularly interesting collection of points that are, or sub, let's say, a subcollection of your points that are interesting somehow. 

And maybe then you can call your favorite botanist friend and have him afterwards tell you, well, actually this group of flowers is versicolor or whatever it is. Okay. But anyway, the thing we're going to talk about today, a method for actually finding these groups based on some notion of similarity. 

And we're going to sort of build that up slowly. So imagine, I guess, 20, 25 years ago, when you were a baby or something like that, there was sort of a blank sheet and you hadn't seen, you haven't experienced the world. And now you're actually exposed to something, namely animals in this case. So someone shows you a cat, and here in this case, an image of a cat. And you can now imagine in your brain, that sort of this abstract vector space. 

Obviously, probably not really, right? Not in the sense that we talk about vector spaces. But you can sort of imagine that when you think about a cat or see a cat, particularly, it gives a certain response. And we can think about that in this abstract vector space. 

You can think about 2D space. Okay, what happens if you see another cat? Well, maybe the pointy ears and sort of maybe the style of eyes and whatnot and the size of the thing seem sort of roughly similar. So you wouldn't say that they're totally different. 

So maybe if we place it in a vector space, it would be something like this. What happens if you continue? Well, in this case, we see a slightly different cat, perhaps, or whatever it is. 

Something slightly different, but it's still relatively close to the other three things you've seen here. Obviously, you don't know it's called a cat, right? The word CAT. Okay, what happens next? The cat's worst enemy comes along. 

Now what happens? Is that sort of close to what you've seen so far as a baby? In the sense that, well, I don't know, if you look at the size, it's probably bigger, on average, at least. Still has four legs and a tail that's somehow slightly more pointy in this case and ears and not a little bit flappy here and not so pointy. 

But still, I mean, we can put it in the vector space and then end it up here. So it's certainly further away from the three cats. So the distance here from the dog to the cat is, let's say, higher than the distance between any of the cats here, among all the cats. Okay, what happens if we bring in more dogs? Well, they tend to sort of group over here. At least we can sort of imagine that. 

And then, well, okay, what can we use that for? Well, anyway, the baby has sort of grouped things into something that's grouped and we can recognize things that are more similar to each other than they are. Sort of, let's say, we can make an argument saying that the baby can sort of figure out that a cat is closer to these two here than there are to any of the dogs. 

So that's sort of a natural ability for humans to be, let's say, it's very easy for us to group things, at least in the visual domain. So we can continue this and now we have four dogs and now something else comes along. And I guess we can only hope it's not a real one, but I guess the baby here wouldn't know if it's real or not, if it's a picture anyway. So where would this thing here in the middle here, this dinosaur? 

Where would that end up in our sort of abstract vector space? Well, in terms of size, it's definitely bigger than everything else we've seen so far. So it's different in terms of that aspect. 

It might also be different in the sense that, I don't know, so the way it stands and the pose of the thing is different and I don't know, it still has sort of four legs or arms or whatever we call it, but they're definitely different from what we've seen so far. So it ends up over here. So what has happened here is that, again, in this sort of very abstract and of course made up vector space here, humans are very good at actually grouping things together. 

And how do we actually make a computer do that? Because what we've done here is sort of eyeball things and say, well, the cats are more similar to each other than they are to any of the other animals here. That's one thing. Of course, also in the visual domain, we are very good at sort of detecting this outlier thing here, this dinosaur, that that's really different from everything else we've seen and maybe that's something you should run away from if you see one. 

I would guess, but anyway. So how do we make a computer do these things? And why would we want to do it in the first place? And so, of course, just to remind you, well, in the super setting, we had this X to Y mapping. We learn a function for doing that. In the unsupervised learning, we don't have the luxury of having this label. At least that's the game we play, even though we may want to, of course, in a course like this, we know the ground truth, but we want to play the game. 

We don't have that label. How do we then do something useful? And something useful here might actually finding, that's our canonical example here, is actually finding these groups that are sort of hidden there. In this, you can think about a high dimensional vector space where you can't eyeball things and you can't imagine easily how the structure looked like, but we would want algorithms to actually find these groups for us in that high dimensional vector space. So that could be regularities, of course, in terms of some of the aspects that you go out and measure is that something that always will, you know, where the certain periodicity goes up and down and stuff like that. And all of these things, we've also talked about that before a little bit, namely, we'd call that exploratory analysis. And if you think back even to what we did with PCA, PCA would also be called an, let's say, exploratory technique for doing things. Of course, you can also do it and use it for other things like a pre-processing step in a supervised pipeline. But again, as is PCA is also an exploratory, let's say, method for finding structure and finding interesting relationships in your data. 

Similarly, what we're going to focus on today, namely the grouping aspect here, clustering, what we're going to do is we're going to, like the baby, take all of our objects, divide them into non-overlapping groups. That means you can't be both a cat and dog at the same time. We'll see that next week, that actually next week, our computer can sort of say that you are cat or dog with a certain probability. 

But today, just assume you're either cat or dog, for example. So they're non-overlapping. So of course, what we want to do here is group things based on some notion of similarity. 

And this is the really hard part in this unsupervised learning business compared to the supervised one. And why might that be? Because you define, first you choose the similarity, let's say function, but you also put in the features and determine what should come out, and that determines somehow what should come out. Because in the end, you're going to get a result. You're going to get a clustering of some sort, whether or not it's meaningful and useful. 

That depends very much on the purpose of your analysis. So you can imagine, if I were to, let's say, at the beginning of the semester, I want to group every one of you into a group based on your educational background. Okay, maybe to assign you to certain rooms or whatnot. Maybe it doesn't make a lot of sense to actually take into account your shoe size and your height or something for doing that. But if I did it, I would get a clustering based on that. 

But of course, in that sense, it wouldn't be useful. And the dangerous part about this unsupervised business is that you need to be really, really careful about the interpretation. And there's sort of a notion of reading tea leaves here in that with unsupervised learning, you sort of get what you want out of it if you put in certain things. 

So just be careful when we're interpreting this. And we're going to come back to ways of actually comparing different, let's say, ground truth of clustering with the ones we obtain using our algorithms. Okay, so just be mindful of, well, it actually needs to, the algorithm needs to give you sort of meaningful groupings based on what you put in, but also why you're actually doing this grouping in the first place and are the features you're putting into your algorithm actually sensible in what you want to do. Okay, there's also some, let's say, notion of what we call unsupervised classification. And that sounds a little bit odd, but it sort of comes back to the example I gave with the Irish flowers in that even though, as you, as a non-expert, you collect some data and you try to group them, if it actually turns out that they group into, let's say, I don't know, two groups like we had before, something like this. 

And what you measured are these, let's say, geometry or width or something of the leaf and an Irish flower and maybe the height of that leaf as well. And you see two groups, but you have no idea what this means. You could now actually call in some expert and ask this person to tell me, what is this particular instance? 

What can you say about this? Maybe he'll tell you, well, it's actually called a versicolor, like the species of the flower. And then maybe a natural next step would be to then actually classify the rest of them, because I'm using K and N kind of idea here, to classify everything in that cluster as versicolor, a particular class. So there's this notion of that we can actually do the unsupervised grouping and then afterwards we can somehow say something more broadly or do a post hoc classification of what's actually in that group. 

Okay. So as I said, you need to be a little bit careful about when doing these unsupervised things and how to interpret the results. So in this case, how many groups do we have? So here in the auditorium, who thinks that one group or is one group, two groups, three groups, four groups, I can continue. I can't tell. 

I'm just saying that this is some of the hot part, right? I mean, need to somehow figure out how many groups there are. And if you actually think about it, this could be one solution. So I could also argue that actually splitting this one into two might also be a sensible solution. 

But at the end of the day, that all comes down to what are you actually going to use this for and what are you interested in. Just be mindful here. So we can continue essentially all the way down to one group per data point, if you want. We're going to follow that idea a little bit going forward. 

Okay. So we maybe to, I wouldn't say to circumvent this, but to somehow give some insight into how many groups we need and what are the nature of these groupings that we have obtained. We're going to look at two different types of algorithms here. One that's going to give us a partitional, let's say, clustering or grouping in the sense that you have distinct non-overlapping groups and you as a data observation, you belong to one of these groups. 

Okay. We're going to look at k-means that could do something for us that's similar to this. Then we're also going to look at a slightly more general idea, namely a hierarchical clustering. So if you just, I think the easiest way to think about this, if you start out with every single data point being in its own cluster, now you start merging things. 

So if you take a look at these two points here, they've been merged. So that means that one of these data points is its own cluster, but it's also part of this larger group. If you continue this game, it's then going to become part of these larger and larger groups or let's say a sequence of nested partitions, if you will. And in the end, you're going to have one group that sort of contains all the data points. And if you think about the structure for representing that, that would be a tree. So we're going to look at that and we call it a dendrogram specifically. So we'll look at these two types of things, or different, let's say ideas for finding groups. 

So one way of sort of illustrating why these hierarchical clustering might be a good idea is sort of looking at this three of life thing. So all life that has been sequenced, I think until 2017 or 2012 or something, it's sort of in this diagram somehow. So humans, I think we are up here somewhere. And you can sort of see, based on the genetic profile or sequencing of the genome here, you can sort of see what happened in terms of evolution of different species and you have bacteria and all of them, everything else. So you can imagine taking a DNA sequence or whatever it is, and then your group things based on their genetic, let's say fingerprints somehow. And then you end up with some kind of a tree structure here that sort of represents, let's say, the evolution of life. So this is just one motivation for thinking about why you may want to actually use this hierarchical clustering idea. 

OK, before we get to some specific methods, we're just going to talk a little bit about types of clustering. So there's four, five, maybe something like that. So the first one we're going to talk about is well separated. So that's where each point here is closer to all the other points in this group than there are to any of the points over here. So if you think about what would a method for finding that be, it would actually probably result in, or should ideally result in something like this. So it means that they're really well separated, right? 

There's no way that you have one data point that's closer to anything over here. So we call that well separated. Then we have center based and maybe I need to draw a little bit here. 

Let me find something sensible here. So in this case, each point in a cluster is closer to the center of that cluster than to the center of any other cluster. So if I think about the center here, if I were to eyeball this, I would say, OK, maybe ideally we would have two clusters like this. And if I then consider the center of these two, that would be this. 

This. So that means that this point here is closer to that center here than it is to that center over here. If you cluster based on that principle, we get this idea here. So that's a center based approach. Then we have a contiguity based clustering. This is where each point is closer to, let's say at least one point. 

In its own cluster compared to any other cluster. So in this case, what do we get? We would get something like this. And so sometimes we call this roughly the half moon data set. And if you sort of think about what's going on here, it means that this point here is very close to this one next to it. So they're clustered together and it sort of changed together all the data points that sort of lying on some, we would call a manifold, but generally lying on this sort of structure here and here. So everything that's sort of close together gets grouped together, so to speak. Okay, we call that contiguity based clustering. 

Then there's a type of clustering that we're going to look at next time in particular. This is where two clusters here, they are separated by, they're very dense in the sense that are many points are in this area in the cluster and they're close together. But they're actually separated by these areas of low density. So you have these green points and they're very low density, not a lot of points there. So it's the density that defines whether or not you're now moving into another cluster here. So if you think about that in terms of distributions and make a cross here, you'll see you have low density, low density, high density going down and up again and so on and so forth. And we're going to look at Gaussian mixture models next week for Farxley doing this. Then there's also something else that we call conceptual clusters. And we're not going to give you methods for actually finding these. But if you were, now I jumped a slide here, but if you were to actually look at this thing and look at what groupings do we see here. I think just based on your priors and what you've seen about data before and maybe wheels in some sense, circles, you'll probably say that this would be one cluster and all of these points lying here should be clustered together. 

So should this one. So in this sense, we're looking for a concept, namely something that's a donut shape and they might actually overlap as well. So that's rather difficult to actually have methods to do this. But there are template based methods that can do this, but we're not going to look into those in this course. OK, so a small task for you. 

Figure out what type of clustering this is. And then I would say that there are, if you argue really, really hard or persistent, then there are probably two correct answers here. But anyway, at least try to find the two that's incorrect. I'll give you five minutes or something for this. OK, I think that was four minutes or something. Let's move on because this is not the hardest question in this lecture. 

So let's perhaps move on. So the correct answer here that we would expect in an exam is the contiguity based one. So B would be the correct one. I would say it's a little bit of an ask the question because how about we try to think about the center based one now. So remember, that's where the point in the cluster is closer to its own center than to the center of another cluster. So of course, it's easy up here to see that all of the black points are closer to the center here. 

And this one over here is also closer. All the red points are closer to this center than to any of the other centers. But if I take the center of the nose here, it's going to be here or something roughly, maybe. Maybe here or something. 

And then if I take the center of the blue stuff, that's tricky. It's going to be probably here-ish. Now you actually have to determine if these points here are the closer to this one or this one. And as it turns out, if you actually use a ruler, then this distance here is shorter than this one. 

So it can't be center based. Now that's the well separated one. That's where the points in the cluster are closer to all the other points in that cluster than to any point in the other cluster. 

So this point here is definitely closer to this red point than it is to this one over here. So we could also rule that one out. So it's not well separated. 

It's not a center based clustering. So what about conceptual? And that's the one I would say, if you argue really hard, then you can probably convince me that's also true. If you knew upfront that you're going to look for a face in your data, then that might make sense. I would say given that we're not particularly worried about that in this course, it's probably not the answer you want to give. But I wouldn't penalize you if you argued and you gave that answer. 

All right, does this make sense to everyone? So be careful. And of course, these are questions like this are going to be slightly tricky because they're so easy if we make them obvious. So that's probably going to be one point like this where you actually do have to check it somehow. Anyway, moving on, how can we actually find clusters like these? Potentially, what kind of algorithms do we know? We don't know anything yet, but we know how to measure distance. And that's exactly what we're going to exploit now. And this is probably the simplest algorithm in the course. 

Maybe other from KK and classification, but that's all of similar. So I'll outline a very simple algorithm here. So we need to select how many clusters, how many centroids do you want? Not how many clusters sort of in the ground truth sense, but how many centroids are you sort of going to start out with? 

And then we are going to repeat something. We are going to form K clusters. So we call all the points associated with a centroid a cluster. And then we're going to assign all the points to the closest centroid. 

So now we need to measure distance somehow, and that's the note down here. And then we're going to recompute after that, we're going to recompute the centroids of each cluster. That means basically what are the coordinates of the centroid? 

And we're going to do that until the centroids don't change. That's it. Remember this one. You need to remember it by heart. 

Okay. So this algorithm here has a very long story, at least 60 years or something like that. Yeah, around that. And the basic setup here is that we generally, when we talk about K-means, we talk about the squared Euclidean distance here. Now, there are other similar measures you can use here. Just need to be a little bit careful and I'll come back to that later. But for now, assume you're using the Euclidean distance. Okay. So I'll outline the D steps and that's probably the easiest way to actually look at this. So remember, we have to initialize two centroids. I'm going to call them U1 and U2. 

And they're going to be in this vector space in this case for this exam. All right. So there's one here. 

That's one there. Now I'm going to run my algorithm here. I'm going to form K clusters by assigning each point to its closest centroid. That means I need to now measure the distance from all the blue points to this centroid. So that's going to be a lot of distance measurements. I'm going to measure the distance from all the blue points to the other centroid here. So again, equally many distance measurements. Okay. 

Once I've done that, I can say for each of the blue points, which of the two centroids is the closest one, I'm going to basically jump to that one and belong to that. Well, not jump to that one. Sorry. But I'm going to assign myself to that centroid. Okay. And then afterwards, I'm going to recompute the centroid. Sorry. 

The coordinates of the centroid. Okay. Let's just see what happens in this example. So in this example, I have a centroid there, one there. Which of the blue points is closer to the black one? Maybe something like this. These blue points are definitely closer to this black one than they are to the white or non-filled one here. This one is a little bit tricky. So you probably need to use a ruler, but maybe it's something like this. 

So let's see what happens. Well, as it turned out, so now I've color coded the points. The blue points are the ones that belong to this solid one. The red points or magenta points here are the ones that belong to this one here. So those are roughly what our intuition would tell us here using the Euclidean distance. 

What do we need to do now? Now we need to recompute the centroids or the location of the centroids. So that means taking the mean. If you're using the square to Euclidean distance here, then we take the mean of all the blue points. And that means, yes, you end up here in the middle of that cloud here. Okay, what about the red points? Maybe what's the mean of the red points? It's probably somewhere here or something. 

Let's see if that's true. I'll slightly off, but at least the blue points, sort of the centroid for the blue points ended up here. The centroid or the other centroid ended up here. Okay, so now we sort of at this step here. Now we need to repeat. So what do we do next? 

We actually assign, reassign all the points to the closest centroid. So what happened here? Well, this centroid here jumped from down here to up here. So maybe we should just reevaluate what happened in the sense that we need to reassign the points. So all these points now are actually closer to this point here than there are to this one, I think. 

So we need to just measure these distances, of course. And then we reassign now. Okay, so now we reassigned all the blue points to this centroid here, the filled one, signed all the red ones to this centroid up here. 

Okay, what do we need to do now? We need to recompute the centroids. We do that and then we end up in the middle of these two, let's say groups that we can sort of eyeball. And that is our solution because if you now reassign things, it's going to be reassigned to the same one and the mean is not going to change. 

The centroid is not going to change. I hope that makes sense. That's a very simple intuitive algorithm for finding groups. And it works in many cases. There are also caveats, right? 

I'll come back to some of these. For now, you need to do this manually. So just relatively quickly try to go through this. I'll stop you sort of when I hear the voices go down. 

So assume sort of four or five minutes or something like that. So go ahead. I should just say, right? I mean, the way this is written is that we've got 1D observations. So they're on a line. So a good way to write this would be to actually do something like this. All right. 

Let's move on. I think this is hopefully relatively easy for you to do. Maybe not in the time I've given you here, but at least conceptually doing the correct answers. So I'll quickly go over it and it's not mainly to do the computations. It's mainly actually to introduce something we're going to use later. 

So we have our problem up here. It's 1D. So in that case, it's always easy or relevant to actually do the computations. You put it on a 1D line kind of thing so you can actually get an overview. 

So what I would do here is simply just compute the distance between the points. That's what we need, right? Oh, what's that? And then between this one, it's 6 and then 12. Okay. 

So the first thing you need to do is figure out where our centroids are. So we have one at mu2 was at 12. Is that correct? 

Yes. And mu1 at 17. There was one there. That was a really bad mu. 

And it was 1. It's going well. Okay. So these are sort of our initial clusters here. And we are sorry, from this, we need to form our clusters. 

So one way, a notation I'm going to use now for indicating the clusters is going to be sort of a small vector here where for each of the data points I have, I'm going to put in the index or the name of the cluster, so to speak. So that I'm assigning that point to. And this is going to be helpful later. 

So just think about why I would maybe want to do this. So for point one, that's 42. What's the closest centroid? It's going to be mu1. Right. It can't be mu2 because there's always five more if you're going that way. That means that all these points, the four points here are going to be assigned to mu1 to that cluster. The point 12 is going to be assigned to mu2. So I'm going to indicate for the first, for the four points there that they belong to one. I'm going to just put in a one here. And then for the 12, it belongs to two. 

So now I have sort of a concise way of indicating which cluster do I belong to. That's basically it. How would you do it on computer? This is probably how you would do it. All right. Then now I've signed all my points. So now I need to update my centroids. And I see that mu, that starts with mu2. So now I have four points that I need to take a mean of all those values. So that's 17 plus 42 plus 48 plus 60 divided by four. And that is, I need to take my notes. 

41 points, 75 or something like that. Okay. Then for the first cluster here, I only have one point. So basically it's going to be that point centered on that point. So it's going to be, sorry, yes, it's going to be 12 divided by one. 

That's 12. What do I need to do? I need to now repeat the stuff. And what happens here is that this is step two. I'm going to just insist on doing this. And what we see is that now maybe I should actually update my locations of the mu here. So we can actually see that. 

Can I choose another color? Maybe no, it's not going to respond. Maybe so I had a point at so mu2 is still, oh, sorry, did I mess this up? Yes. 

So I think I missed this up. So this would be mu1 and mu2, right? Yeah. Good. 

So mu2 is going to be at 12 and mu1 is going to be a 41 point, something here. Great. So now I'm going to reassign my points here based on this. And now this point that was previously, this point here, it was previously in mu1 now belongs to this cluster here. 

That's the only change that's happened. So now we just need to go into this list here and figure out which points needs to be one and which needs to be two. And that turns out to be 42 that still belongs to one. 60 still belongs to one. 17 belongs to two now. 

That was the one change we had. 42 belongs to one. 12 belongs to two. Now I need to update again and again. And it turns out that my mu1 is now the average of 12 plus 17, which is 14.5. Mu, sorry, mu2. I did it again. Mu2 and mu1 is going to be the mean of the rest, 42 plus 48 plus 60 divided by 3. 

That's 50. Okay. If you continue this game, there's not going to be a change in the allocation from this. 

And that means we need to stop now. And now we can see that 48, 40, sorry, 42, 40, 60 and the same cluster, 12 and 17 are in the same cluster. That's the solution here. I hope that makes sense. Good. Otherwise, go through it. 

Check. All right. Is this then the, are we done now? 

Should we go home? And in some sense, well, K-means is a very popular algorithm, right? But it definitely has issues. 

So there are different things you need to worry about here. First, you need to actually figure out how do you measure similarity? And when K-means is presented generally, and when it was actually invented, they use the Euclidean distance. And in that sense, that's one way of measuring distance. And when you do that, the way we compute the centroids afterwards is to use the mean to actually update the centroids. Now there are other ways and other metrics. You could use the distance function. 

You can use the L1 or you can even use cosine. If you do that, then the mean is not necessarily the right way to actually, let's say, update the centroids afterwards. Just be aware of that. Then there's a slightly more pressing issue, namely how to actually decide on the number of clusters. And I'm just going to put something out here that says the number of clusters is somehow our complexity controlling parameter for K-means. 

So what happens generally when we look at complexity controlling algorithms in supervised learning when the complexity goes up? We start out, if you look at the test error, so this would be the test error. And for K-means, that would be the squared error to the centroids. Okay, what happens in general, if you look, I'm just going to plot it in here. In general, for any sort of supervised algorithm, we would typically, in the standard regime, you would see something like this. Unfortunately, that's not the case for K-means. It's always going to go down. Oops, not going to be zero, but it's always going to go down. 

So we have an issue here. We can't actually use this to select the complexity or model. There are ad hoc methods and heuristics for doing it, namely where, I don't know, whether there's a kink here or something like that. And that's also an okay heuristic to use, but there are better ways of actually doing that, which we'll see next week. This is sometimes called the elbow method in for K-means. Then there's another issue, namely how do you actually initialize these centroids? And I'm just going to show you some handcrafted problems here. 

So if you look at the problem here, and you initialize based on what you see over here, so you have three clusters, if you just run through K-means, what will actually happen now? Well, it turns out you can try to do it in your head. These guys up here, so let me plot this. 

These things up here will be clustered, and this one will, these red points will belong to this cluster. And what about this poor guy here? He's going to be empty. So there are not going to be any points associated with that one centroid. So is that an issue? 

Well, I don't know. It's at least the property of the algorithm, so you need some way to actually fix that. It's okay if it's empty, so to speak. You can just leave it, really. But now you've wasted one good centroid here. So there is an element of if you had actually initialized differently. 

So if you initialize like down here, then you have initialized here, here and here. What would happen? You would probably get what you, oops, you would get what you, that was a fancy error, okay. You would get basically what you expect that the algorithm will group into the three clusters. Yes, so the question is, don't we always pick points to initialize our centroids? No, there are many ways, and that's my point here. There are tons of ways of initializing K-means to get a good, let's say, solution. 

So one way to actually do it is that you, first of all, you select a subset of your points as the initial centroid, that's your suggestion. That's not guaranteed to actually give, always do something sensible, because you could, advertedly, choose two down here and one up here. So you could risk the same. 

So what we tend to do is do something called father's first. That means you start out initializing one, this one down here, you now pick the point that's furthest away, maybe this one over here. So you actually select this one as your second centroid, and then you pick the point here that's then furthest away from the other two. Okay, that's one way of getting around this, because now you're sort of spreading, or you're spreading your centroids across the whole dataset. 

So just be aware of this. And the reason why I want you to be aware of this, and that's the key problem here, is that K means that solution depends on your initial, let's say, values of the centroid. So that's not basically, you're not guaranteed to find the globally optimal solution here. So you need to be a little bit careful. That means if you randomly selecting points and stuff like this, the solution might actually change. Just be aware of that when you're running K-means. All right. 

Is there a way to get around this or not get around it somehow? Yes, there is, but I think let's do a break. And then I'll talk about hierarchical clustering after the break. So let's be back at two o'clock. Have a seat, guys. All right. 

Just while you get seated. So a few of you talked about the time it takes to actually run these neural networks for the project. It's okay if it takes multiple hours that you use a, let's say, less than 10-fold in our end-or-outer loops. Just make sure you somehow argue and explain why you've done that. So the TA or whoever's marking your report knows that, okay, I've actually thought about this, and maybe also the consequences of doing that. Okay. So you can use, what is that, a prisoner? Is this the word? 

So you can use K equals three or five or whatever it is if you are in a pinch, basically. Okay. Moving on to hierarchical clustering here, and that's another approach. 

Some prefers this one. So let's just go over this relatively quickly. So remembering K-means, we had to compute the distance between all the points and all the centroids. So there's a lot of distance computation. This is going to be even worse, because what we're starting out with here is actually to initialize a proximity or distance matrix between all the data points. So we're going to basically have to define a distance measure, and here it's less risky to use a different distance measure than the Euclidean one. 

You can use whatever you want, essentially. It doesn't even have to be a proper metric. So anyway, let's just assume we play around with a straight line, the Euclidean distance here. What you have to do is compute the distance between all your data points. So you get an in-by-in matrix. 

Essentially, you don't need both ways, so to speak, because it's symmetric, so you can sort of halve that roughly. Okay. But we have that now. Then you're going to actually start from the individual data points and merge them. And then we are going to essentially implicitly or explicitly update this proximity matrix. So let me show you how important until only one cluster remains. 

Let's see how that works. It's much easier to show. So we're starting out where we have all the individual data points in their own cluster. 

You can think about it like that. Now we are going to, as it says, merge the two closest clusters. So I need a way to measure distance between clusters, not only individual data points. 

So we'll come back to that. But anyway, it's easy in this case for the first step, because the distance between the clusters is naturally simply the distance between the individual data points. Okay. Moving on, now I have to actually merge the two closest clusters. 

So in this case, I think it's probably this one down here. And these two points are really close to each other. So we merge these two clusters. Remember, it's not just the data points I'm merging, because I've sort of made the notion that these two data points are actually in the cluster. So what do we get? Well, we get a new cluster here that now contains two data points. 

And then we remember that the individual points were actually also in their own cluster. Okay. What happens next? Well, now we need to compute the distance between everything we have here, all the clusters we have in this update. So I need to compute the distance between this data point in this cluster, this data point, this cluster. This data point in this cluster. 

And that brings us back to this. I need to be able to compute the distance between a data point and or a cluster and another cluster. And what I'm going to do here simply use the shortest distance. 

So if I compute the distance between this data point and this cluster, just take the shortest distance here between the data points involved. That's one way of doing it. It's called simple linkage. But anyway, if we now consider here, what's the shortest distance we have now between all the available clusters? 

I think it's probably this one up here, so we have to merge those. And again, now we update the proximity matrix, but that's essentially easy. That's a small step. Okay, what happens next? Well, maybe these two gets merged or something like that. And now I can't remember what happens. 

Maybe if we just look at the distance between the clusters, maybe this one up here gets merged or something. Yeah, you'll be good. And we continue this game. So we start merging clusters based on the distance between the clusters. And in the end, we're going to end up with a big cluster that contains all the data parts. 

So what do we have now? This is an iterative scheme, right? So what we actually get is a this partition or the sequence of nested partitions. So we started out from one view of the data that one view of the data where each data point was in the own cluster. Now we merged one or two of these clusters so on, so on, so on, and so forth. 

So if we do that, I'll just show you again. You can think about this as a tree where at the bottom here, we have all the individual data points in their own cluster. So if I essentially cut that tree down here, I get that all data points are in their own cluster. And if you think about what we have here, then we have the first thing that happened was that these two data points down here got merged. And if I actually look at the distance between those two data points that are merged and then put a horizontal line where that happened, that happened here where this is called it 01 and 02 01 and 02. 

Then the height here indicates at which level that means the distance between the two clusters did we merge those two? Okay. What happened next? I can't remember. Was it this one up here? So we can already see that it's probably this one here. 

So these two points correspond to these two and they merged at a height here, I don't know, three, whatever it is. Okay. And then we continue. So at some point, we merged these two points here. There will be these two with this point over here and got this cluster here. So that's represented here where we take the individual points. 

Let me put some names on it. So let's call it nine, 10, 11 or whatever it was. So we had 10, 11 here and then 0.9 was here. And then we merged the cluster containing 10 and 11 with the single point or the cluster with the single point in it, 9. 

And that happened at a certain distance. Okay. So this tree here is one way of actually representing this merging behavior. And if you cut the tree at a certain height, let's say here, then you actually end up with a specific clustering, namely, for example, if you cut it here, that means all the points below here, they are now considered to be in the same cluster. Oh, sorry, here. They're considered to be in the same cluster. 

It must be this one down here. If I cut it again, the same height, cut it here. All the points, that means just one point is now considered to be in its own cluster. It would be this point here. And similarly over here, these points here, 9, 10, 11, they're going to be in this cluster here. 

I cut it here. These three points, they are the points that are here and so on. And in the end, if I cut it up here, and maybe we should actually have sort of a thing here, and then if I cut it up here, I'm going to get a cluster that contains all the data points. So this is a very convenient way of representing all the possible clustering you can get where you have the ability or possibility of actually cutting the tree at certain, let's say, heights to get a different, let's say, behavior or level of the clustering, so to speak, number of clustering essentially. Number of clustering, sorry. Okay. I hope this makes sense. 

So try to actually think about what can be used this for. So two important things to keep in mind, right? We started out with the individual points. We merged them together. We got clusters. We then merged either individual points or clusters of points together with each other until we have a big cluster containing everything. The height indicated here indicates the distance between the clusters at which point where we merged them. 

Now, as we've sort of seen in K-Means, the key thing here is that we are able to actually measure distance, and we need to be able to measure distance between, for example, these two clusters here. And what I just did in the example was I said, okay, how do we measure distance between two points and another set of three points? Well, I can take the shortest distance between the, I mean, there are many possible distances I could compute here, right? I could compute all of these distances. 

This is going to get messy, right? But I basically need to compute all of these distances. So I can either take, I don't know, the minimum one. That would be the one I sort of used in the previous example where I take the minimum distance, which would be this one. I could, in principle, also take the maximum. 

I could also take the average distance between these two sets of points. Okay? So depending on what I do here, when I actually merge things, I'm going to get different behavior. 

And they have fancy names. So the one where I take the minimum distance is going to call the single-linguage maximum, oh, sorry, yeah. Well, the minimum distance here or the minimum linkage or single linkage, we also call it that sometimes. Then we have the maximum or complete linkage. 

So this is going to cover everything here. And then we have the average, which was basically also what I alluded to on the previous slide, where we take the average of all the distances. There's one more, which we're not going to spend a lot of time on it, but it's called Warts method. It's essentially merging clusters where we reduce the squared error. 

So let's imagine we have the two examples on the left here. So we have two clusters that we are now merging and we can compute the squared error. That means the error from the mean of those points to the points. That's a variance measure somehow. Then when we merge it, we can consider, let's say, the squared error, the variance of the merged clusters. And we want to pick the possible merger that gives us the least increase in squared error. So that's just another way of measuring distance. So we have four methods of measuring distance between clusters. And just before we get to actually show you the sort of properties that arises when we use those, you're going to have to actually just relatively quickly work out which of these answers is correct. So we're looking at the same data points, the 1D data points as before. Now we want you to actually build the dendrogram here. 

So I forgot to say this is called a dendrogram, this tree structure here. So try to do that. And I'll give you only two minutes for this and then I'll quickly go over the solution. So have a look at this. 

Okay. So hopefully you somehow managed to at least think about how to solve this. So a very easy way here to discard a few of them would be to actually look at the distance between the points. But just to make a point, I'm just going to draw the dendrogram from scratch. So the way, and the correct answer is D, by the way. So I'm going to do that by just drawing the things on a straight line again. Oh, well, roughly straight line. And what was it? 42, 48, and 60. 

This is going well. And 60 roughly here. So we had a distance of five here. We had distance of 25 here. 

We had a distance of six and we had a distance of 12 here. So I'm using single linkage because that tends to be the easier one to do manually. There's no guarantee that it won't show, let's say, a maximum one in the exam. But at least when I have to explain it, I prefer single linkage because it's easier. So we need to merge the two points. Here we consider the points to be in a cluster that are closest to each other. So the first two we need to compute here or merge, sorry, are the ones that have a distance of five between them and we know that they need to merge that five. 

So 12 and 17 are going to get merged at the height of five. Then we have a distance, the next one, we're going to have a distance. So now we actually need to compute the distance between this cluster containing 12 and 17 and everything else. We're using single linkage so the distance would be 25 between this cluster and 42. 

Everything else up here is going to have a higher distance. Now if we use maximum linkage, we need to do it between 12 and 42, right? So it would be 30, but we're using single linkage, so there's a distance here of 25. That's definitely higher than six and definitely higher than 12. Six is higher than 12, so we need to merge here now. So 42 and 48 are going to get merged at six roughly. 

That was roughly six. And then what happens? Then we need to merge, then we need to consider again the distances between clusters. So we have a cluster containing 42 and 48. Distance up to the cluster containing 60 is going to be 12. And this distance is still 25, so we are now merging this cluster containing 42 and 48 with the one containing 60. 

And we're doing that at a height of 12. Okay, now one thing remains. We have to basically consider merging this cluster and this cluster, and there's only one sort of free distance left, and that will happen at 25 somewhere way up here, right? So this is going to be 25. 

And now we're going to have a cluster that contains all the data points. The only one that actually agrees with that is D. So this is easy with single linkage. It becomes a little bit more tricky with maximum linkage and average linkage. So see if you can find a few exam problems where we actually use maximum linkage or something like that just to practice. Alright, then let's try to actually look at a few examples where we use different linkage functions. 

I'll just highlight a few things relatively quickly. So here you're giving two dendrograms. Oh, sorry, I should point this way. So here you're giving two clustering solutions basically. 

One up here, one down here. They can either, one was obtained using minimum linkage, the other using group average linkage. So the question to you is now which one is which. So there are a few ways of working this out. You can think about what would on average be the relationship between using minimum linkage and group average just on the scale of the distances you'll obtain. 

This is sort of a small hack. Because I'm giving you the scale over here, it actually pretty easy because the minimum is always well on average is going to be smaller than the average. So the one bit smaller, let's say scale here is going to be the minimum one and the other one is going to be the average one. So that was easy right now we're done, but there must be more to this because there are other properties here. 

There are more interesting properties. So don't cheat here and use the scale to work this out, even though I just gave you the hint. But one way of doing this is that, well, we've seen that we have the dendrograms here. So they're caught at a certain height and that means we get two clusters, right? 

These are the ones shown here, similar down here. So what's the difference between these two at this point where we cut them? And if you look at this, you're going to see that, well, this stuff up here is basically the same in both cases. 

Oops, this banana thing here. And if you just look at this stuff here, that's actually the same, right? So the difference is going to be this blob here in the middle. And one way you can check which of the two linkage functions you're talking about is whether or not it's actually consistent with that linkage function. So if we actually thought about, if we don't know what this thing here should be, red or blue, we can actually have a look at what would happen if you use the minimum and the group average linkage. So if I actually look at the, I would need to probably consider this distance. So that's the distance between this group of points and the other group over there. 

And I would need to consider the distance between this blob and this blob and between this blob and this blob. And then where would that be? Maybe this distance here. 

And this one would be here, something like that. So if you actually check these distances, you'll see that this one down here is consistent with the minimum linkage. It sort of resembles a contiguity-based one where we're sort of creating a chain between things, right? Whereas the group average one tends to sort of do K-means-like kind of things where you get these sort of spherical blobs arising. 

Whereas the minimum tends to make sure that, or if of course the data is sort of consistent with that, tends to create these chains or chains through the data. Okay, so maybe another one here just to highlight what you may want to focus on. So I should say that I think, again, you're given two clustering solutions. So I think this one up here is all right. 

You're given one here. The problem here is I think there's something wrong with the coloring. Yeah, there's. So this stuff yellow here should actually be the red stuff here. And this red should actually be, oh, sorry. This red stuff here should be yellow. 

And this yellow stuff here should be red. Yes, and it's correct in the book, I believe, as far as I remember. But anyway, let's have a look at this again. And again, now you can use this trick that I just gave you that every distance is using a maximum tends to be greater than measuring stuff using a minimum linkage. And so now we can always guess that this is the minimum one. But there's it also highlights sort of an issue with this minimum linkage thing here in that we actually do have this blob down here. 

Right. So if they're like there is in this case, if there's a very short distance between these two, what we would sort of consider the correct clustering. There's a very small distance here, then the minimum one tends to actually connect these two blobs before connecting these ones over here. So it is a little bit sensitive to these outliers here. And again, it's about actually considering sort of relevant distances here. 

Okay, maybe one more point here. That is that the maximum one, since it sort of considers the maximum distance or the distance from here to here, for example, and from here to, or sorry, from here to here. Then it tends to create these roundish, let's say compact clusters. And that's just something to keep in mind going on to this example here, because here you sort of see an issue with the maximum one. 

And here you have the, let's see, we have the minimum one down here. It's still consistent with this idea of connecting things that are sort of closely together. But the maximum one and here it's actually well-severed in some sense, not formally, but there is a gap here, right? And then the maximum tends to do something we don't expect. But this is this property of actually finding these roundish clusters of roughly the same size. So just be aware of these different sort of properties of using these linkage functions. Okay, how do we actually get an intuition about this? 

I think the best way is actually to run some code and try to try it out on some different data to get some intuition about what happens here. All right. Good. 

How are we doing on time? Great. Then let's move on to something slightly different. And that's sort of the most technical part of this lecture. And that is how do we actually compare? Let's imagine I give you these two solutions here. 

How similar are they? We want some way of actually quantifying that, because you can imagine that somehow you magically obtained the ground truth for whatever algorithm you're trying to test now or problem that you have. Now you want to sort of apply whatever fancy new clustering algorithm you have in mind, and you want to see does it actually give you the right or the correct result. We want some way of quantifying this. 

So let's build this up. So in this case, we have nine data points and we have two different types of or not types, but we could be different types. We have two different clustering solutions. And this sort of already brings a few questions to mind, namely that in this case, we seem to use three different clusters. 

So you can imagine k means with k equals to three. Okay. So we obtained these ones here. So I'm actually just going to call these cluster one and cluster two and cluster three. And then I can do, if we focus on set here, then I can do what I did for the k-means one where I'm going to do the same thing. 

I actually create a vector. So for each observation, I'm going to put in what was actually the cluster name that I assigned to this data point. So we see that the first data point got into what I call cluster one. So I give it a one and so on for two, three, four, they're all ones. Plus the two, there were two of those. So I call those cluster two and put them into a vector to the same for cluster three here. Okay. 

So let's maybe make a point here. I could have called them whatever I wanted and in what order I wanted, I would call them red, blue, green. So this idea that we can now simply compare the absolute, let's say value or name I gave to them with another cluster is problematic. 

Okay. So let's see why that is maybe that's on top of the fact that we're using different number of cluster in each solution. So in this solution over here, I sort of got, let's see if I can remember here. So here we've got, let's say four different clusters. There was a certain algorithm that returned four different clusters. Okay. So if I actually do the same as before, so I'm just going to label that three and four here, the yellow one, they're going to be in cluster one. 

These are the cluster two, cluster three and cluster four. I do exactly the same. I put these into a vector here. 

And then now I have the issue. I want to compare these two vectors, so to speak. I want to see how similar are they. But I've already alluded to the fact we are using different number of clusters in each solution. And by the way, the naming is arbitrary, meaning that I could have called this cluster 40 and cluster 10 and whatnot. Right. So I can't compare basically one with one here and sort of count how many times did they occur. 

So I need some other way of doing this. And this is pretty neat. And it brings us back to something we've talked about before. So bear with me for, for, for, for a few slides here. 

Okay. So what we're going to do is not sort of compare them one to one and in an absolute sense, we're going to compare them in a pairwise sense. So that means we're going to see if pairs of observation like one and two. Are they clustered together in solution set? 

Yes or no. Are they come clustered together? That means they're in the same cluster. 

It doesn't matter what we call it. It's just other in the same cluster. In set. And we can ask the same question for, for Q. Or then different clusters. So let's take one and three, for example, over here, we can put sort of a zero there in different clusters. 

Okay. In this solution for set over here, sorry, here, then the same cluster. So we put a one over here. 

They're actually in different clusters. So we put a zero. So that means we're considering things in a pairwise sense. So we're going to put up a little bit of notation here, namely sort of this delta function that we've seen before. 

And it's doing exactly what we've done before in that if we're going to put a condition here. So it's going to be zero if observation I, for example, two and observation one are in different clusters. That gives us a zero. If observation, let's say two and nine are, sorry, let's do the same one. 

Sorry. It's going to give us a zero if the one and two are in different clusters. It's going to give us a one if they're in the same cluster. So for the SIT solution here, it's going to be a one, but that specific pairwise comparison. So how many pairwise comparisons do I have if I have nine different observations and I don't care about if you're comparing one way or the other. And I don't care about comparing one to one. You're going to have exactly this number of comparisons that you need to do. 

So you can try to write that up. But if I just start from the beginning, I need to compare one with two, one with three, one with four, one with five. You can see this going on one with three, sorry, two with three and two with four and so on and so on and so forth. This is going to become really boring in the end. Anyway, we can do this. So this is exactly what I've done down here. Basically, for the cluster and solution set, I've considered observation one and two. Are they in the same? 

Yes, they are. So you get a one. One and three in the same. Yes, you get a one. One and five. 

Come remember. No, they're not in the same. So you get a zero. So this vector here containing zeros and ones, it's basically going to indicate on a pair-size level, is this particular pair in the same cluster or not for that particular solution? Funnily enough, I can get one for the queue solution as well. 

So now I have two really, really long binary vectors. And we want to, and this is invariant to the, let's say, how many clusters you have and how many, let's say how you named the clusters, right? I haven't used that at all. It's just whether or not two observations are in the same cluster or not. So I haven't used the absolute naming and how many clusters I have. At the end of the day, we've got two binary vectors now. 

It's really, really long, right? So it's basically sort of in the order of in squared, which is really annoying if you have a million data points. You now have, I don't know, something you can't store in your computer, basically. But at the end of the day, we have a long vector. Do you know how to compare binary vectors? Now the answer should be yes. 

Now, okay. You remember your card distances? You remember what was called a simple matching coefficients? We can even use cosine if you wanted to. So go back to, now we switch the order of the lectures in the beginning. 

So I can't even remember myself, but I think it was probably lecture two where we talked about distances. But at the end of the day, we have two vectors that are invariant to what we are interested in them being invariant to. So we can compare them. And essentially that's what's written here, in that we do know two methods for comparing what's the difference between two binary vectors. 

All right. We have the random index and that fun enough or not fun, but that's also called a simple matching coefficient. It's just that in the context of this sort of topic, it's called the random index. And the way we, if you remember correctly from that, let's say that lecture, we looked at the number of pairs where they were in the same cluster in both set and queue. So that means these two, this particular pair is in the same cluster in both set, let's say one and two and in queue. So we're counting one now. Then we're also looking at the number of pairs that are different clusters in set and queue. 

So would, let's say, let me just pick one here. So one and three, if we compare that particular entry into the vector, then the same cluster and set, but they're not in the same cluster in queue. So that would not count towards this being, let's say, different in both of these solutions. So I think in that particular lecture, we also called S, I think F11, like feature 11, where both of these entries into the vector where they're one. And we called, I think D, we call it zero zero. That's where the entry into these two vectors are zero in both cases. Okay, so we're not really interested in the case, I just outlined one and three, like this one here, where it's in the same, same, let's say, this pair is in the same cluster in set, but not in queue. 

We're not interested in those. So we can compute this simple matching coefficient that's going to give us a number. If it's zero, they're not very similar. If it's one, basically, then it's very similar. It's identical, the clustering solution, based on this way of computing. 

We can also use the card similarity and the advantage of doing that is that it's not really that sensitive to D. So you can imagine what would happen if I keep three clusters here for set, but I increase the number of clusters here to, yeah, well, nine. What would happen? It would simply be that there'll be a lot more Ds simply because I'm using more clusters. So the card is generally slightly less sensitive to D. But anyway, we can do this. We can get a number. We can now compare whether new clustering, two clustering solutions are similar or not. Great. 

Now, this is really, really boring to actually compare all of these pairs. So there is an efficient encoding for actually doing this, and that's going to help you, I think, at least in practice. That's the way we would do it. So we're going to basically compute a count matrix here, not compute. Well, yeah, compute count matrix here. And the easiest thing is to do it by example. 

So I'll do that. I think we have Q up here and we have set here and we call Q one, two, three. Oh, great. Four and one, two, three. So what we're going to do here is fill out this matrix in. It's going to say how many times did we actually see that we were, let's say, in cluster one, whatever we call cluster one in the solution Q. And in whatever we call cluster one in set. So if we take a few of these, let's do one and four. 

This seems like an interesting number. So if we just look at the two solutions we have over here, we're going to look at cluster one in set. That's all of these four guys here, these four guys. And then we need to be in cluster four in Q. So that will be this. And we counted two because that happened two times. 

So it's been just counting how many times were in a particular cluster in Q and also being in another particular cluster in set. Okay. This is relatively simple to do based on two vectors like this. This is also why I wrote it off for the K-means. Good. Then a few more statistics here. Not statistics, but things you could compute. And that is that the, how many times basically in set where you're in one, two or three, how many times in Q where you're in one, two, three or four. So just summing one way or summing the other way or manually counting basically. Again, a sanity check you could do. 

Good. I hope this makes sense. Why is this an advantage compared to what I've just did? As I said, this is going to be a really, really, really long vector. 

And we have to actually compare them sort of here to check whether or not they were the same or different. Right? So as it turns out, if you go to a combinatorial class or look in a book, they will tell you that from this matrix, we can actually compute this. That means the number of pairs that are in the same cluster in both of the two solutions. We can compare that, compare that relatively easily like this. Let's just maybe look at some way of doing this without actually using a computer or whatever. So you can see here, you sort of have in, it's going, everything in here is going to be zero or higher. It's going to be positive. So we just look at the elements here. You're going to see that that's in minus one. So that means the element here, we're going to sum over all the elements in this. 

That's what's going on here. And then we're going to consider the element minus one times the value here. So if you take two, we're going to get two times one divided by two. That's going to give you one. If I have one, I'm going to get zero because one minus zero tends to be zero. So you basically only have to consider the ones that are two or higher. 

So that's essentially what's going on here. Okay. So just this looks a little bit nasty, but it's actually relatively easy to compute even in your head. 

Good. This one, the number of pairs that are different in cluster solution set and queue, that's a little bit more nasty. So here you need to sort of compute all the possible pairs. That means the length basically leads to binary vectors that we sort of start out with. 

And then you now need to sum over the elements in, you need to, this is the sum here, you need to sum over the elements in set here. Okay. But it's the same principle here. So we can ignore everything that's less than, less than two. And then you need to subtract also summing and doing this thing over this particular vector. And then by the way, we need to add this. So that's a little bit nasty to do in your head, but luckily we are actually going to do that now. And finally, for once, we actually do have time to do this. So I'm actually going to give you 10 minutes to do this because it's useful. And by the way, you, I mean, you can take a break or whatever you want, but I'm just going to put in a little bit of naming convention to not confuse everyone. So what you're giving up here is a table. And normally you would look at these values, these distances between the observations. We don't need that here. So it's probably from some compound exam question or whatever it is. 

So I'm going to call. So basically the clusterings are given by these classes of the data points up here. So I'm going to give this a name of C1, C2 and C3. And then just to help you, I'm going to give you, I'm going to look at the dendrogram down here. That's going to give you the other solution. 

So I think let me just see if I call that set or whatever I did. Just give me a second. So did, or is that actually here? It's already here. So we don't need to, we wish to compare the quality of this clustering. 

Compare the quality of this clustering given by the dendrogram. So this down here is Q. This up here is set. Okay. So we have different clustering solutions set in Q. 

And now just the naming for the Q1. We are cutting the dendrogram here. That means everything sort of below this point here is following the tree down to the individual data points. 

There should be in one cluster. So let me just draw that. Something like this. I'm going to call this cluster 11. This is to make a point that the naming is arbitrary. I'm going to call this clustering C22. I'm going to call this particular cluster C33. 

That means when you construct the vector, you can put in 33 if you want to. That's what I'm going to do. So maybe give this a go. 

And then, yeah, come back at 10 to 3. And then we'll go over the solution plus a little bit more. Okay. Okay. 

Should we try to maybe finish what I've sort of started here? So the difficult part of this assignment here is not sort of, you know, the technicality in actually doing it, I think. It's actually maybe actually the first part of actually understanding what you need to do here to create these two vectors. So I created the two vectors we talked about, namely set and Q. And it's essentially just saying for observation one in set, it's in this cluster up here. So one, one, two, two, two, three, three, three, three, five times three, basically. 

Three repeated five times. Okay. So for the other one, and the reason why I used these 11, 22, 33 was to make very explicit that the naming is arbitrary again. So I could have chosen, yeah, green or blue and red if I wanted to. Just to make sure I'm not confusing myself. And I recommend you do the same because if you actually do the one encoded this one by one as well, you're somehow more likely to make a mistake. I think at least for me. Okay. 

So I have the two vectors here and I hope you agree with those. Any mistakes detected somewhere? No. Good. Then I assume it's correct. It's at least correct with what I did this morning. So that's a hope you get the right answer here. 

Now, after doing this, I then use these, this efficient encoding service. So we actually have to construct that matrix there. All it's saying is that in this element that I left out now, I need to count how many times were, yeah, am I in C three in set and C 233 in Q. 

And that tends to be okay. I'm, I'm in three here in set. I'm in 33 here in Q. So that's two times. So I put into, and that's essentially what I did for all the other ones. Oops. As well and saved a little bit of time by doing it up front. Okay. 

Now I also need the end set here and those are the number of times I'm in C one or across the one in set. And I just have to some here. Oh, come on. Just have to some this way. And then I get two, three and five. And you can of course do a sanity check. Do I actually have two observations in cluster one and set three in two and five in three? 

Yes, you can do the same down here. But let's just sum up. So we get six, one and three. And I guess now there's sort of tedious part and I'm not going to go through it in detail. 

I think you can, or maybe can let me just, ah, maybe not in, in, in so much detail, but we know that is. That's the number. Well, that's what we need to compute to the cart. That's the number of times the pair is in the same cluster and both set in Q. We know that that's this sum here. Do we use K and what do we use? 

Yeah. So we use K and M and then we need to multiply K M by N K M minus one divided by two. And I think that's relatively easy. If you use the trigger, I sort of tried to explain on the last side that we only need to consider the ones where we are two or higher. If we are to each element in this is going to be a contribution of one. And since we have four twos and nothing higher, then it's going to be four. So here we're going to get four. And now the nasty one here. 

And let's see if I can remember this. Please correct me if I'm getting wrong. So first we need to take the number of elements in that very long binary one. So that's the number of pairs. That's N minus one divided by two, I think. And then we need to subtract this thing where we sum over set and afterwards subtract where we sum over Q with this thing here. So we need to take K N set the case element and N K set minus one divided by two and then minus M over Q or whatever it is. 

M in M minus one divided by this. And then we need to add S I think. Yes. Okay. Now I think you can do this on your own. So I'm not going to waste your time doing it, but I might just add this to the slides afterwards so you can actually see it. Okay. 

It's going to give you 17, at least if my notes are correct here. Yes. Does anyone disagree? Did anyone actually get this far? You got this far. Great. 

And it's 17. Okay. Now we're done basically, right? 

Okay. Because now you simply need to plug S and D into this relatively simple equation. So why have I spent so long time on doing this? Several reasons I think it's actually useful to think about what's going on here when you're comparing comparisons. Yeah, well, sorry, not comparing comparisons, comparing trust, trusting solutions. 

That's one thing. The other thing is also this was a lot of work without a computer in an exam. So you can expect that I'm not going to make a question like this where you have to go all through all of this manually. What we will do is probably create some exam question where you have to do some of it where we give you some of the intermediate results. So this is maybe also to reassure you a little bit because doing all of this stuff here was trivial before. 

You just had a script. You could input basically these two vectors, right? So now you are not with a computer in the exam and you can rely on us not being too nasty because it would take you 10, 15 minutes at least just to get started on this. 

So that's the reason why I spend a little bit more time on this than I would normally do. Is this other steps at least obvious or clear to people? Yeah. 

So on the previous slide or? So this one. Okay. So yeah, okay, let me just explain what I did again, right? 

So the first thing I did. Okay. Let me just get this roughly. So this part, I hope that this is clear. This is just an encoding of which observation belongs to this cluster. Now the question up here is this count matrix and there's nothing more than counting. 

Or let me pick one here. Let's say how many times are you in cluster two in set and in cluster C11 in queue? So I'm just counting how many times that occur. 

So what did I say? I say cluster two here in set that happens three times. Let me put something around this. So these three here, but I also need to be in C11 in queue. So that would be these two here. These two observation. Those are the two numbers or that's the two I'm putting in here. 

Does that make sense? Okay. Good. I hope that also makes sense for people online. Now. Now we are in about five minutes going to cover a topic that could take at least a full semester to explain. 

Namely information theory. We're going to do it in two minutes. So have to be sharp here. And I jumped a slide, I think. 

Yes. So we're not going to, we don't expect you to know everything about information theory here. We see this sort of as a tool, but let me just go through it in a little bit of detail. So we're now back to probabilities and probability. Mass distributions basically over discrete random variable. So this is, this will lead us to yet another way to compare comparisons from this probabilistic perspective. So you remember that we had a probability distribution over random variable X here. And you can imagine flipping a coin. 

So the random variable is it hits or tails basically, and there's, unless someone's cheating you, you'll probably expect that probability to be 50 50. Okay. Now there's a notion of information related to this, this outcome. So forget about the math here a little bit. But if I tell you that the probability of observing something is, let's say very, very close to one, let's say 0.99. And you actually observe, let's say, let's say that's the probability of hits or someone's cheating you. Would you be very surprised if it's come, it hits come up? Probably not right because you knew that the probability was very high of that happening. 

And that's to some extent talk to the information you gain from observing that outcome. You would be very surprised if tails come up, right? If you're the one who's actually cheating someone with a biased coin and you put a, you manufactured that coin to actually give you an outcome. Let's say 99 out of 100 with heads and now it actually comes up tails. 

You'll be a little bit surprised. And that's sort of essentially what we are measuring here in this notion of information here in that. So another way of writing this thing here is log 1 over P1. So you can imagine something that's 1 here. And if you measure information this way, if you have a probability of 1 or being hits, for example, then basically you get an information of 0. 

Because you knew it was going to be hits before you saw the outcome. But you are going to be infinitely surprised if it's tails because there's zero probability of that. So this is a notion of information about observing an outcome. And if you actually take the average of all possible outcomes here, that means overheads and tails in the coin example, then we got what's called the entropy. And that's to some extent characterizes the uncertainty about, or characterizing the uncertainty of the outcome, given that particular probability distribution. So you think about it as something that measures uncertainty about an outcome. 

So for the coin flip, the entropy is going to look something like this, where if the probability of hits is between 0 and 1, then the entropy is going to look something like this. That means the uncertainty of a particular coin is going to be greatest if it's sort of a fair coin. Otherwise it's biased and you have a little bit of information about what the outcome might be. And as I said, if the outcome of hits is 1, then basically there's no information gained from observing the outcome. 

Good, so that's what we call entropy here. Now you can also do this for joint probability distributions, like if you have a die and a coin at the same time. And let's say that this was the coin still and then you have a die throwing that as well. Then we have a joint probability distribution. 

We can look at what's actually the uncertainty related to that particular outcome of X being, I don't know, hits and the die being 2. Okay, now something a little bit tricky here, which we call the mutual information. So that relates to the entropy, that means the sort of uncertainty of one variable, the other one, and then the joint entropy here. So one way intuitively, I think about mutual information between two random variables is to say if you know the outcome of one, how much does it then reduce the uncertainty of the other? Now if you have, let's say, a totally independent coin and yes, also an independent die basically, so there's no, you wouldn't expect one to inform the other. So if you, at the same time, one, let's say, flips the coin and at the other time, someone over here throws the die, you would not expect knowing the outcome of the die would reduce the uncertainty of the coin. So in that case, the mutual information between those two things, those two random variables, is probably going to be zero. 

But on the other hand, if the two random variables were, let's say, the weather outside, or sorry, let's say the temperature outside and the month we're in, then there's probably going to be quite a lot, you can say, you can reduce the uncertainty about the temperature if you know what month you're in. So think about it this way, this intuitive sort of thinking here. We can normalize, so mutual information is not normalized basically, but we can do that so it becomes between zero and one. Or between, yeah, zero and one in this case. 

There are different ways of doing it, so it must be between zero and one in this case. So if there's zero mutual information between two things, like the die and the coin, then the mutual information will be zero here. And if you by observing, let's say, the coin know exactly what the die is going to be, then you would have a normalize mutual information of one. 

So it's a way of sort of quantifying the relationship between two random variables. You think, have we done that before? We did linear correlations, right? 

But this is more profound. So how can we use this in comparing clusters? So the way we're going to do it is basically sort of a construction here where we take our count matrix in, and then we're going to use that to define a probability that you are in, let's say, a cluster, let me find something interesting. You're in cluster three in set and also cluster three in queue. By looking at the counts we've observed and then normalizing it, we can get a probability distribution for that. 

So how probable is it that you are in a particular cluster i in set and cluster j in queue? Okay, that's one way of doing it. And then you can also, of course, do this sort of marginal. Let's say, what's the probability of being in cluster one, two or three in set and cluster one, two, three or four in queue? That's sort of an, well, it is an empirical estimate of what is this probability distribution. Now we can sort of crunch the numbers a little bit and say, okay, but we have two random variables, one describing the cluster you're in in set and one which one you're in in queue. 

How about we try to measure the mutual information between these two random variables? Okay, and that's essentially what we're going to do here. And that is essentially just plugging in the numbers, so to speak. And if we do that, it turns out we are going to get a measure now. And that's, of course, simply another measure of how similar are these two comparisons. 

So just think about this as yet another way of comparing clustering solutions. Okay, that was all I had today. So let me just summarize what's important maybe for your notes that you have to do. Remember, you need to have these two sheets of A4 notes back in front, so in total four pages. So it's a good idea to actually start writing those now if you haven't started. 

So you start thinking about what's important and not. So in K means definitely remember the algorithm might remember what's K, how we initialize it, what would be important here, what kind of distance function do we have, how do we compute the centroid so on. Hierarchical clustering and linkage function. The linkage function, I think the algorithm in itself is fairly simple. But of course you can, you need to understand a little bit about what the linkage function does and also what this dendrogram means and all of that kind of thing. So maybe think about what is the single linkage maximum average and award. You need to be able to apply that. 

And then of course the difficult part. Then we had these three measures of how similar to clustering solutions were. You need to be able to apply that. Good. That's all for me. So go do the exercises. This is really one time where doing the exercises help in the sense that you see the effect of these very visually right. 
Even better now. Okay. 

All right. So welcome back from the Easter break. I hope you enjoyed a few weeks, not a few weeks, just one week off. 

Certainly I did. But we are now back to intro to machine learning and data mining week 11. So we are almost at the end. 

So bear with us. Today we're going to talk about Mixed-in models and density estimation as a continuation of what we did the week before Easter, where we talked about clustering. So we talked about K-means, hierarchical clustering, and how to compare partitions or clustering. 

Okay. So as usual, the feedback groups for today. And then I think the officials of DTU feedback form is now available, I think. So do remember to input something there about suggestions for how to change things, how did you find the course, so on and so forth. Generally, be constructive. That helps us the most. 

We tend to ignore overly negative comments. But anyway, try to formulate suggestions for how we could improve the course. I can tell you that the course will, from next semester, be split into a master and bachelor course. 

So that means it will diverge somehow in the future compared to now. So it won't be this big mass of people from second to tenth semester. But anyway, you don't get to enjoy that. So anyway, input whatever suggestions you might have. 

Okay. So today you're supposed to read a few things, as usual. And just for context, we are now sort of in this unsupervised learning block where we don't rely on some labels only for post-hoc evaluations. But we can still learn something from the raw data itself, just the observations, the attributes, not necessarily the labels. And then next week, so we cover K-means regular clustering. We're going to call mixture models this week as a natural continuation. And then next week, we're going to look at what's called association mining, which is a little bit of an off topic, but that brings us back to probabilities again. So it's a good way of actually, let's say, reviewing that material. And then in a week after, I'll recap everything, and we can discuss the exam and so on. 

Good. So today, mixture models and density estimation. We'll build up what we call a Gaussian mixture model in a constructive way. So I'll show you how to parameterize a model called a Gaussian mixture model. Then afterwards, I'll outline how we can actually learn parameters in that using the so-called EM algorithm. And then we need, as usual, in all of our machine learning models, we need to choose complexity. So I'll show you how we can use the good old cross-validation to actually select the complexity of our mixture model, and specifically, how many mixture components do we need. And then we're going to, in the context of anomaly detection and outlier detection, we are going to present a slightly simpler version of the mixture model called a kernel density estimator. And we're going to present that, as I said, in the anomaly context, where we're also going to present some less, let's say, principled, or at least principled in the sense that they come from probability and statistics. But they might seem a little bit more natural than the density based ones. Anyway, we'll get to that, hopefully. 

Good. So something about a new model for modeling densities or clusterings that can be applied to clustering analysis. And then something about how can we apply that density model to, for outlier detection, essentially. So just before we get to that, we do expect that the feedback for project two will be available on Thursday, or Tuesday, so in a couple of days, assuming all the TAs, they get to act together and submit stuff on time. But everything's on track for now. And then there should have been a bullet point more here saying that next week we'll make the exam and solu... Well, maybe not the solutions, actually, but the exam from the last semester, named the 424 exam, we'll make that available for you, and you can use that sort of as a mock exam for what's coming up. It's designed in the same way that there are no aids, and just a basic calculator and basic dictionaries. 

And two sheets of A4 notes from the background. So I'll make an announcement on TGU Learn when that happens. All right, so you remember the babies from last week? They are surprisingly good, or maybe not surprisingly good, but they are quite good at grouping things together, especially in the visual domain. So we looked at this collection of cats, and they sort of ended up in our abstract vector space here in 2D, same for the dogs. And then we had this dinosaur thingy here, which obviously is not a dog or not a cat the other way around. 

But so it's sort of on its own here. So last time we focused in terms of actually being able to cluster things together somehow, and then look at the clusters afterwards. So we are also going to do that today in the context of the mixture model and see how we can use mixture models for cluster analysis. 

But we're also going to then do what I sort of alluded to, namely use these density models for actually, for our outline detection. And if you just take a step back a little bit and say, OK, in our vector space here, what are the points that are outliers? And the way we sort of constructed it here, it would make sense that stuff that's sort of far away from everything else, and it's sort of on their own, they must be strange, they must be abnormal in some sense, at least as per your definition of distance, right? 

So this one is far away from the cats, it's far away from the dogs, so it must be on its own. So a slightly more formal notion of this would be to also look at density. So we see that all the cats, they're sort of grouped down here and they're close together, so in that group of, let's say, cats, the density is high. We see a lot of points in this area, so that means everything that falls into that is perhaps something we would expect in the real world, again, if we went into the world, right? Because we've seen a lot of examples in this area before. So stuff that ends up there might, anyway, we cannot say immediately that's abnormal. Whereas stuff over here, like the dinosaur, there's only one point up here, and given that, let's say, we've seen only one of this in the world, then we would say, well, because there's only one point, then the density around this is really low. So just based on this notion of density, we can try to actually figure out what are the abnormal points and what are the outliers or abnormal points. 

So there's sort of two notions here, there's something about distance that we can use to figure out what's an outlier, what's abnormal, then there's something about distance, and of course they're connected somehow, but intuitively think about the distance and then we'll build up the density narrative in a bit. Good. So just taking a step back and maybe just using the example on the bottom left here to sort of think about what did we actually do last time. So we're going to just focus a little bit on k-means. 

So remember, what do we do? It's a really simple algorithm, we choose k, so how many components or clusters do we expect, and then you initialize the centroids of those clusters. So let's just imagine I do that, so I hear the centroids, and then I iterate this. And the way I start out is to simply start out assigning points to the closest centroid. So if I just sort of indicate that, informally here I would say these points probably belong to this centroid here, these points down here belong to this one, and so on. Based on this allocation, this first step, and this becomes really important in a few slides, 10 slides or something. And this first step, what I've done is I've assigned the points to a particular cluster or particular centroid. The next step is now updating the centroids, the center of the centroids, the coordinates. So what would that look like here? Well, let's try to do that just manually. 

Well, I would expect since the idea is that we take a mean of the points allocated to that centroid, we simply take a mean of those. So if you remember, I would probably end up somewhere here, and now actually we are done. So this is what k means is all about. It's a really simple algorithm here, but it does have some deficiencies as we saw last time. 

So one of the problems, if you look at the sort of list of issues here, is that it depends very much on the way we initialize the centroids. That's one thing. That's an issue. It tends to be an issue in a lot of machine learning algorithms, but it's generally something you need to worry about. 

So it doesn't always find the optimal solution. Does it explicitly take into account the size of the clusters? What I mean by this is basically the number of points that are in a given cluster. 

Not explicitly, right? It might end up as it turns out to be sort of roughly similar sized in the sense that the radius is the same in the clusters. And there's these similar sort of relatively compact clusters, but it doesn't explicitly model the size of the cluster, in terms of number of points, for example. So it definitely doesn't explicitly model the shape of the clusters. We tend to, with Euclidean distance, we get these roundish clusters. So that's one and other issue, right? 

But then, of course, there's this sort of, let's say, elephant in the room. How do we actually determine the number of clusters? And while I alluded to a few sort of heuristic large time that you could use this elbow method and stuff like that, there are no principled ways in terms of statistics and probability of X's selecting the number of clusters. We can't even use cross-validation in a sensible way here. 

Okay, so how do we go about this? So we would probably want something that, well, if we just look at the middle example down here, if I use k-means here, what would happen? Well, if I initialize just somewhat randomly, obviously, I've thought about this a bit, but if I initialize it here, what would happen when we iterate k-means? We would probably end up with a solution, something where we would have a center here and some points associated with that cluster and one here and one here. That's probably not what we want, right? 

And that's sort of an issue that comes into the problem one and then also the lack of ability to actually model the shape. Because what would we actually want here? What would we expect? Well, from eyeballing this, and obviously we can't eyeball stuff in high dimensions, right? But simply by eyeballing this, we see, okay, perhaps there's actually a need to be able to model a shape like this and maybe something like this and this. 

So it doesn't become this inherent sort of roundish shape. Okay, so that's something we would like to happen, right? Then if you look at the bottom right example here, what would we expect here? Well, again, from eyeballing things, we would expect to see something like this, perhaps. And this requires that we are actually able to model the shape of the clusters explicitly. Well, maybe you could do it implicitly as well using a special metric or distance function, but we're going to use a special construction for actually modeling these shapes. And also modeling or at least representing sort of how many points are actually associated with a given cluster. 

So how likely is it that you come from a given cluster? Which can be used for interpretability issues and so on. Oh, cases. Okay, so this is basically just what we, I would expect at least if I were to look at this, so eyeball this, roughly what I drew on the previous slide. So how do we actually construct something where we have all of these, where we can take all of these boxes? 

So we model the size, we model the shape, and we can actually determine the number of clusters in a sensible, principal way. Well, back to the good old normal distribution here. We're missing an i, it seems, but you remember the one, the normal distribution here. So what does it actually describe? Well, given that you know the mean and the variance, then you have this parametric form up here. 

But it's always good to think about what does it actually represent. So what we have here is the probability density function. So it's a function you can evaluate it for any x here, and then you get a value. Then there's a sort of special function, this probability density function, meaning that, well, it actually represents the relative, let's say, likelihood of occurring at a particular point here. So for example, if we actually drew examples from this probability density function or this distribution, then we would end up, namely, with a lot of points here in the middle where we have high relative density and very few points out here where we have relative low density. So if I just ask Python or Matlab or whatever you use, just give me samples from this distribution, what would happen? Well, I'll get a lot of points in here, because that's essentially what the probability density function tells me, the density of the points. 

And then I'll get a few out here, maybe, or something like that. Just keep this intuition in mind about what is it actually that this object represents. Of course, now we know that it needs to integrate to one and needs to be bigger than one and so on. So there's certain things related to this probability density function. So hopefully, you'll remember all of this either from statistics or from the couple of first weeks we looked at this in this course, at least. But the more interesting one, because we are not always in 1D as it turns out in machine learning, we tend to be in high D, high dimensions. OK, so what we actually need here is the multivariate normal distribution. And this is, again, a probability density function now able to actually put in a vector and get a density out. And it's parameterized by mean and a covariance function. And as you remember, we typically write it like this with this in simple here. So the important thing here is that, of course, it's now parameterized using matrices and vectors and whatnot. But the important things are still the mean and the sigma. 

So what do we have here? Well, maybe the obvious one is the mean vector here. So that's a vector, let's say it's in 2D, like the example below. 

It can be any dimensions. And here you would have a mu1 and mu2 basically saying, what is the center? Here we are in 2D, so it's called x and y. Think about x1 and x2 or whatever. But here you have x1. Let's actually call it x1, not to confuse myself. x1, x2, x2, x1 or whatever. 

Good. So now we have a vector, two coordinates, right? x1 and x2. And we can evaluate this function for any sort of combination of the two. And then on the z axis here, the vertical one, we actually get the probability density for that particular vector given that we know the mean and covariance. 

And of course, the mean, well, that's where we have the highest point of the probability density. So in this case, it seems to be 0, 0. And of course, what we can do is take the contour line. So we basically cut the function here. And then we plot that as contour lines over here where we have x1 and x2. And then we see maybe if I cut it here, I get this line here. 

OK. But what the mean is sort of easy to understand, right? It's sort of in the middle here. That's where the function has the highest numerical value. But what is the sigma thing? 

And hopefully you remember that. That is a matrix where we have some elements a, b, c, d, for example, where a is the variance along the z1 dimension or axis. In this case, d is the variance along the other x2 in this case. And then we have b and c. They're equal and they determine the covariance. So that means that basically the shape of things here is, if I have a positive covariance and I go one up here, then I'm also on average expected to actually go up on the other along x2 as well. And then the other way around had c and b been negative, it would have been sort of a downward shape. So hopefully you all remember this. 

Important things are the mean and the covariance matrix. Good. How can we then use that? So actually let me jump back a little bit. So if I actually draw, well, let's imagine that I actually have data points that are sort of lying like this in this two-dimensional space. 

OK. So in this case, this particular, let's say, parametric form or this multivariate 2D normal distribution, it's probably a good sort of density model for this data. But what happens if we actually look at the examples we've sort of encountered before on one of the first slides here? Would a single multivariate normal distribution be a good model, a density model for this? So what would that look like if I actually try to tweak the parameters to fit the data as well as possible? 

Well, I'll probably get a mean around here. And then if I look at the contour lines, I'll get something like this. OK. But remember what we just sort of noticed on the previous slide is that, well, the mean is here, and this is where the function has the highest density. But if you actually look at the data, there's absolutely no data in there, so it doesn't make a lot of sense. OK. So we probably need to do something else. And of course, that's why it says, mix your model up here. So if instead of actually looking at a single multivariate Gaussian here, and then maybe just focus on the groups here. So I have three groups, which seems to be OK. They're sort of well-separated and all of that. But if I look at one of them, maybe a multivariate Gaussian distribution would be a good model for that particular subgroup of data. So if I put in, let's say, a mean here and then maybe did this, this thing here would be a good model, but of course, only for this data over here. 

And the similar goes for the other ones. So what can I do now? Well, the obvious thing here is to say, well, let's then actually explicitly take three of these multivariate norm distributions, put them together. That means adding them together with a certain weight. That's one way to create a more advanced distribution that can actually model this data. 

So that is essentially what we are going to do. So remember, we are still looking for the overall goal here is the density. We haven't actually talked a lot about the, it's not sort of defined per, sort of in terms of the clustering abilities or whatnot. It is still the density that's the overall goal. 

So I want a good model of the density. And just a little bit of notation here. So we're going to introduce a variable called set. And that's a random variable. 

It's discrete. And it can be between 1 and k, the number of components you have in your mixture. So the number of these individual components I'm going to use to model this data, just for clarity, right? So set here could be, I don't know, one. So set here could be one up here, two down here, and three down here. 

Of course, I can choose more, but let's come back to that. So that's just a little bit of notation. And now we are simply going to formally write out this construction. I just very briefly explained, namely that we want the density. And we're going to write that as a sum of some base simple densities here. 

And you can think about this as a normal distribution. So what it's saying here is that basically you evaluate for given x, that's a given point up here. You evaluate this particular function. And that particular function is parameterized by which component you belong to. That means if I give you a given, let's say, component here, let's say, set equals three. Then I need to evaluate that particular, let's say, density here. 

Okay. Of course, to make the sum, I also need to evaluate the density at a given point for all the other ones. So in this case, I have, I need to consider three, let's say, terms here in that sum. So I have a distribution for each of these components. And of course, k here relates for the multivariate normal. 

It relates to the mean and the variance, of course. But we'll come to that. Now, of course, we can have a distribution for each of them. But we also talked about maybe, let's say, let's put a weight on it. That this allows us to explicitly model the number of points in the, let's say, in that component or in that distribution. So this thing here is a weight. And we sometimes actually write it v of k. And it is specifically a set of k, sorry, p of set equals k. So that means that each component here or each cluster, you can think about that, actually has a certain weight related to it. 

And then of course, you also know what is the base distribution here. But intuitively, all we've done is, in this case, let's imagine we take three k, big k here, capital K, we take three of those. So we put one on each, put a certain weight on it. Maybe intuitively, it would be how many points do we have in that particular component. And then we sum it all together. 

That's it. That's a mixture model. Of course, we need, well, sorry, there's one more thing we need to remember, that this is a proper distribution. So it needs to sum to one. That means all the weights need to sum to one. 

Good. So that's sort of a prototypical mixture model. And the reason why we call it prototypical here is that, well, essentially, you can put in any distribution you want here. So if you know any, let's say, continuous multivariate distributions here, then you can plug it in here and you can create a mixture model. So from a simple, let's say, distribution, we can add multiple ones together and get a mixture. Okay, that's a generic construction. Of course, in this course, the only, let's say, distribution for multivariate data that you've seen is the multivariate normal. 

So, hey, what about actually formalizing this? And this is where we end up with the Gaussian mixture model, Gaussian slash normal mixture model. So we always call it the Gaussian. So in this case, if you look at the expression here, if we actually plug in a normal distribution here, what we get here is this expression, what we have here. 

So again, what we have, the density, for any point in this space, is parameterized in the following way. So it's a sum of these individual components where the base distribution is multivariate normal. There's a weight associated with each of these components. And of course, each component has its own mean, its own covariance matrix. So a lot of parameters. 

And of course, then they need to sum to one, and it needs to be a proper random variable, essentially, the set one. So are there any questions about this? Or is this makes perfect sense? So this is the construction. This is the way we parameterize from a known simple distribution, the multivariate normal, we actually construct a more complicated one, one that can actually model this type of data. 

So you can imagine this one over here. If I am lucky and I correctly specify that I need two components, well, I could place a normal distribution that looks like this. I can place one that looks like this. 

I can put a weight on it, which is roughly 50-50. And now I actually have a distribution, which is this cross-like shape. And all I had to do was know a little bit about the multivariate normal, combine it with another multivariate normal, put a weight on it, that's it. So we've generalized what we know about multivariate normals to something complicated. But I do appreciate that this is a little bit abstract. 

So let's do a very concrete example here. So I've explicitly written out the sum for Gaussian mixture model here. So if we just focus on what we have here, so we have one component, another component, and another component. So we have three terms in the sum. 

So we have a weight here for this one, a weight for that one, and a weight for the third one. And then we have, of course, the base distribution here. These are normal distributions. 

And if I try to now actually just plot the contour lines for these individually, so I'm not going to actually apply the weight here, that's what I've done over here. So if we look at this normal distribution here, it's centered at 0, 0, and it has a positive covariance. So yeah, well, okay, it matches this one here, because it has a mean 0, 0, positive covariance, so cigar shape looking like this. What about the other one here? Then 2, 2, this one here, it has a negative covariance. Yeah, okay, so it matches this one. If I take this one, then I have, it's a little bit difficult here because scales are a little bit odd, but generally it's a positive covariance and it's centered at 4, 3, so it looks like this blob here. 

All right, so now I have the individual components, and now I actually apply the weights and sum them together and evaluate for all x in this space. Then I get this contour line. So what happened here? Well, we saw that this one had a weight of 0.5, so it's high compared to what we have here, because that weight was only 0.2, and then we have 0.3 for the last one here, and the reason why it's much higher is, of course, it's much more spiky, so the actual value is much higher. Okay, but anyway, that was the contour lines. What does that look like if you actually look at it sort of in 3D, where we have x2 and x1 here, and then you have the density p of x up here. Then you get these sort of bump functions that are centered, yeah, well, at the means here. 

Okay, so of course this is extremely flexible for placing these bumps, right? And what I've done here is actually give you these parameters, and in a few slides I'm going to tell you how we can learn them. But for now, I've just given you the parameters. 

All right, now it's your turn. So I'll give you a few minutes for this and try to see if you can identify from which of these DMMs does this data originate. So now you need to sort of imagine what I did before, that I sampled from a distribution. I sampled from a distribution, and now I need to not sample from a single multivariate normal distribution, but I've now sampled from this mixture here. So how do we do that? 

We first sample the component given by the probability, so let's say here 1 tenth would be from that component, 45 out of 100 would be from that component, and another 45% of the data point would be from that component. Okay, so give this a few minutes and see if you can identify the correct one. I'm muted. Okay, sorry, zoom people. 

All right, so... Where were I? Okay, so we're looking at C first, but before we do that, let's just try to maybe just take a broad look at this, because it seems like all the means here are the same for the different options. So that makes things a little bit easier, so you don't need to check many more things, I would say. 

But anyway, so we see that for option C. Well, we have a component, presumably, at 3.6, which one is that? That's this one up here, let's call that K equals 1. Remember, I'm assigning the label here based on the mean, so it makes it a little bit easier. And then I have one at minus 3.6, that would be this one here. So let's call that K equals to 2, or C equals to 2. And then we have one at 0, 0, that must be the last one, so that's K equals 3. Okay, so if you just look at it like this, then K, so we have a mean here, that seems to fit. 

Then we have a covariance matrix that says, okay, there must be very low variance along the first attribute here, or feature, and then very high variance along the second one. So what shape would that thing be? It'd be sort of a vertical cigar kind of shape, right? So if I just look at it like this, then it'd be something like this. What do we actually have? 

We have something that's the other way. So this is not the correct answer. Okay, as it turns out, it's the same option for D, so we can rule that out. Now we are left with two options here, and if we just sort of eyeball it, maybe... Let's see if it actually matches K1 here for the covariance matrix. So now we have high variance along X1, low variance along X2, which seems to fit. Okay, so this one seems reasonable. What about the other one? So here we've got very low variance along X1, and high variance along X2. So for component two here, that also matches. Does the covariance then match for this one? Yes, it does. 

There's negative covariance. Okay, however, it seems to be the same options we have available here. Okay, are we now lost? No, of course not, because we also have the weights that we can actually look at. And if we look at the two options, maybe just for K equals one, then we have a fairly low weight for this component here, relative to the other ones, of course. It's always relative to the others, because I can choose to draw 10,000 more points, right? 

So it's relative to each other. Good. What about this one down here? It has a relatively high weight compared to the other ones. So I would expect more points associated with this component than with this one. And that doesn't seem to be the case here, right? 

I have fewer points associated with K1 than any of the other ones. Therefore, the correct answer is A. Does this make sense? The best way to actually work this out, or not work this out, because you can do that, but get some intuition about what these KMM models are doing, then actually try to run the exercises and try to simulate some data from them. Then you see what's the influence of the weights, means, and covariances, right? 

But you can do that in Python, or MATLAB, or R, or whatever you're using. Good. Okay, so sanity check, and I guess that's a sanity check for you to make sure I'm still alive here, because it's sort of a small tedious exercise that just shows you how we can manipulate these things. 

So I've already sort of written that down here, and I should have done that, but generally what we do know is that for proper... Is it funny, or...? No? Okay. 

Good. For proper density, if we integrate this across X, and remember X is a vector here, then that must sum to 1. Also, P must be greater than 1, or 0, sorry, right? 

Okay, so is that actually true for this construction that we have here? So if I actually write this up, and I don't expect you to do that in an exam or anything, like actually being able to do this, but if we write this up, then we have an integral over our density, and our density is, of course, given by the mixture. So I'm going to write the sum K equals 1 to K, and then WK normal distribution X mu K, and sigma K. And I need to integrate over this vector thing here, multi-dimensional. So what does that do for us? Not a lot, I would say, but if we know that this thing behaves properly, then you know from calculus that we can switch the integral and sum sign, so let's try to do that. 

Just believe me that it does make sense to do that. In this case, then we have X K, K from 1 to K, and then we have the integral inside now, we have K normal distribution mu K sigma K dx still. Okay, then we realize that actually the integral here doesn't influence this part here, so we can move that outside. It's constant with respect to x here, so let's move that outside, so it just becomes part of the sum. Let's see what that gives us. Now we have an integral of the normal distribution over X still, and then sigma K here, dx. 

And this is sort of inside the sum sign. Then what do we know about this stuff here? We've sort of assumed and hoped and crossed our fingers that this is actually a proper density, right? 

So we know that this integral should be 1. Okay, so now we have a sum over all the weights, and by construction they should sum to 1, so luckily this sanity check here worked out, meaning that this is actually 1, so we're all happy here. So again, this is just a small, basically a small stupid exercise for me to do. 

Good, it just shows you that maybe there are ways of manipulating these things so we can, let's say, obtain some of the properties of the construction. If we look at this, I already said that I gave you all the parameters here, but we are in the business of doing machine learning, not just relying on parameters we're given. So how do we actually learn the parameters in our model? And you've seen that a couple of times in many different models, how do we update the parameters of a linear regression model, how do we update the weights of the logistic regression and the neural networks and so on. 

And I'm going to show you a new sort of family or type of algorithm that you don't need to be able to derive it anything, but I'll try to link it to the K-means algorithm we saw last week. Before we get to that, I just want to remind you about what we're actually doing here. So the goal here is to model the density, that's supposed to be an A, okay? So the goal of this whole exercise is to maximize the likelihood of our data given the parameters, and this is the whole idea of the maximum likelihood principle that we've seen many, many times, and it basically sort of underpins everything we're doing, at least almost everything. 

So the idea here is that we want to maximize the likelihood of our data given the parameters, and we write that up as, let's say, the product of all the individual terms here, where Xn is a data point, given the parameters. And of course I've made an assumption here that these things factorize once I condition on the parameters. Given I know the parameters, then I can do this. So the goal here is now to find these parameters theta. And of course in this particular model, those parameters are the weights, the means, and the covariance, this grants matrices for each of the components, 1 to K. All right, so for the logistic regression model, it would better be in the weights, same for the linear regression model, of course, and also for the ANN, similar principle, that this maximum likelihood principle goes again, or it appears many, many times, but of course here we're just interested in the density. 

We don't have this y thing that we're also sometimes interested in. So here we're just interested in maximizing the likelihood of observing our data given those parameters in the mixture. And you've seen various methods for doing that, like gradient methods, and in some cases we can even come up with an analytical expression, like for the linear and regularized linear regression model. So why not just do gradient descent, whatever. You can write this up and just plug it into a gradient descent where you try to walk down the mountain, so to speak. In this case, up the mountain, we're trying to maximize. 

So there is a particular, let's say, reason for that, and that is that we are in a slightly complicated domain where we have these latent, let's say, add variables, the set variables, so the allocation, let's say, determining which component each data point belongs to, and we don't actually observe this variable set. So it is generally, let's say, standard that we use this type of algorithm called the EM algorithm called expectation maximization, and I'll go through it and then just explain and try to link it back to the k-means one. So there are two steps here in this algorithm, and I'll try to do it by a small example as well. So let's imagine we actually have some data, x1 and x2, and I've got some data down here, I've got some data here, something up here, maybe. So I want to build a model for that, and that's p of x, something that models the density here. 

So what do we have to do? Well, we select an initial set of model parameters, and these are the weights, means, and co-variances. Let's imagine I do, let's say, I initialize using three components, so I sort of assume that there are three clusters, if you will. And if I just initialize it somehow, so I could take the mean of the first component here and do a naive initialization here, I could do one here, I could do one here, maybe looking like this. 

This is sort of my initial guess for what these parameters should be, and that's typically random or somehow cleverly initialized as it turns out. But anyway, I can do that, now we're here. Now I need to do two steps here, repeatedly. I need to take the expectation, and what we mean by this is that for each object, you need to basically figure out which component does it belong to. And if you remember the first step in k-means, this is exactly the first step in k-means, namely which of the clusters are you closest to, which of the centries are you closest to. And then we assign it in a hard way to that, meaning that basically, sorry, in k-means we do it in a hard way, meaning that you belong to a cluster, you do not. Here you actually belong in to the cluster in a soft way. That means you can belong a little bit to all the clusters. 

But we can compute that using Bayes' theorem. Is that a question? Components of clusters. All right, very good questions. We're going to spend 30 minutes on that, so we'll come to that. The question is, how do we determine k? So right now we assume k is fixed, good part. So k is fixed, maybe I should choose a red one here. k here is fixed to some number. 

Then we're going to get to how to actually choose k in a bit. But anyway, the first step here is, for a given data point, we figure out some probability you belong to one of the three clusters here. We can use Bayes' rule to do that. This is essentially just Bayes' probability of set times the probability, or the likelihood of x given the parameters for that particular component that we are evaluating. 

And then we need to normalize it. And it's just an application of Bayes' theorem. And you'll get to do this in a bit. But we do this for all k and all n. That means for all my data points here, I need to work out what's the probability that you belong to this one, this one, and this one. So you get a big table here, essentially, n by k that holds all of these probabilities. But that's the so-called expectation step or the e-step. But intuitively, it sort of matches what we're doing. 

In k means we are allocating points to components. Good. Now what? Well, what do we do in k means? We updated the centroids, right? 

The means of the, sorry, the centroids, the coordinates of the centroids. How about we try to do that again? And that's exactly what's done here in the m-step. So if you actually look at this thing here, which sort of describes the mean of a particular component k, what does it say? Well, it says we should consider the coordinates of the data points and then say to which degree do we actually belong to that particular cluster. And in k means that probability would have been 0 or 1, right? 

But now it sort of belongs a little bit, or in principle it can belong to a little bit to all the components. So we need to weigh it by the probability that this particular data points belong to that cluster. So if the, let's just imagine that in this case here, if I want to update the mean of this one here, I now need to consider all the data points I have available. But given that the probability of these points over here have a higher probability of belonging to this component, they'll influence the updated mean more. The same goes for the covariance. Here you see this is basically the empirical covariance of the points associated with that component. But given that points can belong in a software to that component, we need to put a weight on it to account for that. And then of course, I've sort of not really said about anything about this in k thing here, but that's the efficient number of points associated with a given cluster. So if we look at this sum here, well, in k means the probability here would be 0 or 1 belonging to a cluster. Here it, again, it could belong in a software to all the components, and that is essentially what just would be a summing up here. 

And then the weight, essentially just how many points are associated with that cluster? In a software still. This is it. The point here is that we then need to repeat this and repeat this until we don't see a change in any of these. And that can take some time, and it can take some time because we have many more parameters than in k means, for example. We have a lot of covariance matrices, and estimating covariance can be really tricky, so there are a lot of things you need to be concerned about here. 

But anyway, this is the algorithm that we are basically using for mixture models. Good. Before we move on to this one here, let me just maybe spend a little bit more time on the E-step here, because I said it's base. But if we actually try to write out base rule here for all, let me pick a slightly more pleasant color here. If we actually try to write out base here, the weight here, wk, that is, well, what's the probability of belonging to a cluster? 

So that's sort of the prior. It's the probability of set equals k, and then the normal distribution here, that's p of k. I'm just rewriting this in a slightly different way. The likelihood of xn given my parameters, or given that set equals k. So I'm just writing up the generic version here, and then I need to sum over k or k prime. Sometimes we use that not to confuse ourselves. 

So p of set equals this one, and then p of xn given set equals k prime. This is just a generic version of what's written up here. Maybe just making sure it links back to what you've seen about base. 

So now you actually have to use this and compute something for the e-step in the EM algorithm. And there are a few things you need to consider here. So you've been given some densities, obviously, it's p of x. You have a 1D problem, you have three components. 

You've been given sort of a test point. That means this is probably something you need to worry about. And then we've given the densities for each of the components unweighted. And then there are a few other things you need to worry about, namely that they all have equal weights. 

So that means wk is one third. So try to do this, and I'll give you a few minutes for this. Let me write it up here. 

So let me just repeat what we had before. So the e-step here is, and remember, that is essentially what is the probability that the black cross is assigned to the blue leftmost component. Maybe just instead of having to write blue, red, and yellow, we can just write k equals one, k equals two, k equals three, for example. 

And we know that essentially what we've been asked here is the e-step. So what is the probability that a given point xn belongs to cluster one, given that xn, oops, well, one, given that xn equals zero. And we know this simply by using Bayes' rule. So the probability that you belong to cluster one, and times multiplied by the density of xn equals one at that particular point here. So that would be basically the mean of one and one formally. 

And then you need to normalize some of all the components, and that would be said equals kp of xn equals zero, given mu k, just put prime so we don't confuse ourselves, and then k prime here as well. This is essentially what you need to evaluate. And of course, this expression here can be a little bit tricky if you had to actually evaluate that because the exponential functions and sometimes matrices and stuff. But here you're actually given those components, right? 

So all you have to do is read off this and then plug in the numbers. I'll give you a few minutes for this. Cool, go ahead. All right. In the interest of time, I'll relatively quickly move on. Okay, so what do we need to do here? Well, I've basically given you a solution, I would say. 

All you have to do now is read off the graph. So we need to work out what's the probability of you belonging to k equals one. Well, that's the same as the probability of you belonging to k equals two and three. It's one-third. Okay, just remember that. So if we write it down here, and let me zoom out a little bit. 

So if we write it down here, then we have the probability of set equals one given that x is zero. Okay, that's this thing up here. That was one-third. 

Maybe you can see it there. So that's one-third times what's the density evaluated at zero for component one. So that would be this point. The blue one evaluated at this point here. So that would be this one here. 

So zero point zero five. Now we need to normalize to make sure everything's probability, right? We need to do the same for all the three components. So we have one-third. We can just repeat that for the first component. 

And for the second one, we have one-third times whatever the blue one or red one, sorry, gives us at that point. That is point two five. Point two five plus something now. And the yellow one evaluated at that point is zero. So now we get one-third times zero, roughly zero, right? Zero ish. 

Ish. If you actually do this computation, you can even do it using fractions, then you'll see that the correct option is B. And this is basically doing the E-step for one-D problem. So this is now, well, one part of the EM algorithm. And we are now going to see how it plays out if we actually run it on some data. 

That means both the E-step and the M-step. Are there any questions about this? Sorry, about the problem? 

No, not right now. So the solution is fairly detailed. So you can have a look at that. Okay, so what actually happens if we actually iterate all of this? What we did now was one step of the algorithm. But if we actually look at some real data or slightly more interesting data, then we enter 2D space, X1 and X2. We've got some data points, and they're not labeled here, right? The colors here simply indicate the probability of belonging to one of the components here, and one component is sort of illustrated here. 

Another component is illustrated here. So if you're dark blue, the probability of you belonging to that component is very, very high. If you're sort of in the middle here, and maybe you can't see it, it's sort of a purplish thing here, so somewhere in between blue and red. So that was sort of a correspond to you having 50% probability of belonging to the blue one and 50% belonging to the red one. 

Whereas this point up here, there's almost zero probability of you belonging to the blue component here. Okay, so what happens? We run the EM thing here, and essentially the E-step has been indicated here, right? So what do we need to do now in the EM step? Well, we update the parameters. That means we update the weights, we update the means, we update the covariance matrices. And if you didn't plot sort of an outline of that, well, we saw that it changes a little bit, the mean at least, and then the covariance here changes a lot. So now, actually, we made some progress, I would say. 

Good. What happens if you actually look at the colors again? We sort of still mix the little bit between the blues and the reds with some stuff in here belonging to a little bit to both of them. Okay, if you continue this, so you see this thing here continuing, and then at some point we actually end up with something like this. So of course, in this case, we actually converge to something sensible because, well, we defined the correct number of clusters because we generated the data. 

But we end up with a solution here where the mean for one component is up there, another one down here. And please do remember that these are not labeled data. It's just that we've color coded them based on the probability of belonging to a cluster, or one of the components. 

So this is a totally unsupervised way. We now have a density model. I can describe which area, what's the density in a particular area of space, any area of space here. 

So I can compute what's the density of a given point, for example. Good. I can also, of course, given that I have this notion of components and it's linked to clusters, I can also sort of do a post-doc analysis where I say, okay, but this point here, which of the clusters does it belong to, and what other points do I have in the same sort of associated, in a strong degree to that particular component? And we can sort of say, okay, that's a cluster then. Good. Now, I'll just go over this slide and then you'll have a break. 

But as your colleague down here asked, what's actually the influence of k? So let's just try to do something a little bit extreme here. So we have some data here, this data over here. What would happen ideally using the EAM algorithm if we actually consider all of this training data, and then we have 16 components? If you think about this maximizing the likelihood of our training data here, how do you do that? You do that by putting a delta function, a various spike with infinite height, essentially, on that particular data point, then you get really, really high likelihood for those data points. So what would happen here, ideally, using the EAM algorithm, the way we constructed it, is that you would actually get some spiky functions here on top of each of the data points. 

Something like this, where the variance of that normal distribution associated with that would basically go towards zero. So that's, of course, also an issue, actually, with the EAM algorithm. But anyway, this is what would happen ideally for this case. Is that a good thing that, you know, if you choose k equals to 16, is that sensible? Well, if you now imagine sort of this notion of density and, you know, if we have a group of points and another point near close to it, then we will probably also want relatively high density for that point. So if you place a point here and now look at, we don't, we have the same solution as up here, but we now place a point down here and then we say, what is now under this model, the blue spiky one's up here, what is now actually the density, p of x for that red point? 

And he guess, I guess. Typically when we ask this, it's either zero or one or infinite, zero, yes. Because if you evaluate, if you do a cross section here, right, of these things here, well, that's a little bit difficult, but if I evaluate, let's say if I have all of these spikes and then you need to imagine that's a normal distribution sort of associated, but it is just basically a delta function and you evaluate under that, then there would be zero density for all of the individual components here. So probably a model with 16 individual components is not great here because even though you have a point that's really close to something else, we would probably expect this to have at least non-zero density. 

Okay, so what would be a better way to do this? Well if I take, instead of 16 components, I actually choose three now, which is probably what's ideal here, and then I train my EM algorithm, not on the red points, only on the gray ones, then I'll probably end up with something like this, where I have contour lines like this, something like this, and then one down here. If I now evaluate the density at the red dot, is that going to be zero now? And so it's obviously no, because now we actually evaluated something that's not a delta function on the gray points here. Have a think about this in the break and then come back, yeah, 12 minutes past. 

All right, let's move on. We have quite a lot of slides, luckily not a lot of equations, but so I ended on this note that maybe there's an issue here, that if you actually allow too many k's, that means components, then you risk actually overfitting to your data in the sense that, well, we get one component per data point that reduces the variance around that data point, that means if you now take and evaluate the distribution here at another point, that density is going to be zero. That doesn't generalize well if we now actually have a data point here and we want to know what's the density here, we would expect this to definitely be greater than zero. How do we get around this? And this is obviously a choice about model complexity, and George has told you a lot about cross-validation and how to do that. So what we can do here is actually do what's indicated down here. We can actually train on some of the data on the grade points here, and then we can evaluate for any k that we choose, we can evaluate what's actually the density on our held out data set, or in this case that would be the red points, assuming that the red points actually come from this distribution we're interested in. 

Good, so how does that pan out? So now we're in 1D here, just to illustrate this. So we have some data points here, and we have the blue points here, which might be a bit difficult to see, but blue points here are training points, and then we have some test points that we're not using in our EM algorithm in updating our parameters. Okay, then we choose k1234, and you have sort of been, you see these curves here, they illustrate what's actually the density for each of the components. Then you see some indication of the overall density, unnormalized actually here, but focus on the sort of individual components here, and if we sort of think about what could happen here, then if I just have one component, I have a normal distribution in 1D, if I want to fit that to all my blue points, maybe I should do something like this. That would not be a very good density model because I'm now missing something here, right? 

Because my, let me draw that, because my single Gaussian model here would say there's actually quite a lot of density around this point here, but actually that didn't actually occur in my dataset, so it's probably not a very good model, okay? But we can do it and see what happens, and the way we're going to do this is actually evaluated now on some test points, the red ones. But let's just see what happens. So we run, sorry, EM here for all these four different models with different complexities, and we see what happens. So maybe focus down here on this yellow line here and see what happens. 

That's density related to one of the components. So we run, we run, we run. Oops, what happened here? 

Oops, there we go. So we ended up here, now here we have four components. So a yellow, red, green, and a blue one here, and of course we need to sum them together if we want to evaluate the density at any of the points. So if I zoom in here, what happened here? We see that we have this very spiky density here around this one point. We have a relatively spiky density, red one, around these points here. So of course they correspond to a component, right? 

Yellow component and a red component. But what about this point in between here? If I actually evaluate now p of x for this data point here, because they're so spiky, they're not going to contribute to the density here. So that, like, sorry, this p of x for this red test point is going to be low. 

Okay, is that good or bad? Well, we want, if we assume that the points here come from our data distribution, we would expect the density for this one to actually be, I don't know, relatively high because it's sort of in the middle of a cluster of points. So maybe that's not so great, but at least we can quantify it now because we can evaluate p of x for these test points. So if we actually do that, give me a second. 

So if we actually do that for all of our models here, so we evaluate for all the test points that are available, I simply evaluate what's p of x for the four different models. What do we want to do here? What's your goal? The goal is to maximize the likelihood. So we want a model that maximizes the sum of log likelihoods here or the product of the p of x's. So if we actually combine all of these individual, let's say, contributions for the likelihood and take the sum of the log of p of x, then we want something that's high. 

And that's exactly what's done here. So just maybe to write up what I wrote, oops, that was not a particularly great L. Okay, so this is the test likelihood. And of course, you've heard about test means squared error, test error and all of that. 

This is just the error or actually the goodness of fit that we actually use for the mixture model, is the likelihood. We want to maximize that also on the test points, assuming they come from the distribution. So if we take just, let's say, n equals one to n test, I can evaluate on, did I have four test points? I can evaluate what's p of x and given my model. 

Okay, the reason why I said sum of log is that if I take the log, sorry, if I take the log likelihood, then I take the log to the product that becomes a sum or log p of x and given the parameters. We've done this a couple of times in tests. I want this thing here to be as big as possible. 

Means that my model actually captures the data, it describes the data, right? Good, so here we've sort of plotted this, so we have the number of components I've chosen here and then I have the test log likelihood here, exactly what's written here. I want this to be as high as possible, so how many components should I choose? I should choose this number of components, two, according to this metric. So if you just look at that model and maybe compare it, where did you go? 

There we go. So it's suggesting that we actually picked this model here. So this one, we already sort of alluded to the fact that this is overly complicated, It seems to just fit basically to one data point in one case. And this one, I don't know, it's difficult to eyeball if this solution is better than this one. But if we actually look at, do the thing we've been told to do, namely do cross-validation on a held, or sorry, basically just do a validation on a held out data set, then it turns out that we need two components. This is basically cross-validation for model selection for mixture models. We, as usual, we hold out some data, we fit our model on the training data, and then we evaluate on test data, and then we choose the, let's say we choose the complexity based on that. Now, of course, George has told you, this is probably not a very good estimate of the test log likelihood on some brand new data. Because we have actually used all the data to select this complexity. 

So if you want to be able to report what's the test log likelihood to some external people, you probably need to do the double or two layer cross-validation here. But anyway, we're not going to cover that, just remember that. Okay, so if we are to sum up what K means versus GMM so far, well, if you have K means over here, K means no guarantee of optimal solution. No, well, it actually becomes even worse for the GM, sorry, for the GMM. Many more parameters and much more complicated training, I would say. 

But anyway, if you do it correctly, then be careful, then it's a more powerful model. Does not model shape of the data or cluster for K means, we do that explicitly in GMM. We also explicitly model the size of the data, meaning the number of points associated with a component. And then for K means, it's difficult to actually pick the number of clusters. 

You can use heuristics and whatnot, but what we've seen now is that you can actually use cross-validation for the GMM in a principle way to select K. Just like we did for almost every other model in this course. Good, so they're pros and cons. And of course, the GMM is a proper statistical model. Whereas the K means it's sort of a more algorithmic approach to clustering analysis. 

All right, so does this actually work? So if we use the GMM here on, you remember the Iris data set, then what we've done here is take one component, two components, three components, four components, and do remember that the collage does not represent anything that the algorithm is using here. It's basically just to indicate the different types of Iris flowers in this case. Or actually, that's not entirely true, that the predominant collar here indicates the association to the component. So if you take this one down here, very blue points, then you belong to the blue component. 

So if you see what's going on here, well, in this case, we have one component, one cluster, to describe all the Iris data. If you ask the bottomist thing to go in and look at this, you'll probably say, that's not really true. We know that there are actually three here. And then if you then try two here, then you see something else. 

Well, it seems to capture one of the, let's say, true underground, sort of ground truth classes here. But the other one is covered by one component. So that's why you need to be very careful about components versus clusters here. So if we look at the model, just the parameters, you would say, okay, there's a mean here and a covariance here. And if you look at the probability of those points belonging to that cluster, it's relatively high for this one as well. 

That means this one component actually contains two of the ground truth classes. Then you need to then argue, is that a good thing or not? And you need to know probably a little bit about flowers to make sure that's, if that's the case or not. Three components here actually does a really, really good job. And it's one of the only datasets in the world, but it's so obvious that this works. But in this case, we actually do capture the three underlying classes for the Iris flowers. Another case here where we have four components, you see something similar to up here, just the opposite, or the other way around, in the sense that does seem to capture these two, let's say, classes. But this one class is now covered again by two components. And again, you can do a postdoc analysis and work out, what does this, if you actually look at the probability of belonging to a certain component, what does that actually mean? 

What does each component actually represent? Okay, but anyway, it does work in some cases. And just maybe a side note here, here we're in the PCA space because we can't visualize four dimensional data here when we do this. But of course, well, you could fit the GMM also in four dimensions, right? 

It's just very difficult to then visualize. So here it's done in the PCA space, so PC1 versus PC2. Okay, does the cross-validation then work for this case as well? Well, if we just blindly follow it, then in this case, we've got the negative log likelihood, lower is better. 

Then it turns out we need to choose three components here. But at least we now have a way of doing it, quantifying how well does our model perform on held out data. And that means it gives us an opportunity to actually select the complexity of the model. So here it actually selects the, let's say, correct complexity. But there's a procedure for doing it, right? We can do it and we can report the procedure, we can report the result. 

Good. So maybe one side note here is that the reason why there's variance across here is that we've repeated things five times, I believe, in this case. So that also shows that depending on how you initialize your GMM model here, it will give you different results. So again, just a word of warning that there's no global optima here. Or there is a global optima, but you're not guaranteed to actually find it. 

Unless you try to repeat things and try to initialize in different ways. Right, now we're shifting gear a little bit. We're going to outlier detection and normally detection. So the story here that I'm going to tell you is that, well, we've seen the density models up to now. And now we sort of, let's find, not find an application, but there are certain applications where this is generally really useful. And anomaly detection is one of these aside sort of from this clustering analysis we can do here. Anomaly detection is another application of density based model. We're just going to throw in a bunch of different methods for doing this anomaly detection and see how that works out. 

Okay. So the definition here is that given your data set, we're interested in finding, let's say, objects that somehow deviate from the normal behavior here. And you can already see how that relates to our cats, dogs, and dinosaur example that, well, the dinosaur deviates because it's further away from the cats and dogs than they are. Than the distance between the cats themselves. So we're trying to from the features themselves, the attributes themselves, let's figure out if there's something that sticks out. Good. 

Why would you want to do that? Well, maybe some of you would have been subject to fraud, credit card fraud or something to that effect. But it could also be, let's say, in the health domain where, well, can we somehow automatically detect if you deviate from your normal, let's say, broad pressure or whatever other physical measurement you could do. And also, highly used in, let's say, industrial systems like wind turbine binds or something, is the wind turbine suddenly operating outside its sort of normal specification or something like that? Simply by measuring on the sensors, not having some experts say, yeah, it's actually operating in a weird way. 

So we're doing it only from the measurements themselves. Good. So just to remind you, we still have the cats, dogs, and dinosaurs. The dinosaurs, in this case, object nine. I think the cats are here, the dogs are over here. 

Just to remember. And then, and slightly more interesting data set, at least in my view, is that we've got some whiskey from Scotland and we've got 86 of them. And they each get a human rating and hopefully they didn't actually taste this in one setting or one day or whatever it is. But the person who did this or people who did this, they are asked to actually rate this from one to five on four, 12 different dimensions. So we have basically a 12 by 86 dimensional data set here. So based on, so each of these correspond to one attribute and you have a value between one and five with its low degree of, let's say, sweetness or high degree of sweetness. Okay, so we have that for each of the whiskies. Of course, now we have a 12 dimensional data set. 

So well, what to do? How do we visualize that? How do we explore that without actually trying to map it to some label or something? Well, we can do PCA again. 

So we have a visualization technique here where we project the stuff into 2D and you see something interesting happen. It seems like there's a bunch of things over here and intuitively you would probably say that stuff that's not sort of in this group where the typical whiskey is, maybe they are outliers. I don't know if you know a lot about whiskey there, but these guys here and actually also these guys, they are somewhat strange. So I don't know if anyone knows whiskey, but it turns out that these guys here are from a small island on the west coast of Scotland called Islay. This is where they use peat or toe in Danish to actually dry the barley or the malt. 

So it has a very, very smoky taste. So actually we can already sort of see something sensible going on here. This is not strange, by the way. 

But okay, good. So PCA, then we can already start to sort of think about distances between things and maybe stuff that sort of unarrives a long way away from the sort of norm. Maybe they are outliers. So okay, we can at least think about this. And again, just to highlight this, we see the same plot here. And then you could of course plot it versus one versus three, two versus three and so on. And maybe you see something different pop up. So PCA, and I think you've already done that, presumably. 

You've already in project one, you already tried to do this and so okay, maybe there are some data points that are sort of just intuitively further away from the other or far away from the other ones. And maybe we should have a look at those. I'm not saying they're outliers because people would be really, really sad if you then say, sorry, we can't produce this anymore. Some people really like this very smoky whiskeys, right? So it's just, okay, maybe we could already flag these guys here using a simple method like PCA. 

Good. But this is sort of a PCA plot, right? And it's really difficult to somehow summarize what's going on. So how about using the dendrograms from last, yeah, the week before Easter. And if we actually try to do the dendrogram here, where we try to sort of summarize distances, so to speak, then we can do it for the cats, dogs and dinosaurs. 

And you'll see a few things. You see that the cats here, they're actually sort of grouped at a relatively low level, meaning that the intercat distance is relatively low. Similarly for the dogs and then the dinosaur, it links to the other ones at a very high distance, meaning there's a distance of four point, whatever it is, four point five between the dinosaur and the group of dogs in this case. 

But that gives you an indication that actually the points here that link at a high, let's say distance, maybe that's something we should have a look at. It's very difficult probably for you to read exactly what's going on here. But I'll just hide a lot of few things. So what you've got over here are these, remember? 

You've got like a woolen and you've got Lafoyc here. These are some of these peach whiskeys or smoky whiskeys. As it turns out, these are the two sort of last ones here in the Stendogram. 

And they seem to group together relatively low, but then they group together with other whiskeys at a relatively high level here, right? So maybe there's some information in this if you look really, really careful. But anyway, it can be certainly not sort of just a purely automatic thing. It's something where you actually have to apply maybe domain knowledge or whatever you have, your knowledge about whisky in this case. But anyway, it's useful to actually visualize this. So another thing you probably did in Project One was sort of density-based techniques. So in this case, the previous slides, those are distance-based, right? But another family of methods are basically density-based. And I think you did that in Project One where you plotted the histogram of individual features. 

And then maybe you tried to fit a normal distribution to your features and said, okay, if you're actually in the tail of this distribution, maybe you're an individual thing, I mean, we should have a look at that. And that's exactly what we're going to do here. So typically we standardize it, so compute the set score, some would say. But we basically just standardize the features individually. And then we fit a normal distribution and then we put a certain threshold on it. 

So what we're going to do here is basically take this medicinal one from the whisky data set. So that's one attribute, remember? It's between one and five. It's not really normally distributed, right? But anyway, we can hand-wave a little bit and then standardize it, sorry, fit a normal distribution and then say, well, if you're above three, maybe we should actually have a look at you. 

Because then you have a low density compared to sort of the standard whisky. Okay, so what happens in this case? Well, the problem here is that you actually need to do that individually for all the features. So that becomes a little bit tedious, right? But anyway, if you look at the medicinal one, we sort of see a pattern maybe. I don't know if there's any of the other ones that are probably interesting. 

Not obvious, I would say. So let's have a look at the medicinal one and see these guys over here who really have low density compared to all the other ones. What are they? Well, voila, as it turns out, if you look at the medicinal attribute, look at low density, let's say whiskeys in this case, you get the aderg, lekebullen and lefroyg. 

So all of these, again, turns out there from this small island called Islay. Anyway, so the problem here is, of course, that we're doing this on an individual attribute basis. So could we do something else? And I'll just put in this slide here maybe to just give you an overview. Because what we've done now is maybe recap a little bit what we've already done in the sense that you did PGA in the first project and maybe we could think about distances in a PGA space or we could use the dental grime and so on. 

So basically, that's two different ways here if you try to summarize it. The density-based ones, in terms of you can think about this fitting a 1D Gaussian and looking at examples that have low density, or there are the distance-based ones. Okay, so if you think about the density-based ones, then what could you do? Well, we have the 1D normal that we just did, just highlighted again. We could, of course, fit a multivariate normal distribution. But probably the data is not normally distributed if you look at 12 features and attributes, so maybe we could think about the GMM that we just learned about. 

It's a more complex density. Yeah, we could definitely do that. And then, of course, you would hold out some data, maybe hold out one data point at a time, fit the GMM, it's really computational heavy. Look at the test log likelihood and if the test log likelihood is small, maybe that's an outlier. We're not going to apply this, at least in the lecture, we're not going to apply the GMM for outlier detection. We're going to actually throw in another simpler type of GMM, which is called the kernel density estimator or kernel density estimation. 

But we'll come to that. But basically, any kind of density you can think of here, you could basically use that for outlier detection using this recipe. So, estimate the density of the data objects, leave out the data points you're interested in testing, evaluate the test log likelihood on that data point that you've held out. 

And then if it has relatively low test log likelihood, maybe flag it as a possible outlier. That's the density based one. Then we're going to look at, I'll continue looking at distance based one. So we've already sort of hand-waved a little bit saying, well, if the distance between a point and everything else is high, then probably that's sort of an outlier somehow. 

So we're going to write that up in slightly more formal way, using what's called inverse average density or what's called inverse distance density and average relative density. We're going to do that at the end. Good. 

But I said, well, we've seen what a 1DE can do. I've argued multivariate normal probably doesn't work. The GMM too expensive to use for this purpose. 

At least that's my claim. So we're actually going to do a simpler version of the GMM so we can do it in density based outlier technique. So just remember a few slides ago, we had the GMM. 

It's just building a density where we define a number of components and then we learn the weights, we learn the means, and we learn the covariance for that particular model. And we saw that, we can do that. Now, let's throw in this idea. So instead of actually having separate means for all of the, first of all, sorry. I'm going to set now the number of components to the number of training points. Now I'm going to, before I actually said that was a bad idea, right? 

But it turns out to be a good idea. So I'm going to do that now and then I'm going to set the mean of each of the components to the value of the data points. That means the values of the features, the attributes. Okay, then I'm going to set the covariance matrix to just an identity and then just a constant basically. So now just have one variance for all the components. Okay, that's one parameter, not a full covariance matrix for each of the components. And then I'm going to just say that each of the components that means each of the data points, they actually have the same weight. 

Okay, so what does it actually do? If you look at the math here, well, it's just basically a mixed model again. It's just that the weight is just one over the number of training points. 

My base distribution here is a normal distribution, yes, like before. But the mean of each of these components of which I have in train is just the value of the data point. So that means the bump function is centered on the data point. 

And then I have the standard deviation here, it's basically just a constant. So if I try to draw that in 1D, whoops, that's almost the line. Let's see if I can actually do a line. 

Okay, this is a line, straight line. I have some data points here, here, here, here, here. So what I've said here is that these are my training points. Let me actually throw in a test point just to mix it up. So my training points are the blue sort of crosses here. So what I've said is that I need as many components as training points. 

So one, two, three, four, five. I need to place a bump function like a normal distribution on top of them. It has a fixed variance that I've defined. So if we just draw the individual component densities here, then I'm going to get a bump here, here, that's supposed to be equal. They're supposed to have exactly the same shape, they're just centered at different points. Now I need to add those up. So if I draw the, actually, the resulting density, how come on, if I draw the resulting density, then I need to sum it up right and then normalize by one over in train. So I'll get sort of a, or something like this, maybe zero here and then it goes up. 

Too wide. Anyway, roughly something like this and then something like this. But this is a very simple mixture model. It's simple in the sense that the means are given. I have a constant sigma that means the variance. 

But otherwise the construction is the same. I just need to sum up with a certain weight. And by the way, the weight is also given, right? 

So this is a very simple construction. And now think about, if I could want to remove one of these from the sum here, or I suggest to evaluate the particular density and then subtract it if I want. Okay, so if I want to actually, now, well, what I wanna do now is actually evaluate what's the density at this red test point here. And of course I can do that by summing over all the five components I have here. Okay, I'll come back to the other point I just made. But now you have to do a little bit of work. And I'll give you a few minutes for this as well. Just think about what you need to do here and maybe not talk. 

Just think for one minute, what do you need to do here? I've given you the curl density estimate up here. I've given you sigma and you need to evaluate that at a given x4. You're given some training points. 

And you're given what is the form of the 1D normal distribution here. So just think through what you need to do, then I'll go through it quickly. Okay. So it is essentially what I did on the previous slide, except now I need to use the equation somehow. So I've been given a data set, oops, exactly like I had, oops, yeah, that's okay. Like I had before, so I have one, two, sorry, two and three. And I have five, I have 10, and I have 12. And I'm asked now to evaluate if I build a kernel density estimator, that means I actually construct this or that's a function I can evaluate for any x. 

If I now actually evaluate it at x equals, what is it, four? Oops, I intended to use a different color here. Come on. So I have a test point, yeah, should of course be in between here. 

What do I need to do? Well, I've said I need to have a bump function, so I have that on each of the individual training points here, right? So I have something like this. 

Each of them defined by this thing here, so maybe I should just plug in and see what happens. So what I'm going to do now, because it's really tedious to write this up, because essentially you have to write out this whole sum, where you plug in this thing here for this. But if you just take one, there are going to be five terms here, one, two, three, four, five. And if I just do that, well, one of the terms, then let's see what happens. So I'm going to pick basically the term that involves 12. So that would probably say that's going to be the last term up here, so k equals five if you want. 

So if you write out the normal distribution in that case, we're going to get a constant, 1 over 2 pi sigma squared is given, so that's 4. And then I'm going to get an exponential bit, minus 1 half divided by 2 times 4. That's 8. 

Then I'm going to get the difference between my tis point, the red one, that's 4 minus 12 squared. I'm going to get a term like this. Okay, so if we actually look at what this is going to be. So this thing here is obviously 8 pi exponential to 1 over 8. 

What do we get? So we get 4 minus 12, that's minus 8 squared, that's 64 divided by 8, that's 8. Yeah, do we agree? Good, so I get minus 8. 

So let's see where that brings us, no objections. So I know that this is one of the terms in my sum. Then I also know that this part here must be the same for the other four elements in the sum. So we can pull that out if I want. Okay, but at least I know there should be something matching this in my result down here. And as it turns out, if you look at this, then I have 1 over square root 8 pi, and then I have, I need to divide by 1 over 5. 

That's what's here. So I pulled out that constant. Then I'm looking at, where do I have something that's exponential minus 8? Well, it turns out to be this one, so this is the correct answer. If you wanna be absolutely sure here, write out this whole thing and then reduce afterwards. So the solution here is pretty detailed, so go through that one in detail if you didn't get what I was doing here. 

All right, I have again cheated a little bit. In every single slide, I've told you, well, we have sigma squared here. How do we then find that efficiently, I would say? Well, you could use the EM algorithm, but that's going to be overkill here, and there's a particular reason for why we may not wanna actually do that. 

So how do we determine sigma squared? So if we look at the data set here with the cat, stalks and dinosaurs, and then actually think about what we could do. And when nothing else sort of comes into mind, let's just use cross-validation to find whatever we're interested in. And you can think about, now we're using cross-validation to find the parameter in our model. But is there any difference between using cross-validation, gradient, descent, EM, or whatever method you can think of? 

Probably not, essentially it serves the same purpose. We wanna find either the complexity or some value of a parameter. So here we're using it to find the value of this particular parameter. 

So what can we do? Well, you can basically leave out one data point. Let's leave out the dinosaur guy up here. Now we fit a kernel density estimator that means fitting bump functions on top of everything else. 

And now I'm going to evaluate what's actually the likelihood for this held out dinosaur guy. I can repeat this for every single data set, sorry, every single data point here. So I've held out point nine, I can hold out point eight, and then I can look at the test likelihood. And then I can do that for different values of sigma squared. So that's a sense you're just using cross-validation. So here I've got sigma squared on some scale, log scale. 

And it turns out that if I actually compute the log likelihood here, like we did for the GMM, that means the test likelihood for the point that we didn't include in our GMM training or construction. Then I get a curve like this and it turns out there is a maximum somewhere up here. It has a certain value. 

And if you think about what that value would be, it's probably around this one here. So the kernel density estimator, bandwidth or width, should not be really, really spiky. We sort of already knew that. 

It shouldn't be really, really broad and cover everything. Should be somewhere in between. And we can select that sigma squared using cross-validation. In particular, leave out cross-validation where I leave out one data point, fit the model, evaluate what's the test performance on the held out data point. I do that for every single data point. And then I'll of course sum the log likelihoods at the end. Why is this relevant for the KNN in particular? So let me find the equation for the KNN here. 

And I sort of got into trouble before when I tried to say it. Because as you see here, essentially all you have to do is take the individual data points. For each individual data points you have, so you only need to evaluate this thing here. 

So just leaving out a data point, it's just leaving out one term in this sum. It becomes really, really efficient to do it this way. So you can start out basically evaluating for all your, in case of the dinosaur data set, you can just evaluate this guy and store it in memory. 

So you have nine values for this thing here. And then you can essentially just leave out the one in the sum when you're trying to actually, when you're trying to compute what's the leave out test log likelihood for one of the individual points. Just believe me, it's an efficient way of doing it. Good. Does it actually work? 

What does it do? So again, we have the dinosaur data set here. What we've done here is one by one, we leave out one of the data points. 

So we leave out the dinosaur, which is, you can't really see it here. But this is the ninth data point down here. So when I leave out the ninth data point and fit a GMM, or sorry, not GMM, it could be a GMM, but here it's a KDE, a fit that on the eight remaining data points. What's then the test likelihood on this data point? 

That means this one up here. It turns out to be really, really low, but we could have sort of guessed that. If we leave out this data point, let me see. So this is point five, this one here. 

So this is when I leave out, where is it? I don't know, this one down here probably. When I leave out this data set, well, at least it's not really, really low, right? But it's, well, it has a certain value, which is significantly higher than the one out here. 

So if I were to put a threshold here, maybe I could say somewhere around here, every data point that has a value lower than that should maybe be flagged as an outlier. Okay, good. What happens on the whiskey data set? 

Exactly same story. We figure, we train the KDE, meaning that we find the optimal sigma squared. And then we try to evaluate what's the test likelihood. On the held out whiskey in this case. Okay, it turns out there's a bunch of whiskeys down here that have been flagged. They have low test likelihood. So maybe we should actually try to look into those. And you've seen some of these before, at least Glengiri here and Abalua I think has all also popped up in other contexts. Good, but anyway, we get away of ranking whiskies based on how unlikely we think, sorry, how abnormal we think they are based on the density. 

Good. So this is sort of a, again, a probabilistic statistical way of doing it based on the density. If we now go back a little bit and think about distances and could we not maybe do something that's not already related to statistics and probability? 

And yes, it turns out we can. So let me try to outline a couple of sort of heuristics here. The reason I call them heuristics is that they're not based in any probabilistic sort of framework. 

We still call them density, but it's really not a proper density. So let me just reiterate something I've said before. So imagine we have the cat, dogs and dinosaur data set again. So it makes sense that things that are far away from its neighbors should probably be an outlier somehow. But we can formalize that a little bit and say that things that are sort of on average, at least far away from its neighbors. Well, if we define the density now based on this, we take the inverse of that distance as a measure of the density of that data point. That means points that are far away from everything else, they would have a low density based on this measure here. So of course, the way you measure density and figure out the average density here would be that you consider a number of neighbors. So if we consider the two nearest neighbors here, for example, that will give us an average, so there's a distance from here to here, there's a distance from here to here. We take the average distance and raise it to minus one. So if the distance is high, we're going to get a low density. If the distance is relatively low, we're going to get a high density, meaning things that are close to its neighbors have high density. That's essentially all it means. We haven't even used any probabilistic densities or distributions of any sort. So it's purely based on the distance. So let's just go over this. 

What does it say? So it's the density of a data point. Let's call this one up here. Well, it needs to take it to the coordinates of that point. The number of neighbors we want to actually consider. And then we need to take one over some expression down here. And the expression is basically one over k, so the number of neighbors you're considering. And then you need to sum the distance. 

So we're taking an average distance here. So you need to define a distance measure. And it's the distance between the nearest neighbors here. And this is sort of a slightly convoluted notation here that we need to consider all the points. That means all the neighbors close to the i-th data point. And we need to consider whatever k of those. And then we need to compute the distance from the data point we're interested in and those neighbors. 

That's all it's saying, this slightly convoluted thing. So if I put this as x i up here, need to consider the two, and I want to consider the two nearest neighbors. Well, I need to consider this data point, this data point, and the distance between our dinosaur on this point, the distance between our dinosaur and this point. 

I need to take one over the average distance of those two. That's it. Now, is that a good thing to do? I don't know. 

We'll see a few examples in a bit. It's one way to define a density that's not sort of grounded in the probability and statistic. Now, this doesn't actually take into account, well, what is actually the density of my neighbors? 

And we'll see whether it's important in a bit. But down here, you can sort of see that, well, if I consider this as i's data point, and I just take the distance, well, I'm going to get a relatively high density. But it doesn't take into account the fact that, OK, the normal down here seems to be that the density is high, that things are close. 

You can imagine cases where this becomes important, and I'll show it in a bit. But we're going to define another measure here, which is called the average relative density. This is where you take the density from the fall, but now you actually also consider the density of the neighbors. So if you think about the dinosaur example up here, if I want this thing, I don't know, I want to compute the relative density of this thing up here, I need to consider the two nearest neighbors. 

OK, that's what we did here. That gives me the density. But this thing down here, I now need to normalize by the density of the neighbors. OK, what's the density of the neighbors for the i's data point? Well, this thing down here would have a high density. This thing down here would have a high density. 

That means it would actually be even lower than it was before. So where does this make sense? Well, I think it actually, well, sorry, how to get some entration about what's going on here. And I think maybe looking at a concrete example, and I think the best one over here, this one is the best one to actually consider. Because if you just intuitively look at this, you sort of see that there are two groups. 

There's one up here, there's one down here. We're interested in determining whether some points are outliers. So we probably may want to consider these two points. So if we just consider, let's say, the distance from, maybe I should say that blue indicates very low density, according to this distance-based notion of density, red indicates very high density, white indicates something in the middle-ish. So if you just intuitively think about the properties here, over here it seems to be normal that things are far apart. 

Down here it seems the normal sort of state of affairs seems to be stuff is really close together. So if we want to actually say something is an outlier, maybe we should take that into account. And that is what the average relative density is doing. 

Good. So if you look at this point here, this is the distance density, or the inverse distance density as it turns out. Here we're not considering the density of the neighbors. So all we have to do here is find the six nearest neighbors, take the average distance from this point to the six nearest neighbors, and then take the inverse of that. That gives me a measure of density for this point. 

If it's blue, then it has low density. So this must be an outlier according to this measure. If you look at this stuff down here, since the distance between these two points, the six nearest neighbors of this one, they would all be down here. Since it's really, really low, or there's a short distance, then the density would sort of be relatively high, at least not as low as this one. 

So it's sort of maybe in the white-ish, let's say, area here. This is where we only consider the distance itself to the neighbors. If we also take into account the density of the neighbors, what happens? Well, now we actually take into account the fact that the normal state of affairs over here is that things are far apart. So it means that these guys over here, which tend to be sort of very low density before, they're now actually fairly high density because, well, that's what's expected in this area of space. Same down here. 

But what happens to this guy here now? Because we're now taking into account the sort of behavior of data over here where things apparently seem far apart, then maybe this one is not as clear that this is an outlier as it was before. Okay, so these are just, again, two more measurements, or two measurements of density that you can sort of throw in the mix and maybe use as an outlier detection. And I'll skip this one because we are out of time, so I'll just go through the solution in pretty detail. So have a go at that. I'll just wrap up here. 

So if we now actually use the inverse distance density here, do we get something that's different from using the proper, let's say, statistical densities? And I don't know, I can't remember, but hopefully you would think that there's some overlap. But again, we can do exactly the same thing as we did before. Again, nine, the dinosaur thing here turns out to have a low density, so maybe it's an outlier. Again, for the whiskey, same story, we see some usual suspects here. But generally, again, we get a ranking. We don't know if it's a good or bad ranking, right? But we would have to actually look at the data to determine that. Okay, we can take the average relative density, the story is the same, the dinosaur is still flagged as an outlier. Okay, average relative density on the whiskey is, again, some overlap in what's being ranked here. 

But again, this is a slightly different list because we're now taking into account the density of the neighbors that we're considering. So what have we achieved here? So I've thrown in a lot of methods into the mix here today. And in general, well, we have this sort of kernel density estimator, which was derived from the GMM and some specific, let's say, simplifications that made it usable and efficient for this purpose, it gives us a certain list of whiskeys as outliers. We had the distance density, it gives another list of outliers. Average relative density, yet another list of outliers. 

So what's the story here? I mean, can't you just give us a method that works all the time? And unfortunately, no, not in this sort of unsupervised domain where it's really difficult. We don't have any supervision or human supervision to actually determine if it's an outlier. So what we're basically proposing here are some methods you can use to actually rank them and maybe find the candidates that are outliers. I think that's the way you should be looking at this. 

And then, of course, know the specific methods. That's sort of what we ask of you here. We don't expect you to go taste these whiskeys. You should, obviously, they're pretty good, some of them, but anyway, that's not expected here. 

So to actually determine if these are outliers, right, that means they're abnormal or something, you would need to know a little bit about whisky. You need to know something about the domain you're working in. And I think that goes for everything we do here in machine learning. Don't just blindly do whatever you can do using scikit-learn and enclose your eyes. Actually think about the methods. Think about the evaluations. 

Okay, so I just tried to, for my own sanity here, try to summarize what we did today. We basically said we can do density modeling using a standard Gaussian distribution. That probably doesn't work for real data, complicated data. 

That has multiple modes. We can extend or we can build or construct a mixture model to be able to combine simpler base distributions. And we had the E-amalgorithm to actually find those parameters. And here the important thing for you is understanding what is the density, the construction, not necessarily being able to derive the E-amalgorithm. 

You need to actually be able to apply it, at least some of the steps, especially the E-step, but driving it is not part of the curriculum here. We use cross-validation to find the complexity, as always, as you've seen multiple times. And of course we can use that for density estimation on its own right. We can use it for analysis of visualization. So often what you want to do when you get some real data is actually visualize it somehow. And instead of doing a histogram, maybe somehow you would get a smooth curve. 

And here the kernel density estimator is a nice sort of simplification of the Gaussian mixture to do that. Because you put a bump function, a smooth bump function on top of your data. Now you can actually plot the histogram, or sorry, plot the data distribution in a nice way. So the kernel density estimator is often used for that purpose, visualization, basically. But you can also, as we did today, use it for outlier detection. And then of course we had a whole story about anomaly detection. 

And of course the story here was, yeah, there are many, many different ways. It can give you sort of a rank list, or they can all do that for you. But you probably need to actually go and look at the data to determine if these are abnormal or what's actually the state of the affair, or what's actually going on here. Okay, so there are a few different types. The distance-based ones, maybe slightly more intuitive. 

Then there are the density-based ones based on kernel density estimator, or even mixture models. And I think that's it for today. So go do the exercises. Thank you. 
Speaker 1: Alright. Hello. So it's one o'clock and a few minutes after one o'clock. So today it's the week 12, I suppose, of machine learning and data mining. And we're going to talk about perhaps the only topic that sort of truly comes from the data mining community, namely association mining, where we essentially look at co-occurrences of items that occur and try to infer some rules from that. 

Essentially, it boils down to joint probabilities and conditional probabilities, but we'll have a look at that. As usual, supposed to read something. Hope you read that and took some notes for the exam. You are, as always, encouraged to give some feedback on the content and all of that. 

So please do that. And then just one announcement from George's. He will, after the lecture, send out an announcement on DTU, learn with sort of the proposed test exam, or mock exam, that you can use to practice, including this, what's it called, exam, or sorry, answer sheet, his fancy answer sheet, where you have to tick a box and hand it in. So we encourage you to actually print that and then bring it next time and George can test the whole system. So I think it's also actually this answer sheet is at the end of the course book. 

If you go to DTU, learn, you can sort of see that thing you're supposed to fill out in the exam. And the exam is on the 20th of December, if you haven't noticed, right? So it has a special date because it's a big course. So we don't know the time yet. 

It's usually from nine to one, but it could also be from three to seven, which means that people with extra time would then have go into the evening, basically. But we'll see once it's available on this. Let's see if I can show you. For those of you who don't know, it's supposed to be on this examplan.dtu .dk. In here, you can't see what I'm seeing, but you should be able to see sort of the schedule for the exam and where you're supposed to set. 

So keep an eye out on that. We have absolutely no say in the scheduling of the exam or basically all we do is create the exam and hand it over to someone else and they'll deal with it. Anyway, so you'll see an announcement on DTU Learn from George after the lecture. So today is basically, let's say, the wrap up of the unsupervised learning part. So we talked about a couple of weeks ago, we talked about clustering, namely K-means specifically, hierarchical clustering, if you remember, relatively intuitive, simple algorithms where you look at distances between data points and try to group things together in hopefully some sensible way. 

And then last time we looked at whiskey or specifically methods for grouping types of whiskies together and we looked at using density models for that, like the Gaussian mixture model, which is sort of a, let's say, a key method in the course as well. Then today we're going to wrap up this sort of block with association mining, as I told you, and luckily for you, it's not going to be one of the usual two-hour lectures. So we're going to let you off a little bit early today and then you can go work on the test exam or as we keep telling you, please do the exercises, right? Did the practical stuff, where you run the code, you actually ask yourself questions about what's going on here. So make sure you do that. There will be TA's today and I'll come back to that as well, what happens next week. 

Great. So on to association mining, or rule mining, here we are talking about concepts such as support and confidence of an association rule and if you haven't read the book, I'll come back and define what an association rule is. And then we're going to exploit a few properties about, basically, probability, but we're going to frame that in terms of support and confidence about what's called the a priori algorithm for association mining. 

Quite popular method, but anyway, let's get on to it. So we're going to talk about records here and essentially you can think about an observation. So what we have in a record is a set of items, it could be like if you go into a supermarket, that's going to be the example we'll use, you buy a subset of the items that the supermarket has at display or selling you. So basically a record here would be on your receipt, you see 10 items, so now we have a record showing that you bought 10 items on a certain day, for example. We have a record for each of the customers buying something on particular days and all days, all the year. So we have tons of records basically containing, let's say, sets, which has the potential items from the supermarket. 

So now what we are looking at here, I actually produced so-called dependency rules, association rules, that's where we essentially say that if you bought a certain subset, let's say four items, what's then the probability or the confidence that you're going to buy other types of items in the supermarket. And of course this can be sort of interesting for a supermarket if they want to, let's say, sell a particular product or want to make sure that, I don't know, people don't buy particular items together who knows like painkillers and alcohol together or whatever. So you can sort of think about this if you observe that this happens, you can try to move things around in the supermarket to break this association rule. So it's all about co-occurrences. 

So if you buy beer and chips, they're also likely to buy whatever soda, for example. So just to sort of say that this is a little bit of a special method in a course like this, but it's actually a quite popular one, as it turns out. So it's being used essentially in a number of applications and I would say that there are sort of number of citations that the paper has received over the years, it's sort of a testament to that. 

Of course it doesn't necessarily say if it's interesting or anything, but it's sort of an interesting method we are going to cover anyway. So as I said, you can sort of think about this supermarket example again. We have our records, so here we have five records, and we are interested in sort of seeing what are the common things that people buy in the supermarket and buy together basically. So if you just say okay, it's a slow day, so only five customers came into your shop, and here's a record of what they bought together. Now you're interested in seeing what are some of the common things that, or the common items that customers buy together. So it could be, for example, that we extract some rules saying that if you buy milk, then you're also very likely to buy soda in the same sort of thing. Then there's a slightly more sort of problematic one perhaps, is that if you buy diaper and milk, that was sort of presumed we've got small children or whatever it is, then you're also likely to buy beer. Of course this is an actual right, or at least I hope so. But you could imagine maybe, let's say a morally well adjusted supermarket would try to maybe, let's say break this rule. 

So remove the beers from the milk and diapers or whatever it is. It won't enforce that because they see that this is really something that happened. So just think about how we can use these rules that if you discover a rule like this, based on your analysis of what people are buying, then you can use these sort of discoveries to then change things or reinforce things. For example, changing the position of certain items in the supermarket. Okay, so the point here is that we're interested in sort of first of all looking at what are people buying together and then we're looking at what's the condition of here. So if you buy diaper and milk, what's then essentially we will call a confidence, what's the probability that you also going to buy beer? Remember, there are other things you could buy here. You could buy chips or whatever, I don't know bread or something, but it turns out apparently in this made up example, there's a high probability you're going to buy beer. Okay, so of course there are all tons of different let's say rules you could extract here and we're interested in the one that sort of happens a lot. That means you see these items together often and that this thing here has a high probability, for example. These are the interesting rules. Okay, so the way we're going to represent this thing here, here to represent it as a set, but of course we end data, so numerical business using computers and stuff. 

So a very convenient way of representing these sets of items would be as a sort of indicator matrix here where we have a matrix, each record as a row and then we simply indicate with a one or zero if you bought that item in that record. So let's take the one with the beer, where is it? There are two of them, actually a lot of beer. So let's say we look at this one then, well soda got a one because we had a soda in the set here, milk get a one because there's a milk in the set at beer similar and diaper as well. So fairly simple just indicating if that item is part of that record or not. Good, so in order to then extract these, so let's say rules, association rules, then there's a bit of terminology here. So basically what we're going to say is that any sort of subset of these items or could even be the full set, we call that an item set, so it's just a set of items, easy peasy. 

But then we're going to say what's the support of an item set? That means out of all the transactions or records that you have, how many of them actually contained x? So if you only look at a, let's say let me write one, if you only look at one of these where we had beer, so if you had an item set of size one, this one here, basically we're just counting how many times out of all your records contain beer, that's easy. And then of course it gets a little bit more complicated if you have milk and diaper, then you need to look at the co-occurrence of these two and so on and so forth. All right, and support is one thing and then of a particular item set here, but we're also going to talk about these association rules where if you buy x, in this case down here, then how likely are you in some way to also buy y? So here we're also interested in, as it turns out, we are interested in the support, we're going to write a support of the union of x and y. So if I had the example from before, what was it? 

Was it milk, diaper, what? Then we had the association rules like this. Then in order to actually look at the, at how likely this one is, we actually need to compute the support of the, let's say the union of milk, diaper and beer. So we can talk about the support of that rule, of that association rule, it's just essentially what's written down here, it's the union of x and y. Okay, so the way support is defined here and it has to do with historical terminology that we call it support and not just the joint probability, because it is essentially the same or it is the same. So we say that the support of this association rule x to y, simply counting how many times the data items that occur in all your set of records, then divided by the number of records you have in your dataset. So here the sigma, just to confuse you, it's just a counter, just saying how many times did that occur? 

And that turns out to be exactly how we also looked at the frequentist definition of the joint probability of the speed events. So if we, if we just maybe for sake of argument here, let's, let's do a small example here. So if I'm looking at, let's say the support of beer, I think that was the example we had before, what would that be? Here I'm looking at one item, how many times did that item occur? So one, two, three over five, right? 

One, two, three, we had five in total, so it's essentially one over five. It becomes a little more tricky. Let's say an association rule here like beer to diaper. I don't think we've seen that one, but now I need to take, to find the support. 

I need to look at the union of those two. So how many times did beer and diaper occur? Sorry, three or five is the correct one. 

So I'm going to make a lot of mistakes in this lecture because you're counting a lot and for some reason you make mistakes when counting, but correct, it should be a three over five. Yeah. Okay. But then for beer and diaper, we need to look at, if we want the support of that rule, we need to see how many times did beer and diaper occur together. So important thing here is this co-occurrence. So we need to look at beer and diaper. So here we had beer, but we did not have diaper, so we're not counting it. Here we have beer, here we have diaper, so we're counting one. Beer and diaper counting two. So two over five. 

That's the support for this association rule. So two over five. Yep. 

Okay. Hopefully that all makes sense. And it goes back to when we did naive Bayesian, all of these things here. So basically there's nothing new under the sun here. It's just a different way of framing it for a particular purpose. Okay. 

Great. So now we know what the support is and you'll also see it written as support to be a little bit more explicit like X to Y. So whether or not you want to remember that it's called support and how it's defined, or you want to remember, okay, it's just a joint probability here. That's fine by me, but remember there is this link back to what we did with probability in lecture 10 weeks ago or whatever it was. All right. So now you have to do a little bit of counting and I'll give you a few minutes for this. So just to maybe make it a little bit easier. You're given the table here. 

So here you have 10 records. You have some items. So you should think about these as items basically. And then you're looking at this association rule here. And you have certain items, condition on those you're looking at an association will, what is then essentially will come to that. But would you also then buy H what's called X, A, A6H, whatever that means. 

It doesn't really matter here. So maybe let's just mark the ones we're interested in. So this one, this one, this one, and this one. 

And of course, we also need this one. It's the union of the two sets. Okay. 

Give that a go. So are you getting tired of counting things here? Let's see if we believe here it's C, but let's just quickly do it. Okay. So we need to sort of look at the co occurrences. So all of these things here need to have a one in the table that's sort of in the same row for us to count or increment by one. So let's have a look. 

So those look start out here. It's a condition that must all have ones, right? So we can disregard anything that has a zero. So, so if you start out with X2H here, so essentially this one and look at that column first and then move to the right, then the first one that has a one is this one. And then we need to look at the next item. So also one, also one, also one. 

You'll be, we can count one. So I'm going to add a tick mark if it's, if that's sort of, if this union of X and Y here occurred. So then what about the next line here? Yeah, okay, that's one. But then we get a zero here. So that's not, you can't count that right. Then a bunch of zeros and then a one down here, one, one, one, one. So one more. Okay, then zero, zero again in line A, seven and eight. So we can just immediately skip those. 

And then we have line nine. Let's see. No. 

And then line 10. Yes, yes, yes, yes. No. So we have two out of 10. That's going to be the support here. It's also the probability of, so let me write it for the support. 

I'm going to call this X and this Y, all the X and Y in this case is two over 10. You agree? Yep. 

Good. Okay, and that is also the probability of X and Y happening at the same time. So we're looking at, you can think about them as discrete random variables essentially that, oops, that this one, this one, this one, this one, and this one occurred at the same time. That's the joint probability. Everyone agree with this and get this screen. 

Great. Then I talked about this that the, really what we're interested in is actually given that you buy X, this subset of items, what is then the probability of you all buying Y? Because you had the option of buying everything in this shop, but what's the probability of you specifically buying this particular item, given that you have bought X? So this is not the joint probability anymore. It's the conditional probability and there's a difference as you know. 

Okay, hopefully. So we are going to also call this joint probability a confidence and again, it's terminology, but essentially it comes down to what's written here. So the confidence of a particular association rule is simply, let's say, the support here, or sorry, it's actually not written as support, but the number of times that X and Y occurred together, the union divided by the number of times X occurred. 

So if you think back and thought about what is actually this, this count thing here. So if we do it like, divide here by n and by n, that's essentially the support for X and Y over the support for X. Or in other words, it's simply the joint probability of Y and X divided by probability of X. And that is essentially the definition of a conditional probability here. It's just Bayes rule or the product rule or whatever you want to call it. Okay, it's actually the product rule here. All right, so remember this thing here because it's going to save you a little bit of time later that I said it was the support of X union Y or the rule X to Y over the support of X. 

Okay, so maybe I should just motivate, why are we actually interested in both the, let's say the joint and also the conditional? That means the support and the confidence. So the support sort of indicates that all of these items, both X and Y are bought often. So it's sort of interesting for a shop, for example, to look at that condition typically. And then you're also interested in rules where actually if you buy X, then buying Y is very likely. Because that sort of indicates that okay, we can sort of perhaps notch people that if they have already put X into their shopping basket, we know they're going to buy a specific item. And again, as I said, if the confidence of something you really want to sell Y, it's very low, maybe you want to place it better in the shop so you get an association rule with a high confidence instead. 

So something like that you can use it for. But essentially joint probabilities versus conditional probabilities. Okay, so now we are going to, so just remember this one, this rule here, the support that the confidence is nothing more than the support of the union of the two things over the support of X. Because that might save you a little bit of time now. So now you're going to compute the confidence. 

And again, a few minutes here. So let me write it up. So we had the confidence of the rule X to Y. 

There was the support of X union Y divided by the support of X. Just think about it, we just compute one of these. Okay, so a few minutes for this. All right. 

Let's see where we go with this. So if you're a little bit smart here and remember that we already computed this one right in the previous question, that's the support of the whole association rule. So if you just write that up for completeness, so the support of X and Y what was it, two over 10 or something. Yeah. Okay, and then we need now the support of X. That's what we need in the denominator here. 

So that was, oops, give me a second. There was the support of X. So essentially now we need to count again, but we don't want to take into account the Y variable here. So that's this one. So now we need to count again. So let's do that. And again, same principle. So we look at this one, this one, this one, this one. Yes, definitely we need to count that one for the support of X only. And we had this one, no, this one, this one, this one, this one. Yep. 

And it's clear, of course, that the ones that sort of were part of the support for X union Y also going to be part of this. But now, because we're now not taking into account, so oops, the Y variable here, then you'll see that we also need to include one of these ones. I can't remember not this one, but this one, this one, this one, this one. Yeah. So we count three. So that was three over 10. So we have essentially that this is two over 10, three over 10, and 10 disappears. So we have two over three. All right. So a lot of counting going on here. 

Now, if you then just remember this counting, that was tedious for this small problem. And if you are in the business of finding, let's say, association rule, where the support is greater than something, because that's something that happens a lot, right? And we tend to be interested in those. 

And also confidence needs to be high. So it needs to be something, actually, that particular rule happens a lot. Okay. Then you can sort of think about what we need to do. And we've been through it here. 

You first need to compute the support of the X union Y. And that takes a lot of counting. So if you think about how much counting it actually takes, then look at this problem here. So here we've got how many items, if you look at the final ones, so A, B, C, D, E, five items. If I ask you now to basically compute the support of all of these possibilities, then how many would you need to, how many items sets would you need, or how many association rules would you then need to compute the support for? 

And remember what you did when you compute support, you went into this long list of, well, it wasn't so long in your case, but you would need to go into this long list of record and actually count co-occurrences. So it can take a long time if you're like Amazon with billions of items. So in this case, does anyone want to venture a guess how many item sets would we need to compute this for? 

You can count the number of blue dots, sorry, bubbles here, right? But it's something can be present or not present in that association rules. So it's sort of binary, we have two options for each item. 

So we get two to the mth. That's not too bad here. So m is the number of items. So that's not too bad if you only have five possible items in your shop, but it's a pretty, pretty tacky shop. 

You run if you only sell five items, I guess. If you're now Amazon, as I said, you probably have, I don't know, 100,000 of items or at least maybe even millions. So two to the millionth, that's going to be a problem. 

And it's going to be a problem in the sense we need to go into our database and look up the co-occurrences for all of these. That's what is costly here. So it's not particularly costly to find one of, let's say, support for one of these item sets here. But for the whole thing and for all individual ones, it's going to be a pain if we want to identify all the item sets that have a support greater than something. Okay. 

So what to do? So we're going to exploit something that it's fairly intuitive, I think, once you think about it. So it's a simple rule. It has a fancy name. But essentially, it's about looking at the properties here. So if we look at a, let's say this item set here, CDE, then if we know that that's frequent, you know that all the subsets of that item set must simply also be frequent. Because if you think about this, this means you're counting sort of co-occurrences of CDE in your table. If you're counting that this happens a lot, let's say a hundred times, then surely C and D must also be able to count to a hundred for that subset. 

Same for C, same for D. It's called a downward closure property. Okay. So we're going to exploit this rule in a bit. Because essentially, what we will do is exploit sort of the reverse of this. It's just an equivalent of it. 

And it says that if we then on the other hand know that D and E is infrequent, that means we can't count, let's say, to a hundred of co-occurrences of D and E. Then everything that contains all the supersets that contain D and E, they must then also be infrequent. Think about the logic here. Think about that table and actually identify it. So that's sort of a property that we can use now. Because if we start from up here and sort of start computing support for first all the sort of singleton items that's here. So individually for A and for B, C, D, E, we figure out, okay, E is actually frequent. Okay. 

We keep that one. But if you then go down and figure out that actually DE is not frequent, it means you don't have to actually compute support for all of these guys down here. So we are saving a lot of computation. And when I say a lot, it's sort of, you need to scale this to, let's say, a million items or whatever you have in your shop, right? But it means that we can essentially discard computing support for these guys. 

Remember, computing the support means going into your list of records and counting. So it's expensive also for a computer if you have billions of item sets. So this leads us to, on the surface, a fairly, let's say, nasty algorithm here. And, well, I don't think it's too bad. So I think let's walk through it and spend a little time on it. And I'll probably make a mistake here. 

So please correct me. But what you need to realize here is we're just exploiting this downward closure property. And we're going to do it in a way that we start out looking at, of course, well, the null set is not particularly interesting. If you go into a shop and don't buy anything, well, I hope you're not stealing anything, but at least you're not putting anything into your basket, right? 

Let's assume that's the case. It's not interesting. It might be, but, well, I guess you could count how many times people come into your shop without buying anything. So, sorry, it could potentially be interesting, right? But then we're simply starting out constructing the possible item sets and building this tree up. And then we're going to discard stuff that's not interesting. 

We know upfront it's not going to be interesting, meaning they'll have a support less than some threshold, epsilon. Okay, so let's just look at it. So I think I'm going to do it with an example. And let's see if I can do this without missing out. So basically what we're talking about here is sort of a set of records in transactions. 

Can you read this in the back? Yeah, hopefully. Okay, we have a set of n transactions here. And then we've set a threshold epsilon that sort of we're interested in all item sets with a support greater than that. 

We're only talking about support here. Okay, then a little bit of notation thing here. But so the first line here says we're going to construct a matrix, essentially, or set where we're going to take all the items J, where the support of J is greater than epsilon. 

So here J is an individual item. So let's see how it pans out. So I'll go going to go through this exam or here BD thing. Let's only do four, four items in your shop. So what is in L1? Well, if we look in L1, another way to indicate this would be to do something like this. 

Let's see if I can get this. So we have a, b, c, d, and we need to sort of consider all the individual, all the singleton items. So of course, I can indicate that with an a, b, c, and d. But I can also sort of use a matrix form here, which is a little bit more convenient. So I'm going to put in a one if that item is in that set or not, and then I'm going to put in a dot if it's not. So an item set with item A, okay, we get one for, what's it called? Yeah, for item A, obviously, and then we get a dot for the other ones because they're not in that item set. Similar for b, and we continue. This is sort of just a slightly more convenient way of denoting all the possible singleton items sets. But the key thing here is that I sort of assumed here that the support for A is greater than epsilon. 

Support for b is greater than epsilon, for c is greater than epsilon. That might not always be the case. So this is just one example. Then what do we need to do? Okay, then we have this sort of slightly nasty for loop here. 

And then what happens? Well, it seems like we need to construct yet another sort of set here. It says take whatever is in l, k minus one, we start out with k equals two. So whatever is in this one, let me consider all the sets here, we have four sets. And then we need to take the union of j and remember j is sort of a singleton item. 

So there are two things that are important. It must be in this one here, the set. And we can't sort of, it doesn't make sense to sort of add it onto itself. So a set with a and a. Let's just do that naively. What it says. 

So line four here, we get CK prime. Again, I'm going to do this notation here. So what do I need to do? I don't know how big this thing is going to be. Anyway, I'm going to take basically look at what's in L one. So let's start out with a and then I need to combine that with all the other singleton items I have available. 

Okay. So that would be a B that would be one option in CK prime. AC would be another option. Oh, see there. And then BC would be one because we had B in L one BC and BD. 

We also need BD, I think. Yep. Okay. 

And then we need CD, I guess, because see, see is also here. It's in L one and then we basically need to consider all or add all the singleton item sets to that. I'll take the union. Okay. So what do we get CD? Did I miss one? 

You like something missing here? Let's just take a look. Okay. 

80 is missing. Yeah. Good catch. 

I said I was going to make a mistake, at least one. So let's do 80 as well. And then we do, oops, and then we do the remaining ones. Are we done? I think so. 

Okay. So that was line four and that, you know, there's a reason why we have a computer for this in general. But so essentially what we've done now is just cannot construct another set here. So I'm just going to call that CK C two here. 

That's what happens in line five. Okay. So now we've got a collection of sets. We got multiple sets here. Now we need to move on. And remember, all we've done here is if you look at the tree is that we went from this one here. We only had four items, right? 

But then we constructed these sort of sets of two. Okay. Then what do we need to do? Well, what you could do here and I'm just going to give you a small hint here if what we could do here is actually just ignore line six to twelve. Because really, we end the game of computing the support of things here. 

So, so that's, let's just say for sake of argument, we're going to compute the support of all of these individually. And we can do that, right? We need to do that essentially. 

But before we do it, we in some cases we can gain something from pruning away the ones we know are not going to be frequent up front. So essentially all we're going to do in here, if you go line by line is that for each of sort of the sets here in or each of the rows in C two. And we're going to consider all the subsets of size where we are now K minus one. So in this case, we're going to consider all the subsets of size one. 

So what does that mean concretely here? We're going to look at what are the subsets of NB. It's A and B. We're going to look at those and then essentially remove the ones that are not frequent. That means if one of the subsets is not frequent, we can remove that whole line. 

That came from this downward closure property, if you remember correctly. So if any of these subsets, if A is not frequent, then let's remove the whole thing. So we can do that and then maybe potentially spare ourselves computing the support down here, computing support of this guy. 

And here, of course, we assume in that step 13 is the expensive one. Okay, let's see how that pans out. Just give me a second. 

What happened here? Okay, great. So let's assume, and actually we know this up front, what will happen in this 6 to 12 for this example? 

Because they're all frequent, right? So nothing is basically going to happen. So I'm going to make sort of an assumption here that we're going to look at an L2, construct L2 here, this one here. That's where we're going to keep all the ones from this thing here after pruning. 

Nothing happened when we pruned here. Then I'm going to remove, sorry, then I'm going to only include the ones, the item sets of size two that are frequent. That's after line 13 here. 

So what will L2 look like? Well, now I need to sort of again sort of come up with the example on the spot here. So I'm just going to say that, let's say that two of these item sets in C2, once I've computed line 13, it turns out they're not frequent. 

So let's just for sake of argument say that these two here, they're not frequent. So they will not be part of L2. So I made L2 a little bit too big here. But essentially, we need to sort of do something like this. So we had, whoops, not a Riklin. So we need to construct L2 and that is a little bit boring. 

So that's going to be these ones that I sort of assumed are frequent after having evaluated. Okay, then what? Then I need to increase K and then run again. Okay, so let's do that. 

And now again, it becomes a little bit complicated as you see. So we start out with C3 prime. There we need to take the elements in L2 and then add the singleton options we have. So I need to take AB and then look at the set C, C and D, where I add them individually. So to be clear, right? So we take A, B, C, D. Then we take A and B and then we add on C. Okay, and we do the same for A and AB and D. And we do this. So that was the first line that we sort of augmented with these singletons. And then we take line two here. So this one, and then I consider A needs to be in there. I need to consider an option with B in there as well and then one with C and D. But I already have that. 

So let's not include it twice. Okay. Okay, and then I'm going to look at the one that has A but not B but C and then D. I don't have that up front, right? 

Nope. Okay, so now I'm at sort of this third row here. I'm going to take, it needs to contain A. 

And then I'm going to consider the one where I add B and then C is not there and D. Do I have that one up? I already have that one. So let's remove it. Okay. And then the one where I add C. Do I have that one? 

Yes, I already have that one. So actually I'm not going to do that. And then what about the last line here? Well, I'm going to consider the one where I add A, but as you can see that would result in something similar to the first row. Not going to add that one. And that's the one where we have B, C and D, which I don't have. So I need to add that one. I think that's correct. Let's hope it's correct. Now the question is what happens next? Because now something interesting happens here. 

So remember what I did here. I need to look at all of these rows individually. So I take this one and for this one to keep this one, all subsets of size 2 in this case, we have k equals 3, all subsets of size 2 need to be frequent. But what does that mean? It means that they need to be in L2. So now I'm checking. I need to check A and B. Are they in L2? Yes. 

We have this item set here. A and C in L2. Yes, they are. Okay. What about the next one? 

A and B. Are they in L2? Yes, we just established that. 

B and D in L2. No. Okay. So this one disappears now. 

We pruned it away. So assuming that line 13 is really expensive, we just saved one computation of line 13 now. So basically what I'm going to do is just, I'm going to rename C3 prime and to rename that C3. 

So it's the one we are updating and then we put a red line through the ones that we can already prune away now. Okay. Let's look at the next one. Here we need to know that A and C needs to be frequent. 

They need to be in L2. So A, C, do we have that one? Yes, we do. Do we have C, D? 

No. So that also goes away. UB, we saved another computation of line 13. And then you can see where this is going because C and D, we also have that one there. That one also goes away. Okay. So go through this manually at least once in your life. Okay. But how does that save us? Right? 

It means that C3 basically only contains, only contains one item set of the contains three items. So this one up here, this one. Okay. 

So now we can compute what's actually the support of that one. Okay. And if we assume that that one, well, we essentially, let's assume that that's actually frequent. Let's do that. Then we get an L3 here. 

Oops, not blue. So I'm jumping to the next line. So we get L3 down here. It essentially, let's assume that this one is frequent. Again, that's an assumption. Okay. Okay. Then we need to reiterate this. And essentially what you can do now is that you can sort of imagine that this is going in that, okay, now we go up here again and you add all the singletons to this one. We get an item set of one, two, three, four. Yeah. So C, oh, C4 prime would need to be one, one, one, one. 

They're all in it. And let's assume that that's actually not frequent. That means C4 is going to be empty. 

And now we're done. That seemed like a very complicated operation, right? Just to save a few computations. But you need to sort of have, keep in mind that this line 13 can be extremely expensive if you are like needing to go into all Amazon's records and count stuff. 

That's going to be expensive regardless. So what we've done here in line six and 12, we use what's called the a priori algorithm to remove item sets such that we don't need to compute that, need to compute the support in line 13. Keep in mind that the only rule we used here is that for an item set, let's do this one up here again. 

For an item set to be interesting, then all of size, item sets of size three to be interesting, all the subsets of item sets, let's say K minus one need to be frequent. That you need to use in the next small exercise here. So let's combine this with a break. So let's do 15 minutes. 

So come back five minutes past two. All right. Let's see. Let's see how this works out. So how many managed to actually sort of figure out a way to solve this. I'm not asking you what your salt is just did you feel like you were able to do this. Okay, that that's okay. 

I see some nodding and hands and stuff. So essentially what we're asking you to do is not go through the full algorithm. You can do that. I'll show you that on the next slide. But remember this, this basic property of the downward closure principle where, let's say if we know all the item sets of size two, that are frequent, they must be in this L two thing here. So they're indicated here, right? 

Then we asked to see so which one essentially can be ruled out based on this information. That means item sets of size three. So here we need to use the rule that okay, in tuition that all the items that are size two must be frequent point to make sense to evaluate this one. Okay, so so that's sort of the quick way to solve this without going through all the painful putting in ones and dots and zeros and not so let's just see what happens here. 

And remember this is a little bit nasty here. It's negated the question. So again, when I see questions like this, I would simply, okay, let's work out the option for all. Let's work out if we need to compute all of them. Or, you know, if all of them needs to be computer and then figure out which one to do and then need to answer afterwards. 

So I'm not sure that makes sense. But anyway, let's see, I would go through option a, I would need to identify all the possible subsets of size k minus one. So that means size two. So what subsets, let me find another color. What subsets of size two do we have? Well, we have if two, if three, if two, if four, and if three, if four. 

All right. So how do we figure out if these subsets are frequent? That means that support greater than epsilon or equal to epsilon depending a little bit on setting. But let's see, we need to go. If it's frequent, it must be in this one over here. It means the most co occur in a way we noted it over here. So can we find a one row where if one and if three co occur? Sorry, if two and if three, sorry. Let's see. 

Can we find that? So if two here, not if three, if two and here. Yeah, great. 

So it's frequent that subset. Next one. So if two, if four, we go down here eyeballing if two, if four. So there's a line here. 

This one is also frequent. If three, if four, there we go. One row there. 

Great. That means all three. Well, I don't know if it's great, but it means that all three subsets of size two are frequent. That means I can't rule them out. I can't rule out this item set of size three just yet. So now I would need to go to line 13 in the code and then actually evaluate the support. 

That means looking up into my huge collection of records. So it turns out if you now go down here and do B and C, then you actually also need to evaluate those. So the tick mark means you need to evaluate those. 

They're frequent. All the possible subsets of size two. Then what happens in D? So maybe we can just do quickly. We need to see that if one and if three, do they co-occur over here? If one, if three, actually no. We can't find that, right? It means that that subset, if one, if three, is not frequent. 

That means the superset here of size three is not frequent either. We can throw it away. We save computing that line 13 in the algorithm before. Okay. So the correct answer given that it's negated here in some sense would be that you answer D here. This is the correct answer. Make sure you get this intuition because it's going to save you a lot of time instead of having to do this. Then you would have to write up the whole thing here and go through it painfully and see. So we are using the intuition here to solve it quickly. That doesn't mean you shouldn't actually go in and look at this and see, you know, can I do it both ways, potentially? 

All right. Then we talk about, so far, if you look into the a priori algorithm here, we only look at the support sort of and not the confidence per se. But as it turns out, you can do something very similar for confidence. So if you look at sort of, let's say actually computing the confidence, let's say, of this rule. So if we know that this one, again, we construct a sort of a tree here, and if you know that this one is low confidence, it turns out that these must also be low confidence. And I think maybe we can maybe write up a small thing to illustrate that. So remember the confidence is sort of the union of X and Y. So that was your BCDA. 

Let's write that up. Nope. Go away. No. Okay. So the confidence, what was that? 

It was the support of the union of the two. So let's write B, C, D, A divided by, for this thing here, divided by BCD. And we'll be a little bit more precise here. There we go. 

So it's the support of BCD. Okay. If you now look at maybe this one, so it's basically the same, right? 

We look at the support of CDBA. Just the permutation of the ordering is still the same iPhone set. Okay. And what do we have in the denominator? 

We have AB, for example. Actually, let me do another one. That's a little bit easier. Let's look at this rule over here, where we have B and D on both sides over here. So if you look at B and D, it's on both sides of X here. 

So let's do BD here. Right. To be clear, this is the confidence of this rule here. This is the confidence of this rule here. 

Now can we say something about the relationship between them? Remember what we said about the downward property? It says that basically if, let's say, an item set of size three is frequent, then all the possible subsets of size two must also be frequent. Set in another way, it means that if you look at a support of BCD, that's what we have up here, that must be lower or equal to the support of BD. 

Okay. So if you look at that, if this, what's it called, denumerator here, is bigger than this one, means since what's in the numerator here is the same, means that this one must be smaller, means that this value here must be smaller than this one. And if you know that this one is below a certain threshold, this one must also be below a certain threshold. So it's just a relationship here between support and confidence. So again, it's just, we're not going to write up the algorithm here for the a priori algorithm for confidence, but it's just to say you could do similar stuff for the confidence. 

Okay. Anyway, so let's see sort of what you could potentially extract from a small dataset using these techniques here. So again, we're in this domain here. So we're looking at various items that means combination of things. How many times do they occur? 

Essentially, we're counting it happens 40% of the time. That's the support. Okay. Then we can look at specific association rules that has, let's say, a high support and a high confidence. So the support means that actually often you buy soda and milk. And actually, as it turns out, every time you buy a soda, you're also going to buy milk based on this dataset. 

It could also be that, you know, if you put nothing in your shopping basket, then you're very likely to only buy milk, essentially. So we have a high confidence for that as well. So this is a matter of sort of saying, okay, we need something that, you know, the support indicates these items are often bought together. The confidence says that you have this conditional probability that one, when you bought beer and milk, you're also going to buy diapers with high probability. You're not going to buy soda, right? We don't have a rule that says, be a milk soda. 

It must be low because it's not in this sort of high confidence list of things. So that's sort of interesting that, you know, if we know, look at you and say, you bought beer and milk, then we know you're going to buy the diapers. Okay, maybe we could switch diapers and beers in the shop and see what happens. Would that change anything? Is it just an effect of, let's say, the placement of the items? So that's this. Remember, there's, let's say, the interaction between the joint probability and then the conditional probability. 

All right. This was for sort of a simple, discrete dataset where we have discrete items. But most of the time we've actually, well, at least a lot of the time, we've also worked with continuous variables. So can we do something similar here? 

Simply looking at core occurrences of things. Well, the answer is yes. But what we're going to do is transformation of our continuous variables. So I think this is, it is actually from the Iris dataset. So remember, we're measuring geometry of leaves or what was it? So we measured something. We had three attributes. 

So now we're going to do sort of preprocessing step now. And you'll often see that. I've been sometimes do that where we say, okay, if I plot sort of the density of the first one or histogram, then I get something like this. Then we're going to look at the median. 

Let's say it's roughly here. Then I'm going to say that if you're below the median, then I'm going to create an attribute. That's one, if that's the case. That's a binary attribute. 

So in this case, I'm below the median as it turns out. So this gets a one. But I'm also going to create another attribute, which is actually says it's going to be one if you're above this one. 

So that's another attribute now. The reason why we need to do this is that all of these association rules things are only about core occurrences, not the absence of things. So we're counting only ones, essentially. 

So that's why we need, essentially, for attribute one, we need one that indicates if you're below and we need one that indicates if you're above. And so we're creating this binary matrix here. And so basically all we've done is turn the continuous data set into a discrete one by quantizing it in some quite specific way in that we, for each variable, we actually create two new discrete variables. Okay, but that means that we can now look at the core occurrences between things. 

So what actually happens? And remember, well, we take our data set and we convert it into this binary matrix, essentially. And now we're running our a priori algorithm or essentially extracting, let's say, interesting association rules. So as it turns out, what we get here is that it turns out once we have a low petal length, that was one of the attributes when we have a low petal width, and we have a certain class. So here we actually included a class into it. Then it turns out that the conditional probability of being having a low C pooling is high. 

It has a high confidence. So every time this happens, co-occurs. This happens all the time as well. We have a high confidence here. But how often does this thing here happen when you look at the whole data set? 

Well, it happens one out of 33% of the time, as it turns out. So you can extract these rules. And remember, these are not string rules. These are extracted in an unsupervised way. We haven't used this class, let's say, for example, down here. This is sort of a classifier kind of thing, right? And it was simple sort of, let's say, an array-based classifier to some extent. 

But we haven't actually trained anything. It's just looking at the co-occurences of things. So maybe this one would be sort of the most familiar one. Okay, if you can say that the C pool length is low, width is high, petal length is low, petal width is low, then it turns out you could. There's a high confidence that you are this particular class of Irish law. Okay, so this is a simple way of coming up with these simple classification rules. 

Without actually training using the label. We used it in the sense that it's sort of part of it, and if it co-occurs with these other things, yeah, okay, then we extract the rule because it might be interesting. So this one is maybe sort of a, we're not going to do the full thing, because it takes a long time counting, and we counted enough for today, at least I have. So I'll maybe highlight a few things here. It might be sort of a typical exam question in some sense that you have to figure out. Find all the items sets. And basically now you have a lot of items, possible items here, and you have items that are size two, three, four, and so on that you need to consider. So now you need to find all the items that support greater than a certain threshold. And what I just want to highlight here is that be a little bit clever here and say, okay, we've got 10 items. So what would it mean if you have support greater than 35? 

It means you need to be able to count more than four, or yeah, four more rows here, where something co-occurs. So if we just do the simple ones here, we can take the item sets of size one. That's relatively easy. You need to count how many times did we buy juice? So one, two, three, four, more than four more. 

Great. Counting one. Milk. One, two, three, four. Yeah. Beer. 

One, two, three. No. So don't count that one. Cheese. One, two, no. Chocolate. One, two, three, no. 

One, two, three, four. Yeah. Yogurt is in there. 

You get the point. Now you need to take, you need to count all of these. Now you need to consider items that's of size two. But of course you don't need to enumerate all the possible item sets of size two, because what did we just do? For cheese, we actually excluded that one. 

So all the item sets, including cheese, you can already rule those out. Okay. That's the trick here. That's what we did with the previous quiz as well. So I'm not going to go through it. 

But maybe try to do it and see if you can get the right results here. So in this case, there are 11 item sets. So remember what are the number of possible item sets here? 

We had one, two, oh, doesn't help me pointing. One, two, three, four, five, six, seven, eight, nine, ten. What's two to the tenth? 

One thousand, twenty-four. Something like that. Yep. Must be right. Something like that. A lot. 

And we don't want you to enumerate a thousand item sets and count stuff. So you really need to use this downward closure property here. Try to do this and I'll upload the slides immediately and you can see if you get the right answer. Great. So that's essentially all the material has now been covered in this course. So that was easy. So for the rest of today, we encourage you, of course, to do the exercises for association mining or catch up on what else you've been on to. Just a few more minutes, please. I don't know if George has already uploaded the exam set yet in mind as a sort of mock exam. 

Let me check. No, not yet, but he will send out an email in, I don't know, half an hour or something with a new exam set you haven't seen before and possibly some hints about which questions might actually be interesting for the exam. So have a look at that one for next week and actually try to do it without using your notes or only the four sheets of notes that you... oh, what is it? Two sheets, both sides. 

And then see if there's something. Am I missing something to be able to solve these questions in particular? Then there's also a post-test and you shouldn't see this as a sort of a test exam. This is more for us to see, okay, these types of questions you got in the pre-test. How would you actually do on those types of questions now after the course? 

So try to do that and I believe George will probably bring it up next time. These are not exam questions. These are sort of more probing general aspects of the course and topics. Then next week George will be back and he'll do a recap of the course and material and of course you've already been given, or at least in half an hour or something, sort of what constitutes a mock exam. And of course there are also these tens of other previous exam sets that are highly relevant. But of course disregard the questions where you can see, okay, I need Python to even have a chance to solve it. 

Don't necessarily solve those without using Python. Then for today, we sort of split the TAs a little bit so that some of them will be here today. Some of them will be here next week as well because usually that's not the case. And then if you actually have questions, I've noticed surprisingly that Piazza is very quiet for some reason. 

I don't know if you don't like it or whatever it is, but for the exam it's very, you know, we hope to see more activity there where you help each other out answering questions about previous exam sets and whatnot. The TAs will also monitor that. But yeah, that's pretty much it. But feedback, yeah, project two, I have no idea, George is on that. I guess you'll see that definitely before the exam. But that's it for me. So good luck with your exam because I won't see you before that. 
All right. 

Speaker 1: All right. Guys, folks, I should say, so welcome to the final week in machine learning and data mining. Congrats on making it so far. Today is going to be a relatively informal lecture. I'll take whatever questions you might have during the lecture and about the exam and so on. But before we get to that, I'll recap sort of just the very overall sort of concepts we've covered. And I didn't receive any suggestions for topics, so you're going to get the very bare minimum lecture today. So let's see if we get a need a break at some point and we'll do one. 

Otherwise, I'll just continue until you wake up, I guess. So there is a feedback group of the day thing where you can give a feedback on this particular lecture. I don't think I've enabled that, so just come see me afterwards if you have suggestions for what we could do in this final week in terms of content and stuff. And then, of course, the whole lecture will cover all the many chapters of the book. Of course, it will not, but that is essentially what we've covered so far in the course and I'll recap that briefly. I think the, I don't know, but the official relation from TCU is perhaps still live. 

I don't know. I took a look a few days ago, but go in there and give you feedback on the course and whatnot. But we've at the end, so week 13, we've covered all the basics. We've covered supervised learning, we've covered unsupervised learning, and as I mentioned today, a very short lecture on what we can, on sort of the general aspects and, of course, importantly, the exam and the format of the exam. Good. So not any new material per se, but we'll just remind you of a few things and then cover some practicals about the exam. 

Good. So if you think back to the very first lecture, I showed you a couple of pictures of Turing and Samuel and a few other people. And this slide is essentially just to remind us of what is machine learning, sort of in a conceptual form. And that is what Turing, a bunch of years ago, he actually defined or started to define, I would say. 

And his idea was that we should basically learn like children do. We should have machines do that. And effectively in today's sort of world, that means learning from data with more experience. You should become smarter. You should be more precise and so on. So Turing thought about that many years ago and wrote that in a very famous paper by now. 

And I think you should read it if you haven't. It's sort of interesting to see that the thoughts sort of, that we sort of implement in practice today came about, let's say, 80 years ago or something like that. But of course, it wasn't very operational in the sense that you could now go do some, let's say, implement these algorithms. And that came about a little bit later in terms of Samuel who sort of defined it a little bit more formally, saying it's the field of study that gives computers the ability to learn without being explicitly programmed. 

I think this is operational. So it means if you have a computer, you should not program it explicitly. You should not write down if then else. You know, if pixel 27 is red, then it must be something red. 

And if it's green, then it must be something green. We learned that from seeing a lot of examples from experience and having some notion of a performance measure so we can see how our computer, our algorithm does. And of course, by looking at that, we actually update our algorithm and we've seen a bunch of methods for doing that and a bunch of models where we have parameters that we can update based on experience. And then Mitchell defined that in terms of a little bit more sort of strict definition down here that you can read if you want. 

Okay. So what can we actually use this machine learning idea for that is something that learns from data? Well, let's look at some data and in this and sort of see what instances that arises and one of the instances is supervised learning. That's where you observe pairs of X and Ys. In this course, X is typically a vector and Y is either a continuous variable or a categorical one. So it could be binary 01, blue, red, or it could be, let's say, a multi-class problem where you have multiple colors, for example. 

Okay. But the key ingredient here is that you've got pairs and X and Ys. X in this course tends to be vectors. If you think more broadly about what machine learning is being used for today, X is not just a vector or tabular data. It's graphs. So from molecular modeling, it's time series, it's images, it's, let's say, complicated geometric structures like 3D coordinates and molecules, for example. So X can be anything and in your domain, whatever you're studying, this might be something very, very exotic. 

Okay. Whereas in this course, we sort of took the canonical case and said, okay, X is typically a vector and we know we have functions that can operate on vectors. This is the abstract function F shown here. It has parameters. It can take X as an input and it can output Y. 

Okay. So the whole name of the game here is to then figure out what is F, what should F be? And we spend a lot of time on that in the supervised setting. So we had everything from decision trees, neural networks, K and N classifiers, regression models, and whatnot. 

And I'll get to that point in a bit. But generally, think about this as an abstract function for now. The key property here in the supervised setting, we want to map from X to Y. So that sort of assumes or presumes that we have these pairs of observations we can learn from. So you actually have to observe something relating to your X, you have a Y. What happens in this unsupervised learning setting where you have a slightly, let's say, different personality over here at least, a little bit more exotic maybe. That's where we don't actually have any supervision. Anything can happen, right? 

Well, in this case, we actually still have X. We've observed something about the world and now we want to find some structure in that data. That structure, the natural thing to think about here would be clusters, for example, groups of data points that are similar to each other. They're more similar to each other within that cluster than there are to anything else out in the world. We can use that for basic density estimation. 

We've seen that. Of course, in mixture models, we've also seen association mining, which is slightly, let's say, a variant of unsupervised learning in where we are interested in extracting association rules based on co-occurrences of things of observations. Okay, so these are the two canonical settings in the machine learning domain. There's the third one. I'll come back to that later when we talk about what other courses we might have at DTU. These are the things we've covered in this course. Of course, we also spent quite a long time on the foundations here. 

That means computational linear algebra, whatever we call it, and probabilities, statistics, for example. Hopefully, all of that is the foundation for being able to do what we did here in the later weeks. Good. If we just go through this data set again, the famous one with the iris flowers, again, you are the botanist. You measured stuff, geometric properties about flowers, and importantly, you also measured the class for that flower. So you asked an expert in flowers, what is the class of this particular flower? 

You saved the x coordinates, or the x values, and you saved the y value in that case. Good. What can we do with that? If you just think about some of the examples of our supervised setting here, then we have the x that will be the, let's say, all the geometric properties of the leaves, and then we have the y that would be the type of flower, so a class, and was it three different classes we had, something like that at least? Yes, that's at least what we are talking about here. Then what we could do, well, we could visualize all of this data. 

We saw that in the earlier weeks. We could take two of the attributes, sql links, sql width. We could do a scatter plot here, and then we color-coded by the class names, and then we built a machine learning model. And of course, now we have to make a choice, because we have our x, we have our y, and maybe you need to standardize that and so on and so forth. 

It depends a little bit on your method, of course. But we sort of have a choice to make here about f. What should f be? This abstract function. Yeah, it's something that takes x and maps to y, but what should it be? So in this case, I've taken a linear model. So essentially, the logistic regression model, the multinomial one, and I believe George has told you about that. 

This is where we have three classes, the purpleish, the greenish, and the yellowish color here. And then we build a model. And the model here is a particular parameterization of this function. And here we use the linear, let's say model, and we input some transformed version of x. 

So what I've done in the first plot here is sort of augment x, if you want to call it that, or transform x. And then I include features like an offset, x1, that would be sepal length, and x2, that would be sepal width. And then I put that into my multinomial, let's say, classification model, and then what comes out? Well, if I visualize what comes out here in terms of the decision boundaries that arise in this model, well, what we get here are these straight lines here. Okay, so in this basic form, this model can do straight lines as decision boundaries. And if you're over here, you belong to the yellowish-blueish one. If you're in this area here, you're green, and over here you're yellow. 

And importantly, this model is trained on all the data you can see here. Okay, but we get those boundaries. Are they good or bad? We don't know. Well, we can sort of see that some of them are misclassified, right? We can quantify that at least. 

Good. Well, if I sort of play the game here and say, well, this is too simple. These linear decision boundaries, I need something more complicated. What can I do in the linear model? So when I say linear model here, I mean something of the form. So that one. So it's the form like this. 

So you multiply a weight vector with an X, and typically X tilde, where we transformed, where we transformed, let's say, our original features, C++, C++ with, where we transformed that. Okay, so good. So that's the basic form. But nothing prevents me from actually expanding my features beyond what I have here, sort of in the basic model. I can expand that. So that's what I've done in the middle. 

So what I've included here are terms of this form. So I still have one, that's the offset, still have X1, still have X2. Then I have X1 squared, X2 squared. And then I believe I also have the interaction term. So X1 times X2. 

So now I have a vector not link three, but links one, two, three, four, five, six, something like that. What types of decision boundaries can I do within the linear model and the multinomial setup here? Okay, I get something that still looks, let's say, linear here, but then I get this wiggly line here. So by expanding my features or transforming them, I can actually get something more complicated in terms of the complexity of the decision boundaries. 

Okay, I can continue this game, obviously. I can make more and more complicated models by adding more and more expansions of my original data. So in this case, the last one on the right, I get one X1, X2, X1 squared. Oh, maybe I should just do it like this. So I get whatever I had over here, plus something or more. So whatever I had before, something like this. 

And then I also take the third order terms here. What can I do here? Well, I can get more and more complicated decision boundaries. And of course, I can continue this game, and eventually I will come up with a function that can actually separate all the points correctly. We know that's not the way you should do this, so I'll come back to that later. 

Bear with me for now. If we look at this from sort of a regression point of view, the only thing that has changed, we're still in the linear modeling sort of domain, the only thing that has changed is that y is now a continuous variable. So in this case, we have petal length as the outcome of, that's y, and from sepal width, sepal length, I want to predict petal width. 

Oh, sorry, petal length. Okay, what can we do? Well, similarly as before, I have some inputs. That would be these two variables, and now I want to predict the output. Well, if I have a model of this form here, so in this case, I have the exact same features as before in the simple one. And then my model, maybe I should write that up here, is essentially just WT times X tilde. 

Okay, what types of functions can I do here? Well, we know this from sort of basic calculus. What we can do are hyperplanes, in this case, in this 3D space. Okay, so that's a simple function. It's just a plane. 

So for every value of sepal width, sepal length, I can plug those in and I can evaluate what does this function tells me the output should be, and they happen to all lie on this plane. All right, what if I then do the same as before? So I expand my features like this. 

What can I do now? And now we see we actually get sort of a more complicated surface here. It's not just a simple hyperplane anymore. It's actually some complicated surface in this 3D space. All right, so maybe that fits the data better, or it does, in the sense that the distance from the blue plants to the surface, if we take all of the distances, square them and sum them together, that's lower in this case than it is in this case here. 

But again, nothing prevents me from continuing here, so I add the third-order terms. What do I get? I get this really, not really, really complicated, but even more complicated surface. And again, going from here to here will lower the squared error, or the mean squared error. Okay, good. 

Well, we've done that. All we had to do was choose a function f and fit the parameters. Of course, do the transformation. We write up the model, we fit the parameters, and in this case, we can do that in close form, but that's maybe not the important point here. But same as classification, roughly speaking, right? 

Going from x to y. What about in the unsupervised learning setting? So if you don't happen to have a botanist at hand, when you actually try to label the flowers, and you forget all about the labels, then you still have some information about the world, because you've measured a bunch of flowers, the leaves, and you've measured the seedlings, seedle width. You can plot that. We know by now how to do a scatter plot, so we can plot them. Eyeballing this over here, you sort of see, well, maybe there's a little bit of structure, and then maybe there's a sort of a group over here. 

There's potentially a group here, and I don't know, maybe you can separate it somehow. But anyway, what we can do as sort of machine learning people, well, we can build a density model. What is a density model? Well, it's nothing more, nothing less than actually modeling p of x. So of course, we know that this p of x thing should integrate, when you integrate over x, it should integrate to one and all of that. But it's sort of description of how dense is the data in that area of space. And we know a bunch of densities we could use here, another bunch, sorry, that's wrong. 

We've actually only looked at two sort of fundamental ones. We've looked at the Gaussian, the multivariate Gaussian. We looked at the beta distribution. And when we're talking about densities in this case here, what the only thing we covered is the Gaussian model, or our combinations of Gaussians. So in the simplest case, what I want to do here on the left is to build a density model using just one Gaussian, and now x is a vector, it's a 2D vector. And it has parameters mu and covariance, sorry, covariance given by the matrix sigma. 

So a 2 by 2 matrix. What can we do with a Gaussian distribution? Well, we can build one bump. And if we try to maybe just look at the data up on the top right and figure out, okay, where would this mu thing be for this data? Well, we need to locate the mean, roughly here. Then we need to look at the covariance, that's the tricky part. And how would that sort of look like if we actually were to draw the contour lines? 

Well, there doesn't seem to be any covariance along here or here or anything. So I would guess, my guess would be that there's sort of roughly a circular, something like this. And that's what's essentially shown down here, in that we fitted one big Gaussian to the whole thing, but we know that's not a very good model here, because as we alluded to, we have basically some gap in here, but there are no points. And the Gaussian would actually tell us there should be quite a lot of density here, but there isn't. So maybe it's not a very good model. 

What to do? Well, we've learned how to actually build a mixture model. And if you remember that, it's essentially just adding together the densities for individual components, or Gaussian components. So something like this, where we had a weight for each component, and then we had the base density, or in this case, it's a Gaussian, and we had a specific mean for each component, and a specific covariance for each component. And by doing that, we can have more than just one big Gaussian for the whole thing. We can place, in this case, we can maybe place a Gaussian here, one here, and maybe one here, or something like that. So now I have three centers, I have three covariance matrices, that all in all seems to somehow better fit the data, at least in this case. 

But as before, nothing prevents me from going nuts over here. Just adding, I can't remember how many we have here, eight or something, six or eight or something like that, Gaussian components, and that seems to fit the data at the blue points even better. And they do. In this case, I think we have 150 data points. So what happens when I then increase the number of components to 150? I get one component per data point. The variance goes to zero, so I basically get a delta function on each of the blue points, so I get infinity. That is fitting the data perfectly. For those data points, I basically know exactly what's going on. Of course, it doesn't generalize. 

What to do? Well, the whole thing here, and a key element of what we've been trying to tell you, is that machine learning is not just coming up with a function, finding the biggest and greatest neural network, or latest and greatest, I should say, neural network and fitting it to data. It's a lot about also controlling model complexity and the evolution part. So we have these three cases here where, well, I could choose, I know a lot of functions, so I can choose one function, and I can make it arbitrarily complex. I can do that in the classification, regression, and density estimation case. But the whole name of the game here is to make your model, as Einstein would say, or Occam's racer here would say that, well, the model shouldn't be too simple because then you can't fit anything. You can think about the simplest regression model you can think of. It's just a constant, basically, or even just a constant at zero. That's a model. 

It's not a very good model in general, but that's a model. It's too simple. Well, I can turn out the complexity a little bit so I can do hyperplanes, like we saw, or I can even add a second order of terms to get something that's sort of medium complexity here, for example. But the basic principle of actually having to control model complexity is key in machine learning. Good. 

So we spent quite a long time on that. And the reason why we do that is that the Holy Grail, so to speak, of machine learning, is to minimize the generalization error. So how did George's define the generalization error? 

He probably said something like this. It's the performance on an infinite set of on-scene data. So I don't know about you. I don't have an infinite data set. I certainly don't have an infinite data set of something on-scene, because if it's on-scene, I don't have it. 

I haven't seen it. So it's this sort of the same magic thing that we are wanting to actually minimize, namely the mistakes you're going to make on future on-scene data. Okay. Well, we could give up here and say, oh, sorry, we can't do that. 

I know it's a theoretical concept, so let's drop it here. But of course we can do something because we do have a data set. So in this case, we have a data set. 

Oops, that wasn't particularly pretty. So in this case, we do have a data set. And that's something we've observed. So let's play the game and leave out some of the data for testing. So we're not going to fill around with our parameters on that data. We put it in a safe somewhere. We don't touch it when we try to figure out what the parameters in our model should be. So you saw a bunch of different cross-validation setups. 

So we had holdout. That's the simplest one shown here. You had k-fold cross-validation. That's where you split multiple times. So you get five different training sets. Not totally different because there's overlap, but you get at least five different test sets as it turns out. So you're testing on your data at least once. Then you have the leave one out setting. 

That's where we leave out one data point at a time. Train your model. Evaluate your model on that one data set. And then you continue until you iterate it through your whole data set. If you don't have a lot of data, that's fine. If you have a billion data points, that's not fine. So that's why Google and whatnot, they're not doing that. 

I'm presumed at least. But anyway, the cross-validation here gives us a way, a practical empirical way of estimating the generalization error. Just have to be a little bit careful about how we're doing it because it's basically just a statistical estimator. 

So the question is for the people on Zoom, is what do you actually do in practice, let's say, in a company when you want to deploy a machine learning model? And that's a very good question. No one really knows, but you can try to read some of the latest papers that are coming out. I would hope, but we don't know exactly, right, because they tend to not tell us. We do know a little bit about what they've done on META because a previous graduate from TCU went there to be a very senior engineer, I should say, leading the engineering team there. So we know a little bit about what they're doing. 

He was educated here, so he did it the right way. So they do have test sets and they do cross-validation and things. Of course, the way we present a cross-validation, double cross-validation, now I'm jumping ahead a little bit, is that you actually end up with this ensemble of models, and then in principle you should evaluate, for every test point, you should evaluate the ensemble if you're hoping to get the performance you're estimating. Of course, they don't do that. They actually pick a particular model based maybe on experience or some other way of ad hoc picking a particular model. But they certainly have measures or the same ways of selecting model complexity similar to what we've done here. Of course, they're not doing it as rigorously. They're probably not doing 24 cross-validation on their billion point data set with their billion parameter models, right? But this is sort of a continuum. 

It's a scale. Sometimes you don't need to do the very rigorous cross-validation. You definitely always need to have a test set, but maybe you don't want to do the cross-validation all the time. 

So the question is, is there a theoretical optimal way of choosing the number of folds for a particular problem, I would assume, you mean? This is very much, and I think George might have told you this, this is very much actually still an open research question how to do that. So if you read about set up two in the book, you'll see all the considerations you need to take into account here because it matters also about the statistical properties of your estimator. 

So is it biased, unbiased, what's the variance of that estimator? That goes a little bit beyond what we're doing in this course. But the answer is generally no. 

There's not a optimal way to set it because it's a compromise between bias and variance on the estimated generalization. There's a reference to the papers if you have a look, and we can talk about it later, but it's a little bit beyond what we do here. So it often comes down to practicalities. How many k-folds can you afford in terms of computational, let's say, resources you have available? Maybe none, because again, and maybe it's not necessary to do it if you have a lot of data. So this compromise between the size of your data set and the amount of computational resources you're willing to actually set aside for doing this, it is a, let's say, black magic, so to speak. It's a compromise. It's a trade-off between a lot of different things. So this is, to some extent, the difficult part of doing machine learning. How do you make a hard decision? What should k be in your k-fold cross-validation? Well, you can't sit around and thinking about this forever, so you do need to make a choice, let's say, 10. 

The important thing, and hopefully that's what's been clear from this course, there are some consequences about making that decision, and we talked a little bit about, I think George has talked a little bit about, what are the compromises here? Okay, about how well does it actually capture the variance in terms of the test set? You can expect how much variance do you actually capture in terms of your training set. So what would happen if you pick another training set, do you actually capture the variability in your estimated utilization? I think we're going down a rabbit hole a little bit, and it's super interesting, but let me just maybe bring it back and say, we do have a way called cross-validation to estimate the denonization error. 

Cool, good. So we can do that, and if we do it here on our data set up here, and maybe it's difficult to see, but some of the data points are now crossed, there's a cross in the middle here, it means that we're not using those data points for actually testing, or for training our parameters. So in this case, what we've done, what we've done here is just do the whole out method, so we have a training set, these are the ones without any cross in it, and we have a test set, these are the ones where you see a cross in the middle. 

So what happens? Well, nothing really changes except that of course, we don't have a lot as many data points to actually train on here, but we can train the model and we get results, we still get in this case certainly linear decision boundaries, what happens over here, well, we still get some wiggly stuff, and same over here. So which of these models is now the best? Well, now we have a way of actually estimating this based on the held out test set. 

Good. Same for the regression case. In this case, we have the mean squared error as a performance measure, so maybe just to highlight here, the way we measure performance here in the classification case, that we're looking at the accuracy, we've also seen confusion matrices and stuff like that. But typically the easiest thing to do is look at the accuracy, so we can measure the accuracy of this model, this model, and this model. 

Good. We can do the same for supervised learning as I said, here we have the mean squared error, that's the, let's say the distance from the red point here, that's a test point, remember, it hasn't been used to actually find the slope of this hyperplane, and we measure the distance from the data point to the hyperplane, we square it, sum it together for all the red points, that gives us a test error. Similarly for the other more complicated functions. Alright, the game continues, and this is a little bit more tricky, maybe this is where we talked about the density model. What is the error measure here? So we've still held out some red points. 

So I think you remember the dinosaur data set, cats and dogs and stuff like that, there we held out the dinosaur and did something, we held out the dinosaur as a test, here we're just holding out the red ones. But what is the error measure here? We'll be interested in density, so the error measure is the density, or the likelihood on unseen data. So in that case, it would simply be the product of the densities given those parameters for a test point, let me be explicit here, so that is the test likelihood. Typically we talk about the log likelihood, since log is a monotonic function, it doesn't really change the ordering things, so we can take log safely, we can see what happens, then you get the log likelihood, that is taking log to that product, it now becomes a sum, so we get the sum of log p of x and given the parameters over all the test points. 

Now, since everything else was about minimizing stuff, we can just put a minus in front of it, so we often talk about the negative log likelihood, if I put a minus in front. So we now want to maximize the accuracy, of course, of the error rate, we want to minimize that, we want to minimize the mean squared error, we want to minimize the negative log likelihood. So we have different error measures for different settings. So if we put all of that together in one slide, then for the classification case, we are measuring error rate, one minus the accuracy, we're doing that from the symbol model, the medium complexity model and a very complicated model. And we see certain numbers, in this case the middle one turns out to be the best one, the lowest better, yet the regression case, that's the root mean squared error, so that's the mean squared error and then we take the square root, lower is still better. 

So here it turned out that that middle model, middle complexity model tends to also be the best. In the density case, we're now looking at the negative log likelihood for the test points. Everything is for the test points here. 

Good. So it turns out also that middle model with what was it, three different components turns out to be the best one in this case. The name of the game here is not the particular examples we're looking at, or the important thing is not the particular examples we're looking at, it's the general principle here. 

We have some models, we've seen many different models, we can parameterize them, we can train them somehow, but there's also the complexity aspect which we need to choose, we now have a method for choosing complexity, namely by holding out some test points. Okay. Then there was some added complexity, because George has told you probably, hopefully, that you need something more here. You need the another outer loop in your cross validation to make sure you don't get an unbiased estimate of the generalization error. 

Why is that? So I'm not going to go through all the details again, I'm just going to maybe highlight the intuition that you should have here. So here we have a particular, let's pick this one, here you have a particular number, let's say 0.58 or whatever that is. That number came from us, of course, leaving out some test data. So we haven't seen that test data, but now you're actually making a choice that this is the best model. So in that sense, you have actually used all the data to find the parameters and complexity of the model. So you have now used up all the data. So is this estimate going to be a good estimate? 

Well, it's a number, right? The problem is, on average, this is going to be a biased estimate, because you always pick the lower one. But the intuition here is that, well, you used all the data to make choices about your model. Choices about the model, what could that be? That could be the parameters, the values of the parameters, but also the complexity. And you could think about complexity as just being another parameter in your machine learning model. Now you used all the data to find that. 

We know you're not allowed to do that. That should be a key takeaway. So we now actually need to have that double cross-validation or two-layer cross-validation, where we leave out some test data at the very beginning in the outer loop that we don't use for finding parameters, don't use for choosing complexity. If we evaluate our model on that data, we now have an unbiased estimator of the generalization error. That is the motivation for the two-layer cross-validation. 

All right. Essentially, that is all you need to know about machine learning. A bunch of different mathematical functions in all shapes and forms and whatnot, a way to train those, find the parameters from pairs of data or just data, and then we have a way to evaluate things. And this is key. 

That means it enables us to find the complexity of our models, make choices about the models, and then... Anything interesting? Okay. 

Good. Then it enables us to actually find the complexity of our models. So there's a little bit more to it than what I just covered here and that we also cover statistics and of course all of these basic things about the statistics and probability and linear algebra and similarity measures and all of this that goes into being able to do machine learning here. But what we try to present to you is this holistic viewer machine learning in that it doesn't matter if you're very good at defining mathematical functions and training that on some data, if you don't know what kind of data you're working on, the issues with that data, being able to, let's say, visualize it and get some information about what is actually going on in this dataset, you can visualize it for, let's say, inspection by some experts. 

Of course you need to be able to know all the fancy algorithms and models that are common in this domain and we've given you quite a broad range of methods to choose from and covering classification, regression, clustering, density estimation and so on. Okay. So this is probably where most of you find it difficult in terms of the math, but then there's also a huge aspect of evolution that we've emphasized in almost every lecture. And this is where it often goes wrong for companies or, let's say, even experienced researchers doing machine learning wrong and then publishing results or trying to find money for venture capital or whatever and they confidently say, I've got a very good estimate of my generalization error and they don't know about cross-validation, they don't know about doing proper statistics here. So that is why we also emphasize the importance of actually comparing models, for example, using statistical tools and you've got to do that in project two, I hope. 

And I can tell you it's probably the only sort of introduction to machine learning course in the world that actually introduces you to this. So at least now you know. So don't make those mistakes. And then of course we need to think a little bit about dissemination and you tried that in your reports to actually disseminate what do we actually get out of our models, how do we analyze it, how do we diagnose it and so on. So I hope you've got something from this whole pipeline and of course in terms of the sort of core methods and stuff, then we went from something relatively simple, linear, logistic regression that we've seen some examples of today. We covered decision trees and we covered small neural networks. 

I'll mention large neural networks in a bit, but of course you can think about taking these small neural networks that you did. You had one hidden layer with X number probably only a few hundred hidden units. You can make those deep, meaning you can have more hidden layers. You can have an infinite or you can have many, many thousands hidden units in your hidden layers. 

So you can basically expand this to arbitrary complex function. Now there's interesting research showing that actually in theory all you need is one hidden layer and infinite number of hidden units. That turns out in practice when you're working on something, a subdomain of the real data in the world, images, text, sound, whatever turns out it's better to actually go deep than wide. If you're interested in machine learning and theory of machine learning, maybe that's something you need to look into. Good, so we had something that simple linear models, we actually expanded that relative with a simple small step up to neural networks and we had everything in between in terms of decision trees and even non-parametric K nearest neighbor methods. We also talked about these major methods that if you have a very complicated neural network, one way to actually control the complexity is to do regularization. And this is where you add a penalty to your cost function, essentially penalizing large weights. That's the way we talked about it. 

We also had another, let's say, major method if you want, namely ensemble methods where we combine multiple relatively simple models and exploit whatever properties come from that. Okay, so you need to know about all of this and this builds the foundation for what you might learn in subsequent courses. I think we're doing good on time, so I'll just continue and I think we can wrap up within the hour and then you can go do the lecture, sorry, the exercise stuff, the TAs will come and I have some suggestions for what you can do. So that's essentially what I want to talk about in terms of the lecture material for today, basically just a very high level overview. Welcome to come down afterwards, talk about specific things. So now I'm switching gears entirely to sort of all the things about the course. 

So I took a look at the feedback you gave me and now, well, you didn't put your name on it, I think. So I just took a look at that and it's not particularly surprising. It tends to be the same thing we look at or see every year. 

So generally the feedback is very positive, it always is. Everyone learns a lot, it seems, in this course and we know that there's a lot of stuff in here, so hopefully you learn a lot. So of course there's something you need to remember here and that is that we are in this situation where we have second semester students to 10 semester students. We have bachelor students from all different branches of the TU, we have master students coming from, I think it's every single master program at TU. 

So the diversity in terms of background is huge. So when we look at this feedback, we sometimes try to take the extremes and then see, okay, are we roughly in the middle, so we satisfy everyone. And we do see a lot that, well, generally positive about having learned something and then there's some feedback about sometimes it's too abstract, it should focus more on the programming and practical implications and all of that. 

And then it goes from being well organized to exercise activities not relevant for me. And I can definitely see that, I talked to some of you about this, that yes, if you just sit and look at scripts and don't actually probe the scripts, try to see what can you actually gain from the scripts, yes, you'll learn nothing by just running them. Okay, I can totally get that. 

I would argue you're probably doing something wrong then. But anyway, we take it to heart and I'll have a few more comments later on that. Okay, so then there's everything from too much math on slides and in a book and even in the exam questions to not enough math. It's clear if you come from a mathematical education at DTU, some of the math we present here is fairly trivial to be honest. 

If you're not from a mathematical background at DTU, then the math can be overwhelming. We are perfectly aware of this. Okay, and then we sort of essentially in everything we try to do here, try to hit the sweet spot where we to some extent try to satisfy everyone and in the result we don't satisfy a single person, but hopefully as a co-ordered you gain something from this. Then there's something about the exercise that I already alluded to that, that some people feel that it's too much programming having to run the scripts and maybe change a few things. Other people find it's not enough programming. 

And I think it depends very much on your background, what you find interesting and how you learn. So what we've done here is make available these scripts that you can pick apart, you can extend them and you can do whatever you want with them, but it's not realistic in this particular instance of the course to actually have you implement everything from scratch. So that's just a comment on your comments, essentially we are perfectly aware of this. 

Good. Then there's something about the workload, too high for five ECTS. So I think I covered this in the first lecture and some of you wrote something about me having said that I don't believe you when you say that you spend more than nine hours. 

That's definitely not the case. So when we see a distribution of your, let's say, input here, how should I draw this? Okay. So we have roughly these nine hours per, or the nominal time, nine hours per week. Then we have a bracket that's called more, then we have a bracket called much more. Then we have a bracket called less, then we have a bracket called much less. And typically what happens is there's a few here. I think there's even some here. 

And then we have some here and then maybe some here and then some here. Oops. So hopefully this makes sense. So that was the way, way more than nine hours per week. So we definitely believe you when you say that you spend more time on the course. It also happens that when we look at this, we have to take into account and try to sort of analyze and interpret Do people, that means you on average, I'm not talking about any individual person here, do you as a cohort because we're looking at the distribution here, do you have the correct prerequisites for taking the course? And it's of course, if you don't have that, then you need to spend more time than the nine hours actually getting up to speed. 

So we're not surprised that it's above average, definitely not, given the, let's say, diversity of people and all of that. Then there's also another element and that is that if you like something, you tend to spend more time on it. And the third element that we are trying to actually, let's say, let's say, somehow get rid of, I would say, is the project work and the overhead in doing group projects. And I'm willing to, I'm open to ideas here, but please come see me if you have strong opinions about are the group projects good or bad. 

Would you like to see something else? Okay, but overall, this is not uncommon, this distribution is not uncommon for a technical course at DTU. So overall, we are happy with this. We don't feel like we've burdened you too much. I think you spend a lot of time on it, but hopefully you also gain something from it. And then does this matter, is five ECTS too much? 

Obviously, we would argue no. When you look at what a machine learning five ECTS point course looks like at some of the top universities in machine learning, that means Cambridge, it means Toronto, it means MIT, it means Berkeley. These are the people we want to compete with. At least that's the DTU policy. Then we need to put in this stuff now so that you can go on and do great and advanced stuff later on. So that is why this course might feel a little bit more work than some other courses at DTU. 

I don't think it's our problem. I don't think it's this course. I think it's the other course. We can discuss this all day and night. But anyway, moving on for that, this is a five ECTS course. 

We think this is what a five ECTS course in introduction to machine learning should be. You can disagree. Then there's an interesting one here. It's always fun, by the way, to read feedback and what type of words people use. So it gets worse for the exam. So the exam format is outdated and there was even something saying horrible. 

Okay, now I can refer back to what other universities are doing around the world. This is typically what you do in big courses. That's how popular, where you want an individual assessment of your performance. A multiple choice exam without age is not that uncommon. 

It's uncommon at DTU. All I can say is that I know we've been moving away from this to get so-called constructive alignment about what is it that you're supposed to do when you graduate and all of that with whatever you're doing in the exam. I would love to give you an oral exam. 30 minutes, maybe even an hour, I get to query what you know about this. It's not possible when we have 1100 students a year doing an exam, unless we hire 10 more people, which DTU can't afford. 

So the exam format is here to stay and I can tell you at Compute, more and more courses are actually following our lead on this and doing no-age exam multiple choice. Sorry, but that's the way it is. Good. Then there's something maybe not exciting for you, but at least for me in the sense that we finally get a chance to actually do something about these things we know could be improved. That is that the course will be split and in that sense it's also going to be updated. There's going to be a new course, a BC course or a bachelor course. 

It's only going to run in the spring semester, only in the spring semester. It's going to have a little bit more focus on the fundamentals and programming things from scratch and so on. Then there's going to be a new master course called 2451. That's going to only run in the fall semester. So if you have any ideas for what we should do with these courses, you're welcome to email me or send me or come down and have a chat with me. But we have some ideas at least and I think we'll talk, at least for the bachelor course we are going to talk with some of the students, I see a bunch over here, about what should be included in a second semester course about machine learning. So there's going to be a process about that. But anyway, thanks for the feedback. I think it's productive. 

It reinforces what we to some extent already know, but it also gives us a chance now to actually update it to better match each individual course. Good. Now onto something a little bit more, let's say productive or exciting. So of course not if, but when you want more machine learning. So when you want more machine learning, then I've listed a couple of courses, not a couple, a lot of courses here that we teach in a GTU compute and specifically the section I'm in, that is the one doing the majority of core machine learning, both research and teaching. 

So there's a specific course that some of your colleagues, the kids students they will take, general population cannot take it. But following that, there's going to, there's a course called deep learning. That's, you can sort of guess that that's where we take the small neural networks. 

You then make them deep. You add more parameters, more layers and different architectures, transformers, recurrent neural networks. That's a project course that tends to be very popular and you also learn a lot there and you do a lot of practical work in the project. Then there's a course on machine learning for signal processing and it's not signal processing in the old school definition of signal processing. 

It's about machine learning for various types of problems, online filtering and kernel methods and all of that. There you need a little bit more math. You get to actually do things on pen and paper, math stuff, not sort of quiz problems like we have here. And then you may want to move on to machine learning operations. Oh, question. I'll get to that, maybe. Remind me if I don't. Okay, I'll move on and the person will ask the question again. 

Okay, so I'll go through the list and we can discuss the order of things. Okay, so machine learning operations. After deep learning, you would have, let's say, learned to do complicated deep learning models. Now in machine learning operations, you learn to deploy them, you learn to debug them and all of these things that are really important to slightly more practical and industry-oriented settings. It's also hugely beneficial if you're going to work with these models in a university setting. And it tends to make life much, much easier once you've completed this course, if you want to work on machine learning, of course. Then there's the Bayesian machine learning, probably my favorite. 

You have sort of a small sense of it. If you remember the beta-banuli model we did with the coinflip, where we included some prior knowledge, that's the distribution over the probability of coming up heads and tail. We included that with a likelihood. That was basically just the banuli likelihood. What's the probability of seeing your heads given that you have a certain probability of heads? 

And then we had the posterior. So now you're not just trying to optimize things to find a point estimate or a set of weights, a vector of weights. You're actually wanting to find a distribution over the weights. That is important for uncertainty quantification and in, let's say, mission-critical settings. And it's quite an interesting area, I would say. And it's a big research area at DTU in our section. Then we come to reinforcement learning. So we've looked at supervised, unsupervised learning as two canonical examples of what machine learning is and can be useful. 

The third one, typically you're seeing in textbooks, relates to reinforcement learning or control. This is where you need to make a decision about what to do today, knowing that you need to make a decision tomorrow. So this is about combining utility theory with decision theory and probabilities. So thinking about how do you learn a computer to play... Oh, sorry, yeah, how do you learn machine learning? 

Algorithm to play computer games, for example. It's about making sequential decisions. And then your colleague asked before what causes would be recommended to take this course. So this course actually assumes that you know everything there is to know about expectations before going in. So expectations, like with respect to some probability distribution, they too tell how he expects that you know all of this. He also expects that you are fairly good at programming stuff and maybe even numerical stuff. So causes on that, if you can find those, that would probably be probability theory. That would be a very good course to take. 

It's a very tough course. And maybe some more programming in some shape or form. So that moves on to the last one, sort of in our series of machine learning causes in our section, namely advanced machine learning. And it's a little bit sort of put together by three topics at the moment and they might change. Right now it's Romanian geometry. 

So not just the Euclidean geometry we've been using, but how do you actually do machine learning on manifolds? So if you're interested in that, that's really interesting. Then it's about what is it now, graph neural networks. So instead of inputting a vector to a machine learning model, your input and maybe output a graph. And the last topic, that's generative modeling. Not generative modeling in the chat dbt sense, but generative modeling in the probabilistic sense, where you can build a density model effectively and then you can draw samples from that. So you've probably seen stable diffusion, what's the Google one called? I can't remember, but generating images, for example, by conditioning on some text prompt or something like that. The basic models here, so that would mean what we call the original auto encoders. 

It mean normalizing flows and diffusion models. Okay, so plenty of opportunity here. And then I'll just mention that we do, it's a special thing at D2, I think, but we do special courses. This is where if you have a real strong interest in something, then you're welcome to approach us. That means every faculty member in our section and say, okay, I found this really interesting papers on conditional diffusion models for molecules, whatever it is. You come and say, could we do a special course about this, where I learn about diffusion models, I learn about these particular things, and then we can probably set that up. 

You need to be serious about it. It means you need to have studied this before, you need to make sure the person you approach actually has an interest in that. Of course, we also do Bachelor Master, and eventually when you get to it, PhD projects as well. And there's a link here in the slides where you can see some suggestions for projects if you're interested in going down. I'll say there are many other machine learning courses at D2U that tends to be a little bit more applied in that domain, and of course you can learn a lot about that. 

We tend to sort of focus on the basic core concepts about the learning algorithms in our section at least. Good. Do you need a break? 

I think there's three, four slides left. Do you need a break? No. 

If anyone needs to break, just leave for five minutes. I won't take offense. Okay. This is probably why you came to hear me talk a little bit about the exam. 

So, I have said this before, 13 weeks ago. I've said that the course, that means your final grade in the course is based on an overall assessment. So this is a special thing, it's not just something we say. It means that approximately one tenth of the grade is based on your projects. Some of the feedback would say that's too little. Actually, it's a lot, because if you hadn't done the projects, you couldn't even get 90%. But anyway, just to keep you on your toes, one tenth of the grade is based roughly on the projects. The other 90% roughly on the performance on the exam. The overall assessment comes in here in the sense that, in principle, there's not a strict mathematical formula going from the performance you had on your report and the performance you had on the exam to the grade. The overall assessment comes in, and the external examiner together with me and maybe George will actually look at each individual case, and then we make an overall assessment. Of course, it's informed very strongly about, let's say, the score you obtained in each of these parts of the assessment. But that's also why we can't give you a particular grade for the reports, for example. But if you read between the lines, I'm sure you can see what you obtained on the reports. Good, that's the overall assessment. Okay, so the reports right now, they must be approved or passed before you can pass the course. 

And we do not accept new updated reports at this point, unless you've got an extension for some reason. And the reason for this, and now we're getting back to why we're doing things, but the reason is we want people to have engaged with the material before showing up at a relatively difficult exam. So we want that the people showing up, they're prepared. That's the reason why we make sure that you have at least done the reports to some extent. 

Okay, there's a sort of a tricky thing here. It's not to actually be able to attend the exam, it is to pass the course. I think that you can actually attend the exam, even though if you haven't passed the reports. 

However, don't do that, it's going to be a waste of your time, because you are not going to pass the course, even if you get a perfect score on the exam. It is a strict requirement that you get a pass on the two reports. So don't sit the exam, you'll have to do it again next semester, if you didn't have your reports passed, sorry, approved. 

Good, then the written exam. As far as I know, it's still on the 28th of May. I don't know when, it can be in the morning, it can be in the fall, or sorry, in the afternoon. 

I have no idea. It's not my choice or my decision, so keep an eye out on this examsplan.dtu .dk. You should be able to see it. 

I don't get it before you do, so keep an eye on that. It's here on campus, four hours. You will get the exam set as a hard copy. We will print it for you, it will be on your table, or the invitulators, they will give you the exam set, and the answer sheet. 

It's a, this is again a formal thing, according to DTU rules, it's a no-hates exam, so you can see what that means in the DTU context. It means you can't bring material, course material, books, and old exams that's printed and stuff like that. You can't bring a computer, tablet, phone, or your mother. You are allowed to actually bring two sheets, that's an exception to the rule. You are allowed to bring two, a four sheets of handwritten notes, front and back, we've talked about this before. They do need to be handwritten. 

You cannot write it on your tablet and print it using a printer. We have discussed that before as well, I'm not going to go into that debate. You need to actually write it on pen and paper. 

Good. Then you need, you can also bring a non-programmable calculator, and the only thing I'm going to say about this is that you can look up on the web page for your calculator, and it will tell you if it's programmable. If it says programmable, don't bring it. Every Texas instrument, 83 to 89, they're programmable. 

So we have a big, it's likely that if you have a big calculator with a big screen, it's very likely that it's programmable. It's your responsibility to check, not mine. So if you're in doubt, contact the study administrations, and they will tell you. I borrowed one from one of your colleagues. He brought it there. 

He will show you in a bit. I think it's a standard high school thing here in Denmark, maybe, I don't know. So it's called a GI30XS, for example. It's fairly big. It has a fairly big screen, but it's not programmable. 

It is a scientific calculator. That's okay. So something that says X and something like that, that's fine. Something where you can put in your own programs, or it can do matrix computation, that's not allowed. Your responsibility to check that it's not programmable. Now I said it. 

Good. You know the type you've seen about, I don't know, 20 or 15 or something, previous exam sets. Okay, let me just check, chat. 

Okay, thanks for the info. So the format, you know the format of the type here. It's multiple choice, five options, one correct, negative marking. You get a minus one for a wrong answer. You get zero for don't know. Okay, syllabus, I've already mentioned that actually, chapter one to 21. 

It's in the course handbook or this sort of outline on the webpage. Good. So any questions about this stuff? Okay, you first. Okay, so I should say it's a good question. 

I should have said that. This applies to a, I need to be careful with my words now, to sort of, yeah, let me phrase that differently. If you for some reason have a disability, you have been given extra time for the exam, of course you also have extra time here. If you have, let's say, if you have problems with words or not, I also have that, then well, and you got extra time and even allowed to bring a computer so that computer can read the script for you. That also applies here. You have to contact the study administration to put that into effect. Okay, so this is sort of for the standard situation. 

Yeah, someone there. So the question is, is there a set that means predefined, pass its percentage? No, because it is an overall assessment. Well, you don't need to pass the multiple choice exam per se. 

But of course, if you don't, sorry, there's very little chance that this small amount here can actually save you. But of course, one-tenth of the total grade, if you write on the boundary between passing and no passing, based on the exam, then the one-tenth from the report in this overall assessment can potentially make you pass. So it can switch, if you write on the border, it can actually make you jump up one grade. Conversely, also the other way, if you expect it to actually have one-tenth of the points before going into the exam, then if it turns out that maybe your reports were so bad, then that's not the case, and it might be a disappointment. Anyway, so I can say the general rule at DTU, or rule of thumb, is that if you get 50%, then you are definitely sure to pass. That's the normal sort of standard rule of thumb. Traditionally, I can say that perhaps it's slightly less in this course, but I can't give you the percentage. But aim for 50%, that's the rule of thumb at DTU. 

So I'll just make maybe a comment about that. So you can try to multiply 27 questions by 3, and then see what's 50% of that. That's the way to compute it. Of course, you need to deduct minus one if you expect to make mistakes. 

Don't do mistakes, but if you do make a mistake, you of course need to deduct that. So it's based on this, what is it? Yeah, 3 times 27, right? 50% of that, if you obtain that, then all I can say is I've never seen someone fail, if you get half of that, and do decent reports. Okay, of course something that some of you might not have seen before is this answer sheet thing here. So of course, as I said, the exam set will be available and printed on site. So will this exam sheet here, or answer sheet, can I wait? Okay, so this exam sheet here will be, or answer sheet will be available in printed form. You need to hand in this thing here only, not the whole exam set. So the only thing you get to do is fill out this thing here, and be careful when doing it, and then you hand that in, normally you would get a green envelope, you put this sheet in there, and you write your student name, student number, and table number, and room number, on I think both on the green envelope, and here, I'll come back to this. 

You need to put in your student ID here, so that means for me, you would need to put in 001416, but to make this easily, automatically, let's say, passable, you also need to then take the corresponding box here. So that was a zero, that was a zero, that was a one, that was a four, oops, four down here, it was a one, and it was a six here. Please do this carefully, yeah? And the way you take a box here is not by doing this, it is by actually completely filling in that bubble or that circle. This will make this office redundancy, in that you have written your student number, you have also indicated in sort of a passable form. You also need, and again, this is redundancy for your sake, so we don't somehow make a mistake here, you also need to write down the room name and number, so if you happen to have an exam in here, which you don't, but one of the smaller rooms next door, it will be 116, you will write 0112, for example. 

The table number you should, when you actually go to the exam, you know your table number, it could be, I don't know, 28, something like this. You just write that in free form. Now, this you can hopefully do without making mistakes, and I won't deduct you, but please don't make mistakes here. And then, of course, the important thing here is that you need to actually answer the exam questions. And there will be 27 questions, and that's a weird number, I know, but there's a history here, but there's 27 questions. 

You need to select one answer for each question, one answer only. So if I zoom in here, you get the chance to actually fill in, let's say the first one, you think it was A, and I recommend you use a pencil, like not a pen, but a pencil and bring rubber so you can actually erase stuff. So let me sort of emulate that here. I have a pencil, I fill in A because I know that the answer here is A, something like that. I fill in B for the next one, something like that. And I try to be careful not drawing outside the lines, but of course, I know it's a stressful situation, so it'll panic, right? I fill in this one. 

Then I get down here. Okay, first four ones, I'm absolutely sure of these answers. Then maybe at some point at the end, I would recommend that you actually fill these in completely at the end of the exam, maybe 10 minutes before or something, you have to add in or something. Yeah, but let's imagine when you fill it in, you put in A, then you make a mistake, you realize, shit, I actually don't know this. 

What to do? Well, then you erase this and you fill in E. That, of course, only happens if you use a pencil and rubber. If you, for some reason, happen to, by mistake or whatever, you actually use a pen and fill in A, and then you realize, it was actually D. What you do is you fill in D, you cross over this one clearly, and then you just, as a safety precaution, again, for your benefit, you just write D here, then there's no doubt that what you actually meant was D. Does this make sense? 

Yeah, relatively simple. Just be aware that there will be a computer reading this, there will be a program reading it, and we are using these weird markers to make sure that there won't be any mistakes here. Okay, so hopefully that's clear, and you do need, please, to fill in the E, even if you don't know the answer. So if you don't know the answer to question 10, please fill in E. Again, it's a safety precaution, it makes it much more, let's say, resilient by mistake thinking it's A because there's a little bit of dirt or whatever it is. Okay, so this tends to work quite well, so don't panic about this. And let me just zoom out. 

Okay, then a few more comments at C. Yeah, so this is all you need to do. Do not try to write equations or explanations on the sheet, and keep it as clean as possible. Okay, clean means two things. It means that whatever answer you give in, make sure it's clearly, let's say, discernible, and it's clear what you mean by the indication you have given. It also means don't pour coffee all over it. It also means don't write your notes on this sheet. You will get paper from the invigilators there to make notes, because this will have to be automatically passed. Anything else? 

Yes, use black pencil, a very dark pencil, and bring a rubber. That will be my advice. What else? I think that's basically it what I have here. Any questions about this? Zoom, people, do you have questions? No? Great. Then that's clear. 

Good. So that, as I said before, we're talking about the date of the exam. This is the date of the exam. Okay, so the question is about the exam date. 

So let me show you how to see this. It has a special exam date. That's why it's not in the standard schema, I guess. So if you look here, let me find the signature. Date of examination. The exam will be held on a special day. Click date of the examination on the left to see the date. 

Let's do that. Ordinary exams. And then we look for 2450, which should be 28th. Yeah, agreed. I don't decide this, so it's not up to me. So it's definitely on the 28th, and it will not change now unless the world breaks down, I guess. Okay, so this date is correct up here. It will also say the time when we get to it. And the sheet might look slightly different in terms of the markers and stuff, but roughly the same, please put in the room number and name. 

Please put in the table number. Please put in your student ID carefully. Don't put in a wrong student ID. I know you don't care too much about my time, but it takes at least 5, 10 minutes every time someone puts in a wrong thing here. So please think of me when you put in this. Yes, question. You can scan the Q... Well, the question is what's the QR code for? 

Try to scan it, you can see. It's just a number, essentially. It's just a small test to see what happens. There's another question somewhere. 

Yeah. The question is if someone is a little bit sloppy and spills coffee or chocolate or whatever, not you necessarily, but someone does that, can I get another sheet? So we need to print 600 of these sheets. We need to print about 6,000 pages. So we will print a few extra ones, but please, please don't do this, everyone. Please don't ask for a second sheet, right? I have to then run all over campus to basically be a mailman delivering answer sheets. 

But yes, of course, if there's a problem with your sheet and it broke or somehow you indicated something wrong and it can't be fixed, yes, you can get a new clean sheet. Okay. We're diverging a little bit, but a question about the notes. You can bring two A4 sheets. That's two pages here, well, it depends on two sheets, right? Two pieces of paper. You can write on both sides of that piece of paper on each of them. 

So in total, four pages, right, written. Yeah. Is that clear? Yeah. Anything else about the practicalities of actually having to do this with a pen and paper? 

Yeah. So is the question, how many do you need to get correct to? So I think that that relates to your colleagues questions over there in that, you know, use the following thing. As a rule of thumb, you need to get 50% of the available points. 

If you do that, then you are, I can't guarantee you pass, but I've never seen anyone fail if they get that. That means if you count here, three points for 27, what is that 81? Yeah. 

81 points available. Am I right? Yes. So there's 81 points available here. If you answer, let's say all of them correct, obviously get 81 points. 

50% of 81, yeah, well, you can do the math. The problem here is, and I have a note on this later, and I'll jump to that slide now. The problem is that if you answer incorrectly or say don't know, then you essentially, you lose the, you lose some points, right? Either you lose three points or you actually lose four points in the sense that we need to deduct one point. 

So there's negative marking for wrong answers. Okay. A few more points here. 

Good. So before starting the exam, and this is just generalized based on my sort of perception of how you may want to think about this. Before starting the exam, you'll get the script. Some usually elderly person will hand you the script. It's typically between 12 and 14 sort of pages, so seven sheets, something like that. 

They'll be stable together. Before you actually start, I would highly recommend that you actually skim through the whole exam set. Because the questions are not ordered, necessarily at least, by your perceived difficulty of the questions. There are probably topics you are more comfortable with than others. 

Make sure you answer those first. Unfortunately, do see people trying to solve them sort of chronologically, well, not chronologically, but in order. And then they never really get to the easy questions that they have studied for really, really hard. So make sure you somehow answer the questions that you find easy first. 

Those are the safe points. And make sure you do that before you spend two hours actually trying to solve a hard question or whatever it is. On average, it's about eight minutes per question. Some of them you'll be able to solve in two minutes, one minute even. 

Others you may need 15 minutes, but that's very individual what you've studied for, what you find easier. So just make sure you sort of have a strategy here. And unfortunately, we do see people not really having a strategy going into this exam. And that's a problem and there's negative marking. Always, excuse me, always consider all options and carefully compute the full solution when necessary. Of course, it depends a little bit if there's something to compute in terms of if you have a conceptual question, it might just be a sentence. But don't just go down the list and say, sorry, yeah, that was correct. 

So I'll never really look at the three remaining options. Probably there are questions that look correct, but unfortunately wrong. For some reason, it could be a sign you make a mistake with the sign somewhere in your computations or whatnot. You read off something wrong. So make sure you go through all options and then make a rational choice here. 

Now, you're, well, people are not generally rational, but what I really want you to do here is have a strategy and be rational. Because if you just sort of put in a uniformly at random choice between one and five and take that box, the expectation that means on average, what score will you get, you'll get zero. Of course, that's a simplified sort of thing here. And if you get zero and you see there are 81 points available, then you're not going to pass. It also brings me to something else. 

If you answer every single question and you answer every single question wrongly, you can get negative score. We do see that. Okay. 

In that case, we typically have a conversation with that person. But okay, so the expected score here is zero. It means don't guess at random at least. Don't just put in an answer to put in an answer. 

It's definitely not going to benefit you. If you can rule out two options or something like that, maybe you would want to take a guess. You can do the expectation in that case. Maybe you want to take a guess there, right? But at least don't do it just to put in an answer and guess and hope for the best that usually results in a negative score. Okay. 

Good. Then something else, and this is maybe not a big problem anymore in that you can't actually bring the 20 previous exam sets to the exam. But really try to look at the variability and type of questions in previous exam sets in this course. Unlike other courses at DTU, we always make new types of questions. That means they're formulated in a different way. 

You need to look for something else. Because we're not actually in the business of testing whether or not you can solve a specific type of question just switching out the numbers. We want to go a little bit deeper here. So we spend a lot of time on creating the exam and just keep that in mind when you look at previous exam sets. Try to compare two exam sets. About half the questions will definitely be different in the way that they're formulated or the thing you have to do. But of course you have plenty of exam sets to get comfortable with this. Now, our advice here would be that you actually practice several of these exam sets in this no condition, sorry, no eighth condition, and without the solution. You have to promise me that you're not going to just sit with the problems here and the solutions next to you. You're going to cheat yourself, especially on multiple choice. 

I personally don't like multiple choice questions. But if you have the solution next to you while you're actually studying this, you're not going to learn the underlying things you need to learn. And you're not, you're going to cheat yourself, you're going to overfit to that specific problem. 

Literally overfit. So please practice this. That means go into a room for four hours, don't bring your computer, bring a pen and paper. And of course maybe some of the previous exam questions, if you don't look at the fall 24 version, they might actually require a computer to do efficiently. So leave that question like doing a complicated matrix operation or whatever it is. But in general, you can use the fall 2024 version and actually practice this. Then I'll end with maybe this one here. Actually, there's one more slide. 

Almost done. But I know that some of you don't necessarily like these practical exercises. Do them anyway. 

And then really listen to what I'm saying now, because they give you that intuition about, think about when we have to actually do an exam question. You know, we've done hundreds by now, so we've a little bit bored maybe. We actually run some of the old scripts and see, okay, what kind of figures came out there? What happens if I change the parameter and can we make the students actually identify that parameter or whatever it is? So actually going through these scripts, changing things and looking at different data sets for a particular algorithm, will give you some intuition about what's going on when I change the parameter or what happens when n goes to infinity or something like that. So please, I mean, do that also in your operation. 

Don't focus only on a specific exam questions. Right. Almost done. So the rest of today, there's no exercise session per se, but the TAs will be available actually from some of them from in 10 minutes. And they will answer questions about all exam sets and all the material if you need to catch up on something. They're going to be in here. They're going to be in the area outside here. They're going to be in the two rooms reserved for the kids students. So you're free to go wherever you want today. 

So any of these rooms will, there'll be a TA at some point. I suggest you, as I said, you catch up on what you may have missed. Some of the practical exercises. You do some of the old exam sets or you generally ask the TA. 

So the TAs are here. So exploit them in a good way and ask them questions about conceptual things and have them actually, let's say, contribute to your learning here through a last chance. Then from next week, basically from today onwards, the only help and only resources we have on Piazza. So please help each other out. 

You can go in there. You can ask a question without giving your name or revealing your name. And then actually think about answering questions from other students. 

One way to learn is to actually try to help another student find the answer. See, this is an opportunity to help them out. We will, that means the TAs mainly and staff, me and George, at some time we will go in and moderate and make sure that, you know, something doesn't diverge totally. 

But in general, some of the TAs will have a look as well. But don't count on us answering all the questions. Okay, then we will not, we will need to put a cut-off date because there's a tendency to put up questions 11 o'clock the day before the exam and expect that to be addressed. 

So after the 23rd of May, we are not going to actually moderate anything in there. It's up to you. You help each other out. Maybe just one comment. So after the exam, you can give feedback again via DTU system. And the reason why I put three dots is that I don't necessarily think it's a very productive thing that you walk out the exam and maybe you had a bad day and you feel like yelling at someone that someone tends to be me. 

And then you just write, whatever comes to mind. All I can encourage you, please be a little bit, okay, what just happened here? Why did that go wrong? Or why did it go well? 

I would also like to hear if you think it's a good exam. So think a little bit about before maybe giving feedback there. Okay, we will make the answers available, but please give us a little bit of time. I mean, I might not actually be here while I'll be here during the exam period, but we have other things to do. So we might take a look at your answers and see if there's something totally wrong or something before we actually publish the results. Okay, so I will also do a small experiment. 

So once we pass these exam sets, I will create an assignment on DTU Learn and I'll upload your exam set there with your, with the detected answer. That serves two purposes. One you see and you remind yourself what did I actually answer. It makes sure that if you had a very clouded, whatever exam sheet, then we actually passed the results correctly. So it's also to check that the whole procedure is okay. So that means you will get to see your exam, sorry, answer sheet once again, hopefully, if this works out. Okay, final thing here and I will ignore any email about this. We cannot give the grade to anyone. 

It doesn't matter if, you know, the world is falling apart. We cannot give the answer before the external examiner has actually vetted or censored the grades. So we will not inform anyone of the grades before they enter through the usual channels. 

So don't ask us. Good. I think that's basically it. So I hope you learned something and then thank you. Thank you. 
